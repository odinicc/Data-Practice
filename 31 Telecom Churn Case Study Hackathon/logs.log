2024-02-07 22:35:56,493:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-07 22:35:56,493:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-07 22:35:56,493:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-07 22:35:56,493:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-07 22:43:19,715:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-07 22:43:19,715:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-07 22:43:19,715:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-07 22:43:19,715:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-07 23:33:48,992:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-07 23:33:48,993:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-07 23:33:48,993:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-07 23:33:48,993:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-08 01:21:32,634:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-08 01:21:32,635:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-08 01:21:32,635:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-08 01:21:32,635:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-08 01:45:53,418:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-08 01:45:53,419:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-08 01:45:53,419:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-08 01:45:53,419:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-08 01:48:01,529:INFO:PyCaret ClassificationExperiment
2024-02-08 01:48:01,529:INFO:Logging name: clf-default-name
2024-02-08 01:48:01,529:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-02-08 01:48:01,529:INFO:version 3.2.0
2024-02-08 01:48:01,529:INFO:Initializing setup()
2024-02-08 01:48:01,529:INFO:self.USI: 727f
2024-02-08 01:48:01,529:INFO:self._variable_keys: {'seed', 'y_test', 'is_multiclass', 'exp_name_log', 'gpu_param', 'fold_groups_param', 'target_param', 'data', '_available_plots', 'exp_id', 'gpu_n_jobs_param', 'memory', 'logging_param', 'fold_generator', 'pipeline', 'X_test', 'n_jobs_param', 'y', 'fold_shuffle_param', 'USI', 'y_train', 'html_param', 'log_plots_param', 'X', '_ml_usecase', 'fix_imbalance', 'idx', 'X_train'}
2024-02-08 01:48:01,529:INFO:Checking environment
2024-02-08 01:48:01,529:INFO:python_version: 3.10.9
2024-02-08 01:48:01,530:INFO:python_build: ('main', 'Jan 11 2023 15:15:40')
2024-02-08 01:48:01,530:INFO:machine: AMD64
2024-02-08 01:48:01,530:INFO:platform: Windows-10-10.0.19045-SP0
2024-02-08 01:48:01,530:INFO:Memory: svmem(total=16856182784, available=3750866944, percent=77.7, used=13105315840, free=3750866944)
2024-02-08 01:48:01,530:INFO:Physical Core: 4
2024-02-08 01:48:01,530:INFO:Logical Core: 8
2024-02-08 01:48:01,530:INFO:Checking libraries
2024-02-08 01:48:01,530:INFO:System:
2024-02-08 01:48:01,530:INFO:    python: 3.10.9 | packaged by conda-forge | (main, Jan 11 2023, 15:15:40) [MSC v.1916 64 bit (AMD64)]
2024-02-08 01:48:01,530:INFO:executable: C:\ProgramData\miniconda3\python.exe
2024-02-08 01:48:01,530:INFO:   machine: Windows-10-10.0.19045-SP0
2024-02-08 01:48:01,530:INFO:PyCaret required dependencies:
2024-02-08 01:48:01,684:INFO:                 pip: 22.3.1
2024-02-08 01:48:01,684:INFO:          setuptools: 65.6.3
2024-02-08 01:48:01,684:INFO:             pycaret: 3.2.0
2024-02-08 01:48:01,684:INFO:             IPython: 8.20.0
2024-02-08 01:48:01,684:INFO:          ipywidgets: 8.0.4
2024-02-08 01:48:01,684:INFO:                tqdm: 4.64.1
2024-02-08 01:48:01,684:INFO:               numpy: 1.25.2
2024-02-08 01:48:01,684:INFO:              pandas: 1.5.3
2024-02-08 01:48:01,685:INFO:              jinja2: 3.1.3
2024-02-08 01:48:01,685:INFO:               scipy: 1.10.1
2024-02-08 01:48:01,685:INFO:              joblib: 1.3.2
2024-02-08 01:48:01,685:INFO:             sklearn: 1.2.2
2024-02-08 01:48:01,685:INFO:                pyod: 1.1.2
2024-02-08 01:48:01,685:INFO:            imblearn: 0.12.0
2024-02-08 01:48:01,685:INFO:   category_encoders: 2.6.3
2024-02-08 01:48:01,685:INFO:            lightgbm: 4.3.0
2024-02-08 01:48:01,685:INFO:               numba: 0.59.0
2024-02-08 01:48:01,685:INFO:            requests: 2.31.0
2024-02-08 01:48:01,685:INFO:          matplotlib: 3.6.0
2024-02-08 01:48:01,685:INFO:          scikitplot: 0.3.7
2024-02-08 01:48:01,685:INFO:         yellowbrick: 1.5
2024-02-08 01:48:01,685:INFO:              plotly: 5.18.0
2024-02-08 01:48:01,685:INFO:    plotly-resampler: Not installed
2024-02-08 01:48:01,685:INFO:             kaleido: 0.2.1
2024-02-08 01:48:01,685:INFO:           schemdraw: 0.15
2024-02-08 01:48:01,685:INFO:         statsmodels: 0.14.1
2024-02-08 01:48:01,685:INFO:              sktime: 0.21.1
2024-02-08 01:48:01,685:INFO:               tbats: 1.1.3
2024-02-08 01:48:01,685:INFO:            pmdarima: 2.0.4
2024-02-08 01:48:01,685:INFO:              psutil: 5.9.0
2024-02-08 01:48:01,685:INFO:          markupsafe: 2.1.3
2024-02-08 01:48:01,685:INFO:             pickle5: Not installed
2024-02-08 01:48:01,686:INFO:         cloudpickle: 3.0.0
2024-02-08 01:48:01,686:INFO:         deprecation: 2.1.0
2024-02-08 01:48:01,686:INFO:              xxhash: 3.4.1
2024-02-08 01:48:01,686:INFO:           wurlitzer: Not installed
2024-02-08 01:48:01,686:INFO:PyCaret optional dependencies:
2024-02-08 01:48:01,702:INFO:                shap: 0.44.1
2024-02-08 01:48:01,703:INFO:           interpret: Not installed
2024-02-08 01:48:01,703:INFO:                umap: Not installed
2024-02-08 01:48:01,703:INFO:     ydata_profiling: Not installed
2024-02-08 01:48:01,703:INFO:  explainerdashboard: 0.4.5
2024-02-08 01:48:01,703:INFO:             autoviz: Not installed
2024-02-08 01:48:01,703:INFO:           fairlearn: Not installed
2024-02-08 01:48:01,703:INFO:          deepchecks: Not installed
2024-02-08 01:48:01,703:INFO:             xgboost: Not installed
2024-02-08 01:48:01,703:INFO:            catboost: 1.2.2
2024-02-08 01:48:01,703:INFO:              kmodes: Not installed
2024-02-08 01:48:01,703:INFO:             mlxtend: Not installed
2024-02-08 01:48:01,703:INFO:       statsforecast: Not installed
2024-02-08 01:48:01,703:INFO:        tune_sklearn: Not installed
2024-02-08 01:48:01,703:INFO:                 ray: Not installed
2024-02-08 01:48:01,703:INFO:            hyperopt: Not installed
2024-02-08 01:48:01,703:INFO:              optuna: Not installed
2024-02-08 01:48:01,703:INFO:               skopt: Not installed
2024-02-08 01:48:01,703:INFO:              mlflow: 2.10.0
2024-02-08 01:48:01,704:INFO:              gradio: Not installed
2024-02-08 01:48:01,704:INFO:             fastapi: Not installed
2024-02-08 01:48:01,704:INFO:             uvicorn: Not installed
2024-02-08 01:48:01,704:INFO:              m2cgen: Not installed
2024-02-08 01:48:01,704:INFO:           evidently: Not installed
2024-02-08 01:48:01,704:INFO:               fugue: Not installed
2024-02-08 01:48:01,704:INFO:           streamlit: Not installed
2024-02-08 01:48:01,704:INFO:             prophet: Not installed
2024-02-08 01:48:01,704:INFO:None
2024-02-08 01:48:01,704:INFO:Set up data.
2024-02-08 01:48:01,936:INFO:Set up folding strategy.
2024-02-08 01:48:01,936:INFO:Set up train/test split.
2024-02-08 01:48:02,099:INFO:Set up index.
2024-02-08 01:48:02,107:INFO:Assigning column types.
2024-02-08 01:48:02,285:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-02-08 01:48:02,349:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-08 01:48:02,353:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-02-08 01:48:02,390:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-08 01:48:02,390:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-08 01:48:02,614:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-08 01:48:02,616:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-02-08 01:48:02,650:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-08 01:48:02,650:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-08 01:48:02,651:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-02-08 01:48:02,701:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-02-08 01:48:02,731:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-08 01:48:02,731:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-08 01:48:02,784:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-02-08 01:48:02,820:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-08 01:48:02,821:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-08 01:48:02,821:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-02-08 01:48:02,909:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-08 01:48:02,909:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-08 01:48:03,000:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-08 01:48:03,000:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-08 01:48:03,006:INFO:Preparing preprocessing pipeline...
2024-02-08 01:48:03,023:INFO:Set up simple imputation.
2024-02-08 01:48:03,023:INFO:Set up imbalanced handling.
2024-02-08 01:48:03,477:INFO:Finished creating preprocessing pipeline.
2024-02-08 01:48:03,485:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\user\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id', 'last_date_of_month_7',
                                             'last_date_of_month_8', 'arpu_6',
                                             'arpu_7', 'arpu_8', 'onnet_mou_6',
                                             'onnet_mou_7', 'onnet_mou_8',
                                             'offnet_mou_6', 'offnet_mou_7',
                                             'offnet_mou_8', 'roam_ic_mou_6',
                                             'roam_ic_mou_7', 'roam_i...
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              n_jobs=None,
                                                                              random_state=None,
                                                                              sampling_strategy='auto'))))],
         verbose=False)
2024-02-08 01:48:03,485:INFO:Creating final display dataframe.
2024-02-08 01:48:05,820:INFO:Setup _display_container:                     Description              Value
0                    Session id               1466
1                        Target  churn_probability
2                   Target type             Binary
3           Original data shape       (69999, 129)
4        Transformed data shape      (109014, 129)
5   Transformed train set shape       (88014, 129)
6    Transformed test set shape       (21000, 129)
7              Numeric features                128
8                    Preprocess               True
9               Imputation type             simple
10           Numeric imputation               mean
11       Categorical imputation               mode
12                Fix imbalance               True
13         Fix imbalance method              SMOTE
14               Fold Generator    StratifiedKFold
15                  Fold Number                 10
16                     CPU Jobs                 -1
17                      Use GPU              False
18               Log Experiment              False
19              Experiment Name   clf-default-name
20                          USI               727f
2024-02-08 01:48:05,941:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-08 01:48:05,942:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-08 01:48:06,053:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-08 01:48:06,053:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-08 01:48:06,054:INFO:setup() successfully completed in 4.57s...............
2024-02-08 01:50:01,973:INFO:PyCaret ClassificationExperiment
2024-02-08 01:50:01,973:INFO:Logging name: clf-default-name
2024-02-08 01:50:01,973:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-02-08 01:50:01,973:INFO:version 3.2.0
2024-02-08 01:50:01,973:INFO:Initializing setup()
2024-02-08 01:50:01,973:INFO:self.USI: e8f0
2024-02-08 01:50:01,973:INFO:self._variable_keys: {'seed', 'y_test', 'is_multiclass', 'exp_name_log', 'gpu_param', 'fold_groups_param', 'target_param', 'data', '_available_plots', 'exp_id', 'gpu_n_jobs_param', 'memory', 'logging_param', 'fold_generator', 'pipeline', 'X_test', 'n_jobs_param', 'y', 'fold_shuffle_param', 'USI', 'y_train', 'html_param', 'log_plots_param', 'X', '_ml_usecase', 'fix_imbalance', 'idx', 'X_train'}
2024-02-08 01:50:01,973:INFO:Checking environment
2024-02-08 01:50:01,974:INFO:python_version: 3.10.9
2024-02-08 01:50:01,974:INFO:python_build: ('main', 'Jan 11 2023 15:15:40')
2024-02-08 01:50:01,974:INFO:machine: AMD64
2024-02-08 01:50:01,974:INFO:platform: Windows-10-10.0.19045-SP0
2024-02-08 01:50:01,974:INFO:Memory: svmem(total=16856182784, available=4107190272, percent=75.6, used=12748992512, free=4107190272)
2024-02-08 01:50:01,974:INFO:Physical Core: 4
2024-02-08 01:50:01,974:INFO:Logical Core: 8
2024-02-08 01:50:01,974:INFO:Checking libraries
2024-02-08 01:50:01,974:INFO:System:
2024-02-08 01:50:01,974:INFO:    python: 3.10.9 | packaged by conda-forge | (main, Jan 11 2023, 15:15:40) [MSC v.1916 64 bit (AMD64)]
2024-02-08 01:50:01,975:INFO:executable: C:\ProgramData\miniconda3\python.exe
2024-02-08 01:50:01,975:INFO:   machine: Windows-10-10.0.19045-SP0
2024-02-08 01:50:01,975:INFO:PyCaret required dependencies:
2024-02-08 01:50:01,975:INFO:                 pip: 22.3.1
2024-02-08 01:50:01,975:INFO:          setuptools: 65.6.3
2024-02-08 01:50:01,975:INFO:             pycaret: 3.2.0
2024-02-08 01:50:01,975:INFO:             IPython: 8.20.0
2024-02-08 01:50:01,975:INFO:          ipywidgets: 8.0.4
2024-02-08 01:50:01,975:INFO:                tqdm: 4.64.1
2024-02-08 01:50:01,975:INFO:               numpy: 1.25.2
2024-02-08 01:50:01,975:INFO:              pandas: 1.5.3
2024-02-08 01:50:01,975:INFO:              jinja2: 3.1.3
2024-02-08 01:50:01,975:INFO:               scipy: 1.10.1
2024-02-08 01:50:01,975:INFO:              joblib: 1.3.2
2024-02-08 01:50:01,975:INFO:             sklearn: 1.2.2
2024-02-08 01:50:01,975:INFO:                pyod: 1.1.2
2024-02-08 01:50:01,975:INFO:            imblearn: 0.12.0
2024-02-08 01:50:01,975:INFO:   category_encoders: 2.6.3
2024-02-08 01:50:01,975:INFO:            lightgbm: 4.3.0
2024-02-08 01:50:01,975:INFO:               numba: 0.59.0
2024-02-08 01:50:01,975:INFO:            requests: 2.31.0
2024-02-08 01:50:01,975:INFO:          matplotlib: 3.6.0
2024-02-08 01:50:01,975:INFO:          scikitplot: 0.3.7
2024-02-08 01:50:01,975:INFO:         yellowbrick: 1.5
2024-02-08 01:50:01,975:INFO:              plotly: 5.18.0
2024-02-08 01:50:01,975:INFO:    plotly-resampler: Not installed
2024-02-08 01:50:01,975:INFO:             kaleido: 0.2.1
2024-02-08 01:50:01,975:INFO:           schemdraw: 0.15
2024-02-08 01:50:01,975:INFO:         statsmodels: 0.14.1
2024-02-08 01:50:01,976:INFO:              sktime: 0.21.1
2024-02-08 01:50:01,976:INFO:               tbats: 1.1.3
2024-02-08 01:50:01,976:INFO:            pmdarima: 2.0.4
2024-02-08 01:50:01,976:INFO:              psutil: 5.9.0
2024-02-08 01:50:01,976:INFO:          markupsafe: 2.1.3
2024-02-08 01:50:01,976:INFO:             pickle5: Not installed
2024-02-08 01:50:01,976:INFO:         cloudpickle: 3.0.0
2024-02-08 01:50:01,976:INFO:         deprecation: 2.1.0
2024-02-08 01:50:01,976:INFO:              xxhash: 3.4.1
2024-02-08 01:50:01,976:INFO:           wurlitzer: Not installed
2024-02-08 01:50:01,976:INFO:PyCaret optional dependencies:
2024-02-08 01:50:01,976:INFO:                shap: 0.44.1
2024-02-08 01:50:01,976:INFO:           interpret: Not installed
2024-02-08 01:50:01,976:INFO:                umap: Not installed
2024-02-08 01:50:01,976:INFO:     ydata_profiling: Not installed
2024-02-08 01:50:01,976:INFO:  explainerdashboard: 0.4.5
2024-02-08 01:50:01,976:INFO:             autoviz: Not installed
2024-02-08 01:50:01,976:INFO:           fairlearn: Not installed
2024-02-08 01:50:01,976:INFO:          deepchecks: Not installed
2024-02-08 01:50:01,976:INFO:             xgboost: Not installed
2024-02-08 01:50:01,976:INFO:            catboost: 1.2.2
2024-02-08 01:50:01,977:INFO:              kmodes: Not installed
2024-02-08 01:50:01,977:INFO:             mlxtend: Not installed
2024-02-08 01:50:01,977:INFO:       statsforecast: Not installed
2024-02-08 01:50:01,977:INFO:        tune_sklearn: Not installed
2024-02-08 01:50:01,977:INFO:                 ray: Not installed
2024-02-08 01:50:01,977:INFO:            hyperopt: Not installed
2024-02-08 01:50:01,977:INFO:              optuna: Not installed
2024-02-08 01:50:01,977:INFO:               skopt: Not installed
2024-02-08 01:50:01,977:INFO:              mlflow: 2.10.0
2024-02-08 01:50:01,977:INFO:              gradio: Not installed
2024-02-08 01:50:01,977:INFO:             fastapi: Not installed
2024-02-08 01:50:01,977:INFO:             uvicorn: Not installed
2024-02-08 01:50:01,977:INFO:              m2cgen: Not installed
2024-02-08 01:50:01,977:INFO:           evidently: Not installed
2024-02-08 01:50:01,977:INFO:               fugue: Not installed
2024-02-08 01:50:01,977:INFO:           streamlit: Not installed
2024-02-08 01:50:01,977:INFO:             prophet: Not installed
2024-02-08 01:50:01,977:INFO:None
2024-02-08 01:50:01,977:INFO:Set up data.
2024-02-08 01:50:02,170:INFO:Set up folding strategy.
2024-02-08 01:50:02,170:INFO:Set up train/test split.
2024-02-08 01:50:02,304:INFO:Set up index.
2024-02-08 01:50:02,311:INFO:Assigning column types.
2024-02-08 01:50:02,423:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-02-08 01:50:02,471:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-08 01:50:02,471:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-02-08 01:50:02,507:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-08 01:50:02,508:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-08 01:50:02,566:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-08 01:50:02,567:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-02-08 01:50:02,602:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-08 01:50:02,602:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-08 01:50:02,603:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-02-08 01:50:02,656:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-02-08 01:50:02,687:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-08 01:50:02,687:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-08 01:50:02,742:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-02-08 01:50:02,788:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-08 01:50:02,788:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-08 01:50:02,789:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-02-08 01:50:02,861:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-08 01:50:02,861:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-08 01:50:02,957:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-08 01:50:02,958:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-08 01:50:02,960:INFO:Preparing preprocessing pipeline...
2024-02-08 01:50:02,973:INFO:Set up simple imputation.
2024-02-08 01:50:02,974:INFO:Set up imbalanced handling.
2024-02-08 01:50:02,974:INFO:Set up feature selection.
2024-02-08 01:50:03,058:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-08 01:50:03,059:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-08 01:50:07,244:INFO:Finished creating preprocessing pipeline.
2024-02-08 01:50:07,263:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\user\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id', 'last_date_of_month_7',
                                             'last_date_of_month_8', 'arpu_6',
                                             'arpu_7', 'arpu_8', 'onnet_mou_6',
                                             'onnet_mou_7', 'onnet_mou_8',
                                             'offnet_mou_6', 'offnet_mou_7',
                                             'offnet_mou_8', 'roam_ic_mou_6',
                                             'roam_ic_mou_7', 'roam_i...
                                                                                         learning_rate=0.1,
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=None,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=25,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2024-02-08 01:50:07,263:INFO:Creating final display dataframe.
2024-02-08 01:50:08,785:INFO:Setup _display_container:                     Description              Value
0                    Session id                689
1                        Target  churn_probability
2                   Target type             Binary
3           Original data shape       (69999, 129)
4        Transformed data shape       (109014, 26)
5   Transformed train set shape        (88014, 26)
6    Transformed test set shape        (21000, 26)
7              Numeric features                128
8                    Preprocess               True
9               Imputation type             simple
10           Numeric imputation               mean
11       Categorical imputation               mode
12                Fix imbalance               True
13         Fix imbalance method              SMOTE
14            Feature selection               True
15     Feature selection method            classic
16  Feature selection estimator           lightgbm
17  Number of features selected                0.2
18               Fold Generator    StratifiedKFold
19                  Fold Number                 10
20                     CPU Jobs                 -1
21                      Use GPU              False
22               Log Experiment              False
23              Experiment Name   clf-default-name
24                          USI               e8f0
2024-02-08 01:50:08,872:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-08 01:50:08,873:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-08 01:50:08,943:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-08 01:50:08,944:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-08 01:50:08,944:INFO:setup() successfully completed in 7.0s...............
2024-02-08 01:50:59,762:INFO:PyCaret ClassificationExperiment
2024-02-08 01:50:59,762:INFO:Logging name: clf-default-name
2024-02-08 01:50:59,762:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-02-08 01:50:59,762:INFO:version 3.2.0
2024-02-08 01:50:59,763:INFO:Initializing setup()
2024-02-08 01:50:59,763:INFO:self.USI: e2a0
2024-02-08 01:50:59,763:INFO:self._variable_keys: {'seed', 'y_test', 'is_multiclass', 'exp_name_log', 'gpu_param', 'fold_groups_param', 'target_param', 'data', '_available_plots', 'exp_id', 'gpu_n_jobs_param', 'memory', 'logging_param', 'fold_generator', 'pipeline', 'X_test', 'n_jobs_param', 'y', 'fold_shuffle_param', 'USI', 'y_train', 'html_param', 'log_plots_param', 'X', '_ml_usecase', 'fix_imbalance', 'idx', 'X_train'}
2024-02-08 01:50:59,763:INFO:Checking environment
2024-02-08 01:50:59,763:INFO:python_version: 3.10.9
2024-02-08 01:50:59,763:INFO:python_build: ('main', 'Jan 11 2023 15:15:40')
2024-02-08 01:50:59,763:INFO:machine: AMD64
2024-02-08 01:50:59,763:INFO:platform: Windows-10-10.0.19045-SP0
2024-02-08 01:50:59,763:INFO:Memory: svmem(total=16856182784, available=4023656448, percent=76.1, used=12832526336, free=4023656448)
2024-02-08 01:50:59,763:INFO:Physical Core: 4
2024-02-08 01:50:59,763:INFO:Logical Core: 8
2024-02-08 01:50:59,763:INFO:Checking libraries
2024-02-08 01:50:59,763:INFO:System:
2024-02-08 01:50:59,764:INFO:    python: 3.10.9 | packaged by conda-forge | (main, Jan 11 2023, 15:15:40) [MSC v.1916 64 bit (AMD64)]
2024-02-08 01:50:59,764:INFO:executable: C:\ProgramData\miniconda3\python.exe
2024-02-08 01:50:59,764:INFO:   machine: Windows-10-10.0.19045-SP0
2024-02-08 01:50:59,764:INFO:PyCaret required dependencies:
2024-02-08 01:50:59,764:INFO:                 pip: 22.3.1
2024-02-08 01:50:59,764:INFO:          setuptools: 65.6.3
2024-02-08 01:50:59,764:INFO:             pycaret: 3.2.0
2024-02-08 01:50:59,764:INFO:             IPython: 8.20.0
2024-02-08 01:50:59,764:INFO:          ipywidgets: 8.0.4
2024-02-08 01:50:59,764:INFO:                tqdm: 4.64.1
2024-02-08 01:50:59,764:INFO:               numpy: 1.25.2
2024-02-08 01:50:59,764:INFO:              pandas: 1.5.3
2024-02-08 01:50:59,764:INFO:              jinja2: 3.1.3
2024-02-08 01:50:59,764:INFO:               scipy: 1.10.1
2024-02-08 01:50:59,764:INFO:              joblib: 1.3.2
2024-02-08 01:50:59,764:INFO:             sklearn: 1.2.2
2024-02-08 01:50:59,764:INFO:                pyod: 1.1.2
2024-02-08 01:50:59,764:INFO:            imblearn: 0.12.0
2024-02-08 01:50:59,764:INFO:   category_encoders: 2.6.3
2024-02-08 01:50:59,765:INFO:            lightgbm: 4.3.0
2024-02-08 01:50:59,765:INFO:               numba: 0.59.0
2024-02-08 01:50:59,765:INFO:            requests: 2.31.0
2024-02-08 01:50:59,765:INFO:          matplotlib: 3.6.0
2024-02-08 01:50:59,765:INFO:          scikitplot: 0.3.7
2024-02-08 01:50:59,765:INFO:         yellowbrick: 1.5
2024-02-08 01:50:59,765:INFO:              plotly: 5.18.0
2024-02-08 01:50:59,765:INFO:    plotly-resampler: Not installed
2024-02-08 01:50:59,765:INFO:             kaleido: 0.2.1
2024-02-08 01:50:59,765:INFO:           schemdraw: 0.15
2024-02-08 01:50:59,765:INFO:         statsmodels: 0.14.1
2024-02-08 01:50:59,765:INFO:              sktime: 0.21.1
2024-02-08 01:50:59,765:INFO:               tbats: 1.1.3
2024-02-08 01:50:59,765:INFO:            pmdarima: 2.0.4
2024-02-08 01:50:59,765:INFO:              psutil: 5.9.0
2024-02-08 01:50:59,765:INFO:          markupsafe: 2.1.3
2024-02-08 01:50:59,765:INFO:             pickle5: Not installed
2024-02-08 01:50:59,765:INFO:         cloudpickle: 3.0.0
2024-02-08 01:50:59,765:INFO:         deprecation: 2.1.0
2024-02-08 01:50:59,765:INFO:              xxhash: 3.4.1
2024-02-08 01:50:59,765:INFO:           wurlitzer: Not installed
2024-02-08 01:50:59,765:INFO:PyCaret optional dependencies:
2024-02-08 01:50:59,765:INFO:                shap: 0.44.1
2024-02-08 01:50:59,765:INFO:           interpret: Not installed
2024-02-08 01:50:59,766:INFO:                umap: Not installed
2024-02-08 01:50:59,766:INFO:     ydata_profiling: Not installed
2024-02-08 01:50:59,766:INFO:  explainerdashboard: 0.4.5
2024-02-08 01:50:59,766:INFO:             autoviz: Not installed
2024-02-08 01:50:59,766:INFO:           fairlearn: Not installed
2024-02-08 01:50:59,766:INFO:          deepchecks: Not installed
2024-02-08 01:50:59,766:INFO:             xgboost: Not installed
2024-02-08 01:50:59,766:INFO:            catboost: 1.2.2
2024-02-08 01:50:59,766:INFO:              kmodes: Not installed
2024-02-08 01:50:59,766:INFO:             mlxtend: Not installed
2024-02-08 01:50:59,766:INFO:       statsforecast: Not installed
2024-02-08 01:50:59,766:INFO:        tune_sklearn: Not installed
2024-02-08 01:50:59,766:INFO:                 ray: Not installed
2024-02-08 01:50:59,766:INFO:            hyperopt: Not installed
2024-02-08 01:50:59,766:INFO:              optuna: Not installed
2024-02-08 01:50:59,766:INFO:               skopt: Not installed
2024-02-08 01:50:59,766:INFO:              mlflow: 2.10.0
2024-02-08 01:50:59,766:INFO:              gradio: Not installed
2024-02-08 01:50:59,766:INFO:             fastapi: Not installed
2024-02-08 01:50:59,766:INFO:             uvicorn: Not installed
2024-02-08 01:50:59,766:INFO:              m2cgen: Not installed
2024-02-08 01:50:59,766:INFO:           evidently: Not installed
2024-02-08 01:50:59,766:INFO:               fugue: Not installed
2024-02-08 01:50:59,766:INFO:           streamlit: Not installed
2024-02-08 01:50:59,766:INFO:             prophet: Not installed
2024-02-08 01:50:59,766:INFO:None
2024-02-08 01:50:59,767:INFO:Set up data.
2024-02-08 01:50:59,973:INFO:Set up folding strategy.
2024-02-08 01:50:59,973:INFO:Set up train/test split.
2024-02-08 01:51:00,195:INFO:Set up index.
2024-02-08 01:51:00,203:INFO:Assigning column types.
2024-02-08 01:51:00,335:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-02-08 01:51:00,378:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-08 01:51:00,379:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-02-08 01:51:00,413:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-08 01:51:00,414:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-08 01:51:00,457:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-08 01:51:00,458:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-02-08 01:51:00,487:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-08 01:51:00,487:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-08 01:51:00,487:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-02-08 01:51:00,539:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-02-08 01:51:00,572:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-08 01:51:00,572:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-08 01:51:00,622:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-02-08 01:51:00,657:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-08 01:51:00,657:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-08 01:51:00,658:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-02-08 01:51:00,751:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-08 01:51:00,751:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-08 01:51:00,845:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-08 01:51:00,845:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-08 01:51:00,847:INFO:Preparing preprocessing pipeline...
2024-02-08 01:51:00,861:INFO:Set up simple imputation.
2024-02-08 01:51:00,861:INFO:Set up removing multicollinearity.
2024-02-08 01:51:00,861:INFO:Set up imbalanced handling.
2024-02-08 01:51:00,861:INFO:Set up feature selection.
2024-02-08 01:51:00,939:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-08 01:51:00,939:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-08 01:51:06,616:INFO:Finished creating preprocessing pipeline.
2024-02-08 01:51:06,631:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\user\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id', 'last_date_of_month_7',
                                             'last_date_of_month_8', 'arpu_6',
                                             'arpu_7', 'arpu_8', 'onnet_mou_6',
                                             'onnet_mou_7', 'onnet_mou_8',
                                             'offnet_mou_6', 'offnet_mou_7',
                                             'offnet_mou_8', 'roam_ic_mou_6',
                                             'roam_ic_mou_7', 'roam_i...
                                                                                         learning_rate=0.1,
                                                                                         max_depth=-1,
                                                                                         min_child_samples=20,
                                                                                         min_child_weight=0.001,
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=None,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=25,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf)))],
         verbose=False)
2024-02-08 01:51:06,631:INFO:Creating final display dataframe.
2024-02-08 01:51:08,105:INFO:Setup _display_container:                     Description              Value
0                    Session id               6300
1                        Target  churn_probability
2                   Target type             Binary
3           Original data shape       (69999, 129)
4        Transformed data shape       (109014, 26)
5   Transformed train set shape        (88014, 26)
6    Transformed test set shape        (21000, 26)
7              Numeric features                128
8                    Preprocess               True
9               Imputation type             simple
10           Numeric imputation               mean
11       Categorical imputation               mode
12     Remove multicollinearity               True
13  Multicollinearity threshold                0.9
14                Fix imbalance               True
15         Fix imbalance method              SMOTE
16            Feature selection               True
17     Feature selection method            classic
18  Feature selection estimator           lightgbm
19  Number of features selected                0.2
20               Fold Generator    StratifiedKFold
21                  Fold Number                 10
22                     CPU Jobs                 -1
23                      Use GPU              False
24               Log Experiment              False
25              Experiment Name   clf-default-name
26                          USI               e2a0
2024-02-08 01:51:08,205:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-08 01:51:08,206:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-08 01:51:08,294:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-08 01:51:08,294:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-08 01:51:08,295:INFO:setup() successfully completed in 8.57s...............
2024-02-08 01:53:29,045:INFO:Initializing compare_models()
2024-02-08 01:53:29,045:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, include=None, fold=None, round=4, cross_validation=True, sort=Accurancy, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accurancy', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-02-08 01:53:29,045:INFO:Checking exceptions
2024-02-08 01:54:01,823:INFO:Initializing compare_models()
2024-02-08 01:54:01,824:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-02-08 01:54:01,824:INFO:Checking exceptions
2024-02-08 01:54:01,899:INFO:Preparing display monitor
2024-02-08 01:54:01,934:INFO:Initializing Logistic Regression
2024-02-08 01:54:01,934:INFO:Total runtime is 0.0 minutes
2024-02-08 01:54:01,939:INFO:SubProcess create_model() called ==================================
2024-02-08 01:54:01,940:INFO:Initializing create_model()
2024-02-08 01:54:01,940:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E27ABD60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 01:54:01,940:INFO:Checking exceptions
2024-02-08 01:54:01,940:INFO:Importing libraries
2024-02-08 01:54:01,940:INFO:Copying training dataset
2024-02-08 01:54:02,057:INFO:Defining folds
2024-02-08 01:54:02,057:INFO:Declaring metric variables
2024-02-08 01:54:02,061:INFO:Importing untrained model
2024-02-08 01:54:02,065:INFO:Logistic Regression Imported successfully
2024-02-08 01:54:02,074:INFO:Starting cross validation
2024-02-08 01:54:02,086:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 01:54:42,879:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 01:54:43,612:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 01:54:43,917:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 01:54:44,566:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 01:54:55,285:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 01:54:56,560:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 01:54:58,636:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 01:55:00,555:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 01:55:09,490:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 01:55:09,822:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 01:55:10,014:INFO:Calculating mean and std
2024-02-08 01:55:10,016:INFO:Creating metrics dataframe
2024-02-08 01:55:10,027:INFO:Uploading results into container
2024-02-08 01:55:10,029:INFO:Uploading model into container now
2024-02-08 01:55:10,034:INFO:_master_model_container: 1
2024-02-08 01:55:10,034:INFO:_display_container: 2
2024-02-08 01:55:10,036:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=6300, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-02-08 01:55:10,036:INFO:create_model() successfully completed......................................
2024-02-08 01:55:10,282:INFO:SubProcess create_model() end ==================================
2024-02-08 01:55:10,283:INFO:Creating metrics dataframe
2024-02-08 01:55:10,296:INFO:Initializing K Neighbors Classifier
2024-02-08 01:55:10,296:INFO:Total runtime is 1.1393545349438985 minutes
2024-02-08 01:55:10,300:INFO:SubProcess create_model() called ==================================
2024-02-08 01:55:10,301:INFO:Initializing create_model()
2024-02-08 01:55:10,301:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E27ABD60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 01:55:10,301:INFO:Checking exceptions
2024-02-08 01:55:10,301:INFO:Importing libraries
2024-02-08 01:55:10,302:INFO:Copying training dataset
2024-02-08 01:55:10,429:INFO:Defining folds
2024-02-08 01:55:10,429:INFO:Declaring metric variables
2024-02-08 01:55:10,433:INFO:Importing untrained model
2024-02-08 01:55:10,438:INFO:K Neighbors Classifier Imported successfully
2024-02-08 01:55:10,448:INFO:Starting cross validation
2024-02-08 01:55:10,473:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 01:56:08,164:INFO:Calculating mean and std
2024-02-08 01:56:08,167:INFO:Creating metrics dataframe
2024-02-08 01:56:08,179:INFO:Uploading results into container
2024-02-08 01:56:08,181:INFO:Uploading model into container now
2024-02-08 01:56:08,182:INFO:_master_model_container: 2
2024-02-08 01:56:08,182:INFO:_display_container: 2
2024-02-08 01:56:08,183:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-02-08 01:56:08,183:INFO:create_model() successfully completed......................................
2024-02-08 01:56:08,339:INFO:SubProcess create_model() end ==================================
2024-02-08 01:56:08,340:INFO:Creating metrics dataframe
2024-02-08 01:56:08,351:INFO:Initializing Naive Bayes
2024-02-08 01:56:08,351:INFO:Total runtime is 2.1069378018379212 minutes
2024-02-08 01:56:08,356:INFO:SubProcess create_model() called ==================================
2024-02-08 01:56:08,357:INFO:Initializing create_model()
2024-02-08 01:56:08,357:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E27ABD60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 01:56:08,358:INFO:Checking exceptions
2024-02-08 01:56:08,358:INFO:Importing libraries
2024-02-08 01:56:08,358:INFO:Copying training dataset
2024-02-08 01:56:08,463:INFO:Defining folds
2024-02-08 01:56:08,464:INFO:Declaring metric variables
2024-02-08 01:56:08,475:INFO:Importing untrained model
2024-02-08 01:56:08,484:INFO:Naive Bayes Imported successfully
2024-02-08 01:56:08,493:INFO:Starting cross validation
2024-02-08 01:56:08,506:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 01:56:40,491:INFO:Calculating mean and std
2024-02-08 01:56:40,494:INFO:Creating metrics dataframe
2024-02-08 01:56:40,504:INFO:Uploading results into container
2024-02-08 01:56:40,506:INFO:Uploading model into container now
2024-02-08 01:56:40,507:INFO:_master_model_container: 3
2024-02-08 01:56:40,507:INFO:_display_container: 2
2024-02-08 01:56:40,509:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-02-08 01:56:40,509:INFO:create_model() successfully completed......................................
2024-02-08 01:56:40,641:INFO:SubProcess create_model() end ==================================
2024-02-08 01:56:40,642:INFO:Creating metrics dataframe
2024-02-08 01:56:40,652:INFO:Initializing Decision Tree Classifier
2024-02-08 01:56:40,652:INFO:Total runtime is 2.6452887574831645 minutes
2024-02-08 01:56:40,656:INFO:SubProcess create_model() called ==================================
2024-02-08 01:56:40,656:INFO:Initializing create_model()
2024-02-08 01:56:40,656:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E27ABD60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 01:56:40,656:INFO:Checking exceptions
2024-02-08 01:56:40,656:INFO:Importing libraries
2024-02-08 01:56:40,656:INFO:Copying training dataset
2024-02-08 01:56:40,744:INFO:Defining folds
2024-02-08 01:56:40,744:INFO:Declaring metric variables
2024-02-08 01:56:40,750:INFO:Importing untrained model
2024-02-08 01:56:40,758:INFO:Decision Tree Classifier Imported successfully
2024-02-08 01:56:40,771:INFO:Starting cross validation
2024-02-08 01:56:40,789:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 01:57:19,150:INFO:Calculating mean and std
2024-02-08 01:57:19,151:INFO:Creating metrics dataframe
2024-02-08 01:57:19,157:INFO:Uploading results into container
2024-02-08 01:57:19,158:INFO:Uploading model into container now
2024-02-08 01:57:19,159:INFO:_master_model_container: 4
2024-02-08 01:57:19,159:INFO:_display_container: 2
2024-02-08 01:57:19,160:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=6300, splitter='best')
2024-02-08 01:57:19,160:INFO:create_model() successfully completed......................................
2024-02-08 01:57:19,334:INFO:SubProcess create_model() end ==================================
2024-02-08 01:57:19,334:INFO:Creating metrics dataframe
2024-02-08 01:57:19,348:INFO:Initializing SVM - Linear Kernel
2024-02-08 01:57:19,348:INFO:Total runtime is 3.2902221202850344 minutes
2024-02-08 01:57:19,356:INFO:SubProcess create_model() called ==================================
2024-02-08 01:57:19,357:INFO:Initializing create_model()
2024-02-08 01:57:19,357:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E27ABD60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 01:57:19,357:INFO:Checking exceptions
2024-02-08 01:57:19,357:INFO:Importing libraries
2024-02-08 01:57:19,358:INFO:Copying training dataset
2024-02-08 01:57:19,498:INFO:Defining folds
2024-02-08 01:57:19,498:INFO:Declaring metric variables
2024-02-08 01:57:19,505:INFO:Importing untrained model
2024-02-08 01:57:19,512:INFO:SVM - Linear Kernel Imported successfully
2024-02-08 01:57:19,526:INFO:Starting cross validation
2024-02-08 01:57:19,543:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 01:57:50,432:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-08 01:57:50,709:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-08 01:57:55,178:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-08 01:57:55,380:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-08 01:57:55,785:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-08 01:57:56,169:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-08 01:57:56,604:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-08 01:57:56,792:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-08 01:58:06,484:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-08 01:58:07,303:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-08 01:58:07,434:INFO:Calculating mean and std
2024-02-08 01:58:07,438:INFO:Creating metrics dataframe
2024-02-08 01:58:07,447:INFO:Uploading results into container
2024-02-08 01:58:07,448:INFO:Uploading model into container now
2024-02-08 01:58:07,449:INFO:_master_model_container: 5
2024-02-08 01:58:07,449:INFO:_display_container: 2
2024-02-08 01:58:07,450:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=6300, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-02-08 01:58:07,451:INFO:create_model() successfully completed......................................
2024-02-08 01:58:07,636:INFO:SubProcess create_model() end ==================================
2024-02-08 01:58:07,636:INFO:Creating metrics dataframe
2024-02-08 01:58:07,653:INFO:Initializing Ridge Classifier
2024-02-08 01:58:07,654:INFO:Total runtime is 4.095330123106638 minutes
2024-02-08 01:58:07,661:INFO:SubProcess create_model() called ==================================
2024-02-08 01:58:07,662:INFO:Initializing create_model()
2024-02-08 01:58:07,662:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E27ABD60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 01:58:07,662:INFO:Checking exceptions
2024-02-08 01:58:07,662:INFO:Importing libraries
2024-02-08 01:58:07,662:INFO:Copying training dataset
2024-02-08 01:58:07,797:INFO:Defining folds
2024-02-08 01:58:07,798:INFO:Declaring metric variables
2024-02-08 01:58:07,805:INFO:Importing untrained model
2024-02-08 01:58:07,812:INFO:Ridge Classifier Imported successfully
2024-02-08 01:58:07,827:INFO:Starting cross validation
2024-02-08 01:58:07,846:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 01:58:29,667:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-08 01:58:29,954:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-08 01:58:32,943:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-08 01:58:33,286:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-08 01:58:33,502:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-08 01:58:33,660:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-08 01:58:33,755:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-08 01:58:33,880:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-08 01:58:39,635:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-08 01:58:39,824:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-08 01:58:39,971:INFO:Calculating mean and std
2024-02-08 01:58:39,974:INFO:Creating metrics dataframe
2024-02-08 01:58:39,984:INFO:Uploading results into container
2024-02-08 01:58:39,986:INFO:Uploading model into container now
2024-02-08 01:58:39,987:INFO:_master_model_container: 6
2024-02-08 01:58:39,987:INFO:_display_container: 2
2024-02-08 01:58:39,989:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=6300, solver='auto',
                tol=0.0001)
2024-02-08 01:58:39,989:INFO:create_model() successfully completed......................................
2024-02-08 01:58:40,130:INFO:SubProcess create_model() end ==================================
2024-02-08 01:58:40,130:INFO:Creating metrics dataframe
2024-02-08 01:58:40,152:INFO:Initializing Random Forest Classifier
2024-02-08 01:58:40,152:INFO:Total runtime is 4.636962274710337 minutes
2024-02-08 01:58:40,156:INFO:SubProcess create_model() called ==================================
2024-02-08 01:58:40,156:INFO:Initializing create_model()
2024-02-08 01:58:40,157:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E27ABD60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 01:58:40,157:INFO:Checking exceptions
2024-02-08 01:58:40,157:INFO:Importing libraries
2024-02-08 01:58:40,157:INFO:Copying training dataset
2024-02-08 01:58:40,258:INFO:Defining folds
2024-02-08 01:58:40,258:INFO:Declaring metric variables
2024-02-08 01:58:40,262:INFO:Importing untrained model
2024-02-08 01:58:40,274:INFO:Random Forest Classifier Imported successfully
2024-02-08 01:58:40,291:INFO:Starting cross validation
2024-02-08 01:58:40,309:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 02:00:39,448:INFO:Calculating mean and std
2024-02-08 02:00:39,449:INFO:Creating metrics dataframe
2024-02-08 02:00:39,455:INFO:Uploading results into container
2024-02-08 02:00:39,457:INFO:Uploading model into container now
2024-02-08 02:00:39,459:INFO:_master_model_container: 7
2024-02-08 02:00:39,459:INFO:_display_container: 2
2024-02-08 02:00:39,461:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6300, verbose=0, warm_start=False)
2024-02-08 02:00:39,461:INFO:create_model() successfully completed......................................
2024-02-08 02:00:39,691:INFO:SubProcess create_model() end ==================================
2024-02-08 02:00:39,691:INFO:Creating metrics dataframe
2024-02-08 02:00:39,712:INFO:Initializing Quadratic Discriminant Analysis
2024-02-08 02:00:39,713:INFO:Total runtime is 6.629642323652902 minutes
2024-02-08 02:00:39,719:INFO:SubProcess create_model() called ==================================
2024-02-08 02:00:39,720:INFO:Initializing create_model()
2024-02-08 02:00:39,720:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E27ABD60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:00:39,720:INFO:Checking exceptions
2024-02-08 02:00:39,720:INFO:Importing libraries
2024-02-08 02:00:39,720:INFO:Copying training dataset
2024-02-08 02:00:39,882:INFO:Defining folds
2024-02-08 02:00:39,882:INFO:Declaring metric variables
2024-02-08 02:00:39,890:INFO:Importing untrained model
2024-02-08 02:00:39,898:INFO:Quadratic Discriminant Analysis Imported successfully
2024-02-08 02:00:39,913:INFO:Starting cross validation
2024-02-08 02:00:39,928:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 02:01:14,092:INFO:Calculating mean and std
2024-02-08 02:01:14,095:INFO:Creating metrics dataframe
2024-02-08 02:01:14,105:INFO:Uploading results into container
2024-02-08 02:01:14,107:INFO:Uploading model into container now
2024-02-08 02:01:14,109:INFO:_master_model_container: 8
2024-02-08 02:01:14,109:INFO:_display_container: 2
2024-02-08 02:01:14,111:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-02-08 02:01:14,111:INFO:create_model() successfully completed......................................
2024-02-08 02:01:14,265:INFO:SubProcess create_model() end ==================================
2024-02-08 02:01:14,265:INFO:Creating metrics dataframe
2024-02-08 02:01:14,288:INFO:Initializing Ada Boost Classifier
2024-02-08 02:01:14,289:INFO:Total runtime is 7.205918296178181 minutes
2024-02-08 02:01:14,292:INFO:SubProcess create_model() called ==================================
2024-02-08 02:01:14,293:INFO:Initializing create_model()
2024-02-08 02:01:14,293:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E27ABD60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:01:14,293:INFO:Checking exceptions
2024-02-08 02:01:14,293:INFO:Importing libraries
2024-02-08 02:01:14,293:INFO:Copying training dataset
2024-02-08 02:01:14,386:INFO:Defining folds
2024-02-08 02:01:14,386:INFO:Declaring metric variables
2024-02-08 02:01:14,391:INFO:Importing untrained model
2024-02-08 02:01:14,407:INFO:Ada Boost Classifier Imported successfully
2024-02-08 02:01:14,420:INFO:Starting cross validation
2024-02-08 02:01:14,433:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 02:02:27,496:INFO:Calculating mean and std
2024-02-08 02:02:27,499:INFO:Creating metrics dataframe
2024-02-08 02:02:27,509:INFO:Uploading results into container
2024-02-08 02:02:27,511:INFO:Uploading model into container now
2024-02-08 02:02:27,513:INFO:_master_model_container: 9
2024-02-08 02:02:27,513:INFO:_display_container: 2
2024-02-08 02:02:27,515:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=6300)
2024-02-08 02:02:27,516:INFO:create_model() successfully completed......................................
2024-02-08 02:02:27,672:INFO:SubProcess create_model() end ==================================
2024-02-08 02:02:27,672:INFO:Creating metrics dataframe
2024-02-08 02:02:27,684:INFO:Initializing Gradient Boosting Classifier
2024-02-08 02:02:27,684:INFO:Total runtime is 8.429156816005706 minutes
2024-02-08 02:02:27,688:INFO:SubProcess create_model() called ==================================
2024-02-08 02:02:27,689:INFO:Initializing create_model()
2024-02-08 02:02:27,689:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E27ABD60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:02:27,689:INFO:Checking exceptions
2024-02-08 02:02:27,689:INFO:Importing libraries
2024-02-08 02:02:27,690:INFO:Copying training dataset
2024-02-08 02:02:27,791:INFO:Defining folds
2024-02-08 02:02:27,792:INFO:Declaring metric variables
2024-02-08 02:02:27,796:INFO:Importing untrained model
2024-02-08 02:02:27,805:INFO:Gradient Boosting Classifier Imported successfully
2024-02-08 02:02:27,813:INFO:Starting cross validation
2024-02-08 02:02:27,826:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 02:06:07,769:INFO:Calculating mean and std
2024-02-08 02:06:07,773:INFO:Creating metrics dataframe
2024-02-08 02:06:07,783:INFO:Uploading results into container
2024-02-08 02:06:07,785:INFO:Uploading model into container now
2024-02-08 02:06:07,785:INFO:_master_model_container: 10
2024-02-08 02:06:07,786:INFO:_display_container: 2
2024-02-08 02:06:07,787:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6300, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-02-08 02:06:07,787:INFO:create_model() successfully completed......................................
2024-02-08 02:06:07,976:INFO:SubProcess create_model() end ==================================
2024-02-08 02:06:07,977:INFO:Creating metrics dataframe
2024-02-08 02:06:07,997:INFO:Initializing Linear Discriminant Analysis
2024-02-08 02:06:07,997:INFO:Total runtime is 12.101041110356649 minutes
2024-02-08 02:06:08,002:INFO:SubProcess create_model() called ==================================
2024-02-08 02:06:08,003:INFO:Initializing create_model()
2024-02-08 02:06:08,003:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E27ABD60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:06:08,003:INFO:Checking exceptions
2024-02-08 02:06:08,003:INFO:Importing libraries
2024-02-08 02:06:08,003:INFO:Copying training dataset
2024-02-08 02:06:08,139:INFO:Defining folds
2024-02-08 02:06:08,139:INFO:Declaring metric variables
2024-02-08 02:06:08,144:INFO:Importing untrained model
2024-02-08 02:06:08,152:INFO:Linear Discriminant Analysis Imported successfully
2024-02-08 02:06:08,161:INFO:Starting cross validation
2024-02-08 02:06:08,173:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 02:06:40,572:INFO:Calculating mean and std
2024-02-08 02:06:40,575:INFO:Creating metrics dataframe
2024-02-08 02:06:40,584:INFO:Uploading results into container
2024-02-08 02:06:40,588:INFO:Uploading model into container now
2024-02-08 02:06:40,588:INFO:_master_model_container: 11
2024-02-08 02:06:40,590:INFO:_display_container: 2
2024-02-08 02:06:40,590:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-02-08 02:06:40,590:INFO:create_model() successfully completed......................................
2024-02-08 02:06:40,763:INFO:SubProcess create_model() end ==================================
2024-02-08 02:06:40,763:INFO:Creating metrics dataframe
2024-02-08 02:06:40,781:INFO:Initializing Extra Trees Classifier
2024-02-08 02:06:40,782:INFO:Total runtime is 12.647458040714264 minutes
2024-02-08 02:06:40,788:INFO:SubProcess create_model() called ==================================
2024-02-08 02:06:40,789:INFO:Initializing create_model()
2024-02-08 02:06:40,789:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E27ABD60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:06:40,789:INFO:Checking exceptions
2024-02-08 02:06:40,790:INFO:Importing libraries
2024-02-08 02:06:40,790:INFO:Copying training dataset
2024-02-08 02:06:40,969:INFO:Defining folds
2024-02-08 02:06:40,969:INFO:Declaring metric variables
2024-02-08 02:06:40,977:INFO:Importing untrained model
2024-02-08 02:06:40,985:INFO:Extra Trees Classifier Imported successfully
2024-02-08 02:06:40,997:INFO:Starting cross validation
2024-02-08 02:06:41,013:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 02:07:48,590:INFO:Calculating mean and std
2024-02-08 02:07:48,593:INFO:Creating metrics dataframe
2024-02-08 02:07:48,603:INFO:Uploading results into container
2024-02-08 02:07:48,605:INFO:Uploading model into container now
2024-02-08 02:07:48,605:INFO:_master_model_container: 12
2024-02-08 02:07:48,606:INFO:_display_container: 2
2024-02-08 02:07:48,607:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=6300, verbose=0, warm_start=False)
2024-02-08 02:07:48,608:INFO:create_model() successfully completed......................................
2024-02-08 02:07:48,817:INFO:SubProcess create_model() end ==================================
2024-02-08 02:07:48,818:INFO:Creating metrics dataframe
2024-02-08 02:07:48,852:INFO:Initializing Light Gradient Boosting Machine
2024-02-08 02:07:48,852:INFO:Total runtime is 13.781957828998566 minutes
2024-02-08 02:07:48,860:INFO:SubProcess create_model() called ==================================
2024-02-08 02:07:48,861:INFO:Initializing create_model()
2024-02-08 02:07:48,861:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E27ABD60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:07:48,861:INFO:Checking exceptions
2024-02-08 02:07:48,862:INFO:Importing libraries
2024-02-08 02:07:48,862:INFO:Copying training dataset
2024-02-08 02:07:49,038:INFO:Defining folds
2024-02-08 02:07:49,038:INFO:Declaring metric variables
2024-02-08 02:07:49,044:INFO:Importing untrained model
2024-02-08 02:07:49,055:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-08 02:07:49,066:INFO:Starting cross validation
2024-02-08 02:07:49,080:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 02:08:30,402:INFO:Calculating mean and std
2024-02-08 02:08:30,404:INFO:Creating metrics dataframe
2024-02-08 02:08:30,413:INFO:Uploading results into container
2024-02-08 02:08:30,414:INFO:Uploading model into container now
2024-02-08 02:08:30,415:INFO:_master_model_container: 13
2024-02-08 02:08:30,416:INFO:_display_container: 2
2024-02-08 02:08:30,417:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6300, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-02-08 02:08:30,418:INFO:create_model() successfully completed......................................
2024-02-08 02:08:30,622:INFO:SubProcess create_model() end ==================================
2024-02-08 02:08:30,622:INFO:Creating metrics dataframe
2024-02-08 02:08:30,643:INFO:Initializing CatBoost Classifier
2024-02-08 02:08:30,643:INFO:Total runtime is 14.478477700551352 minutes
2024-02-08 02:08:30,649:INFO:SubProcess create_model() called ==================================
2024-02-08 02:08:30,650:INFO:Initializing create_model()
2024-02-08 02:08:30,650:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E27ABD60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:08:30,650:INFO:Checking exceptions
2024-02-08 02:08:30,651:INFO:Importing libraries
2024-02-08 02:08:30,651:INFO:Copying training dataset
2024-02-08 02:08:30,822:INFO:Defining folds
2024-02-08 02:08:30,822:INFO:Declaring metric variables
2024-02-08 02:08:30,830:INFO:Importing untrained model
2024-02-08 02:08:30,837:INFO:CatBoost Classifier Imported successfully
2024-02-08 02:08:30,852:INFO:Starting cross validation
2024-02-08 02:08:30,869:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 02:11:09,952:INFO:Calculating mean and std
2024-02-08 02:11:09,954:INFO:Creating metrics dataframe
2024-02-08 02:11:09,961:INFO:Uploading results into container
2024-02-08 02:11:09,963:INFO:Uploading model into container now
2024-02-08 02:11:09,964:INFO:_master_model_container: 14
2024-02-08 02:11:09,964:INFO:_display_container: 2
2024-02-08 02:11:09,964:INFO:<catboost.core.CatBoostClassifier object at 0x0000023698F21D80>
2024-02-08 02:11:09,964:INFO:create_model() successfully completed......................................
2024-02-08 02:11:10,160:INFO:SubProcess create_model() end ==================================
2024-02-08 02:11:10,161:INFO:Creating metrics dataframe
2024-02-08 02:11:10,194:INFO:Initializing Dummy Classifier
2024-02-08 02:11:10,195:INFO:Total runtime is 17.137672372659047 minutes
2024-02-08 02:11:10,203:INFO:SubProcess create_model() called ==================================
2024-02-08 02:11:10,204:INFO:Initializing create_model()
2024-02-08 02:11:10,204:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E27ABD60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:11:10,204:INFO:Checking exceptions
2024-02-08 02:11:10,204:INFO:Importing libraries
2024-02-08 02:11:10,204:INFO:Copying training dataset
2024-02-08 02:11:10,395:INFO:Defining folds
2024-02-08 02:11:10,395:INFO:Declaring metric variables
2024-02-08 02:11:10,401:INFO:Importing untrained model
2024-02-08 02:11:10,409:INFO:Dummy Classifier Imported successfully
2024-02-08 02:11:10,425:INFO:Starting cross validation
2024-02-08 02:11:10,441:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 02:11:32,986:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-08 02:11:33,008:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-08 02:11:33,664:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-08 02:11:33,708:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-08 02:11:33,993:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-08 02:11:36,112:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-08 02:11:36,158:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-08 02:11:36,192:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-08 02:11:42,794:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-08 02:11:42,876:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-08 02:11:43,010:INFO:Calculating mean and std
2024-02-08 02:11:43,012:INFO:Creating metrics dataframe
2024-02-08 02:11:43,021:INFO:Uploading results into container
2024-02-08 02:11:43,022:INFO:Uploading model into container now
2024-02-08 02:11:43,022:INFO:_master_model_container: 15
2024-02-08 02:11:43,022:INFO:_display_container: 2
2024-02-08 02:11:43,023:INFO:DummyClassifier(constant=None, random_state=6300, strategy='prior')
2024-02-08 02:11:43,023:INFO:create_model() successfully completed......................................
2024-02-08 02:11:43,216:INFO:SubProcess create_model() end ==================================
2024-02-08 02:11:43,217:INFO:Creating metrics dataframe
2024-02-08 02:11:43,263:INFO:Initializing create_model()
2024-02-08 02:11:43,263:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6300, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:11:43,263:INFO:Checking exceptions
2024-02-08 02:11:43,267:INFO:Importing libraries
2024-02-08 02:11:43,267:INFO:Copying training dataset
2024-02-08 02:11:43,430:INFO:Defining folds
2024-02-08 02:11:43,430:INFO:Declaring metric variables
2024-02-08 02:11:43,430:INFO:Importing untrained model
2024-02-08 02:11:43,430:INFO:Declaring custom model
2024-02-08 02:11:43,431:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-08 02:11:43,448:INFO:Cross validation set to False
2024-02-08 02:11:43,449:INFO:Fitting Model
2024-02-08 02:11:48,263:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 02:11:48,381:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089323 seconds.
2024-02-08 02:11:48,381:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-08 02:11:48,383:INFO:[LightGBM] [Info] Total Bins 31359
2024-02-08 02:11:48,386:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 123
2024-02-08 02:11:48,388:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 02:11:51,047:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 02:11:51,060:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009886 seconds.
2024-02-08 02:11:51,060:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-08 02:11:51,060:INFO:[LightGBM] [Info] Total Bins 6375
2024-02-08 02:11:51,061:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 25
2024-02-08 02:11:51,062:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 02:11:51,533:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6300, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-02-08 02:11:51,533:INFO:create_model() successfully completed......................................
2024-02-08 02:11:51,731:INFO:Initializing create_model()
2024-02-08 02:11:51,732:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=6300, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:11:51,732:INFO:Checking exceptions
2024-02-08 02:11:51,735:INFO:Importing libraries
2024-02-08 02:11:51,738:INFO:Copying training dataset
2024-02-08 02:11:51,891:INFO:Defining folds
2024-02-08 02:11:51,891:INFO:Declaring metric variables
2024-02-08 02:11:51,892:INFO:Importing untrained model
2024-02-08 02:11:51,892:INFO:Declaring custom model
2024-02-08 02:11:51,893:INFO:Extra Trees Classifier Imported successfully
2024-02-08 02:11:51,904:INFO:Cross validation set to False
2024-02-08 02:11:51,904:INFO:Fitting Model
2024-02-08 02:11:55,383:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 02:11:55,473:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069595 seconds.
2024-02-08 02:11:55,474:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-08 02:11:55,476:INFO:[LightGBM] [Info] Total Bins 31359
2024-02-08 02:11:55,478:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 123
2024-02-08 02:11:55,479:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 02:12:02,581:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=6300, verbose=0, warm_start=False)
2024-02-08 02:12:02,581:INFO:create_model() successfully completed......................................
2024-02-08 02:12:02,739:INFO:Initializing create_model()
2024-02-08 02:12:02,739:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6300, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:12:02,739:INFO:Checking exceptions
2024-02-08 02:12:02,743:INFO:Importing libraries
2024-02-08 02:12:02,744:INFO:Copying training dataset
2024-02-08 02:12:02,899:INFO:Defining folds
2024-02-08 02:12:02,899:INFO:Declaring metric variables
2024-02-08 02:12:02,899:INFO:Importing untrained model
2024-02-08 02:12:02,899:INFO:Declaring custom model
2024-02-08 02:12:02,900:INFO:Random Forest Classifier Imported successfully
2024-02-08 02:12:02,911:INFO:Cross validation set to False
2024-02-08 02:12:02,912:INFO:Fitting Model
2024-02-08 02:12:06,097:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 02:12:06,191:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071349 seconds.
2024-02-08 02:12:06,191:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-08 02:12:06,192:INFO:[LightGBM] [Info] Total Bins 31359
2024-02-08 02:12:06,195:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 123
2024-02-08 02:12:06,196:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 02:12:18,708:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6300, verbose=0, warm_start=False)
2024-02-08 02:12:18,708:INFO:create_model() successfully completed......................................
2024-02-08 02:12:18,916:INFO:Initializing create_model()
2024-02-08 02:12:18,916:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6300, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:12:18,917:INFO:Checking exceptions
2024-02-08 02:12:18,921:INFO:Importing libraries
2024-02-08 02:12:18,921:INFO:Copying training dataset
2024-02-08 02:12:19,092:INFO:Defining folds
2024-02-08 02:12:19,093:INFO:Declaring metric variables
2024-02-08 02:12:19,093:INFO:Importing untrained model
2024-02-08 02:12:19,093:INFO:Declaring custom model
2024-02-08 02:12:19,094:INFO:Gradient Boosting Classifier Imported successfully
2024-02-08 02:12:19,109:INFO:Cross validation set to False
2024-02-08 02:12:19,109:INFO:Fitting Model
2024-02-08 02:12:22,954:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 02:12:23,042:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065020 seconds.
2024-02-08 02:12:23,042:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-08 02:12:23,044:INFO:[LightGBM] [Info] Total Bins 31359
2024-02-08 02:12:23,046:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 123
2024-02-08 02:12:23,047:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 02:13:22,064:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6300, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-02-08 02:13:22,064:INFO:create_model() successfully completed......................................
2024-02-08 02:13:22,211:INFO:Initializing create_model()
2024-02-08 02:13:22,212:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=6300), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:13:22,212:INFO:Checking exceptions
2024-02-08 02:13:22,213:INFO:Importing libraries
2024-02-08 02:13:22,213:INFO:Copying training dataset
2024-02-08 02:13:22,353:INFO:Defining folds
2024-02-08 02:13:22,353:INFO:Declaring metric variables
2024-02-08 02:13:22,353:INFO:Importing untrained model
2024-02-08 02:13:22,353:INFO:Declaring custom model
2024-02-08 02:13:22,354:INFO:Ada Boost Classifier Imported successfully
2024-02-08 02:13:22,364:INFO:Cross validation set to False
2024-02-08 02:13:22,364:INFO:Fitting Model
2024-02-08 02:13:25,776:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 02:13:25,849:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021644 seconds.
2024-02-08 02:13:25,849:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-02-08 02:13:25,849:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-02-08 02:13:25,849:INFO:[LightGBM] [Info] Total Bins 31359
2024-02-08 02:13:25,851:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 123
2024-02-08 02:13:25,852:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 02:13:37,916:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=6300)
2024-02-08 02:13:37,916:INFO:create_model() successfully completed......................................
2024-02-08 02:13:38,072:INFO:_master_model_container: 15
2024-02-08 02:13:38,072:INFO:_display_container: 2
2024-02-08 02:13:38,073:INFO:[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6300, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=6300, verbose=0, warm_start=False), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6300, verbose=0, warm_start=False), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6300, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=6300)]
2024-02-08 02:13:38,073:INFO:compare_models() successfully completed......................................
2024-02-08 02:26:55,471:INFO:Initializing compare_models()
2024-02-08 02:26:55,471:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-02-08 02:26:55,472:INFO:Checking exceptions
2024-02-08 02:26:55,514:INFO:Preparing display monitor
2024-02-08 02:26:55,547:INFO:Initializing Logistic Regression
2024-02-08 02:26:55,547:INFO:Total runtime is 0.0 minutes
2024-02-08 02:26:55,553:INFO:SubProcess create_model() called ==================================
2024-02-08 02:26:55,553:INFO:Initializing create_model()
2024-02-08 02:26:55,553:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E20F4CA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:26:55,554:INFO:Checking exceptions
2024-02-08 02:26:55,554:INFO:Importing libraries
2024-02-08 02:26:55,554:INFO:Copying training dataset
2024-02-08 02:26:55,673:INFO:Defining folds
2024-02-08 02:26:55,673:INFO:Declaring metric variables
2024-02-08 02:26:55,679:INFO:Importing untrained model
2024-02-08 02:26:55,684:INFO:Logistic Regression Imported successfully
2024-02-08 02:26:55,692:INFO:Starting cross validation
2024-02-08 02:26:55,705:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 02:27:36,706:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 02:27:38,200:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 02:27:38,342:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 02:27:40,050:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 02:27:40,225:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 02:27:40,258:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 02:27:41,637:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 02:27:51,362:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 02:27:56,100:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 02:27:56,106:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 02:27:56,284:INFO:Calculating mean and std
2024-02-08 02:27:56,286:INFO:Creating metrics dataframe
2024-02-08 02:27:56,292:INFO:Uploading results into container
2024-02-08 02:27:56,293:INFO:Uploading model into container now
2024-02-08 02:27:56,293:INFO:_master_model_container: 16
2024-02-08 02:27:56,294:INFO:_display_container: 2
2024-02-08 02:27:56,294:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=6300, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-02-08 02:27:56,295:INFO:create_model() successfully completed......................................
2024-02-08 02:27:56,470:INFO:SubProcess create_model() end ==================================
2024-02-08 02:27:56,471:INFO:Creating metrics dataframe
2024-02-08 02:27:56,482:INFO:Initializing K Neighbors Classifier
2024-02-08 02:27:56,482:INFO:Total runtime is 1.0155830224355062 minutes
2024-02-08 02:27:56,487:INFO:SubProcess create_model() called ==================================
2024-02-08 02:27:56,487:INFO:Initializing create_model()
2024-02-08 02:27:56,487:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E20F4CA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:27:56,488:INFO:Checking exceptions
2024-02-08 02:27:56,488:INFO:Importing libraries
2024-02-08 02:27:56,488:INFO:Copying training dataset
2024-02-08 02:27:56,603:INFO:Defining folds
2024-02-08 02:27:56,603:INFO:Declaring metric variables
2024-02-08 02:27:56,608:INFO:Importing untrained model
2024-02-08 02:27:56,614:INFO:K Neighbors Classifier Imported successfully
2024-02-08 02:27:56,621:INFO:Starting cross validation
2024-02-08 02:27:56,633:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 02:28:46,389:INFO:Calculating mean and std
2024-02-08 02:28:46,392:INFO:Creating metrics dataframe
2024-02-08 02:28:46,400:INFO:Uploading results into container
2024-02-08 02:28:46,401:INFO:Uploading model into container now
2024-02-08 02:28:46,401:INFO:_master_model_container: 17
2024-02-08 02:28:46,401:INFO:_display_container: 2
2024-02-08 02:28:46,402:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-02-08 02:28:46,402:INFO:create_model() successfully completed......................................
2024-02-08 02:28:46,524:INFO:SubProcess create_model() end ==================================
2024-02-08 02:28:46,524:INFO:Creating metrics dataframe
2024-02-08 02:28:46,535:INFO:Initializing Naive Bayes
2024-02-08 02:28:46,535:INFO:Total runtime is 1.8498025576273602 minutes
2024-02-08 02:28:46,540:INFO:SubProcess create_model() called ==================================
2024-02-08 02:28:46,540:INFO:Initializing create_model()
2024-02-08 02:28:46,540:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E20F4CA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:28:46,540:INFO:Checking exceptions
2024-02-08 02:28:46,540:INFO:Importing libraries
2024-02-08 02:28:46,540:INFO:Copying training dataset
2024-02-08 02:28:46,631:INFO:Defining folds
2024-02-08 02:28:46,632:INFO:Declaring metric variables
2024-02-08 02:28:46,637:INFO:Importing untrained model
2024-02-08 02:28:46,643:INFO:Naive Bayes Imported successfully
2024-02-08 02:28:46,653:INFO:Starting cross validation
2024-02-08 02:28:46,661:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 02:29:18,508:INFO:Calculating mean and std
2024-02-08 02:29:18,511:INFO:Creating metrics dataframe
2024-02-08 02:29:18,522:INFO:Uploading results into container
2024-02-08 02:29:18,523:INFO:Uploading model into container now
2024-02-08 02:29:18,525:INFO:_master_model_container: 18
2024-02-08 02:29:18,525:INFO:_display_container: 2
2024-02-08 02:29:18,526:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-02-08 02:29:18,526:INFO:create_model() successfully completed......................................
2024-02-08 02:29:18,665:INFO:SubProcess create_model() end ==================================
2024-02-08 02:29:18,665:INFO:Creating metrics dataframe
2024-02-08 02:29:18,677:INFO:Initializing Decision Tree Classifier
2024-02-08 02:29:18,677:INFO:Total runtime is 2.385511040687561 minutes
2024-02-08 02:29:18,682:INFO:SubProcess create_model() called ==================================
2024-02-08 02:29:18,683:INFO:Initializing create_model()
2024-02-08 02:29:18,683:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E20F4CA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:29:18,683:INFO:Checking exceptions
2024-02-08 02:29:18,683:INFO:Importing libraries
2024-02-08 02:29:18,683:INFO:Copying training dataset
2024-02-08 02:29:18,765:INFO:Defining folds
2024-02-08 02:29:18,765:INFO:Declaring metric variables
2024-02-08 02:29:18,771:INFO:Importing untrained model
2024-02-08 02:29:18,776:INFO:Decision Tree Classifier Imported successfully
2024-02-08 02:29:18,784:INFO:Starting cross validation
2024-02-08 02:29:18,792:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 02:29:55,199:INFO:Calculating mean and std
2024-02-08 02:29:55,202:INFO:Creating metrics dataframe
2024-02-08 02:29:55,207:INFO:Uploading results into container
2024-02-08 02:29:55,208:INFO:Uploading model into container now
2024-02-08 02:29:55,209:INFO:_master_model_container: 19
2024-02-08 02:29:55,209:INFO:_display_container: 2
2024-02-08 02:29:55,209:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=6300, splitter='best')
2024-02-08 02:29:55,209:INFO:create_model() successfully completed......................................
2024-02-08 02:29:55,335:INFO:SubProcess create_model() end ==================================
2024-02-08 02:29:55,336:INFO:Creating metrics dataframe
2024-02-08 02:29:55,348:INFO:Initializing SVM - Linear Kernel
2024-02-08 02:29:55,348:INFO:Total runtime is 2.996685802936554 minutes
2024-02-08 02:29:55,353:INFO:SubProcess create_model() called ==================================
2024-02-08 02:29:55,354:INFO:Initializing create_model()
2024-02-08 02:29:55,354:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E20F4CA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:29:55,355:INFO:Checking exceptions
2024-02-08 02:29:55,355:INFO:Importing libraries
2024-02-08 02:29:55,355:INFO:Copying training dataset
2024-02-08 02:29:55,443:INFO:Defining folds
2024-02-08 02:29:55,444:INFO:Declaring metric variables
2024-02-08 02:29:55,449:INFO:Importing untrained model
2024-02-08 02:29:55,454:INFO:SVM - Linear Kernel Imported successfully
2024-02-08 02:29:55,461:INFO:Starting cross validation
2024-02-08 02:29:55,472:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 02:30:26,491:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-08 02:30:26,992:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-08 02:30:27,181:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-08 02:30:28,692:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-08 02:30:29,488:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-08 02:30:30,215:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-08 02:30:31,256:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-08 02:30:34,690:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-08 02:30:38,700:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-08 02:30:40,280:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-08 02:30:40,405:INFO:Calculating mean and std
2024-02-08 02:30:40,407:INFO:Creating metrics dataframe
2024-02-08 02:30:40,413:INFO:Uploading results into container
2024-02-08 02:30:40,414:INFO:Uploading model into container now
2024-02-08 02:30:40,415:INFO:_master_model_container: 20
2024-02-08 02:30:40,415:INFO:_display_container: 2
2024-02-08 02:30:40,416:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=6300, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-02-08 02:30:40,416:INFO:create_model() successfully completed......................................
2024-02-08 02:30:40,583:INFO:SubProcess create_model() end ==================================
2024-02-08 02:30:40,583:INFO:Creating metrics dataframe
2024-02-08 02:30:40,596:INFO:Initializing Ridge Classifier
2024-02-08 02:30:40,597:INFO:Total runtime is 3.7508379300435384 minutes
2024-02-08 02:30:40,602:INFO:SubProcess create_model() called ==================================
2024-02-08 02:30:40,603:INFO:Initializing create_model()
2024-02-08 02:30:40,603:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E20F4CA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:30:40,603:INFO:Checking exceptions
2024-02-08 02:30:40,603:INFO:Importing libraries
2024-02-08 02:30:40,603:INFO:Copying training dataset
2024-02-08 02:30:40,723:INFO:Defining folds
2024-02-08 02:30:40,723:INFO:Declaring metric variables
2024-02-08 02:30:40,728:INFO:Importing untrained model
2024-02-08 02:30:40,735:INFO:Ridge Classifier Imported successfully
2024-02-08 02:30:40,744:INFO:Starting cross validation
2024-02-08 02:30:40,755:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 02:31:04,683:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-08 02:31:04,741:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-08 02:31:04,853:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-08 02:31:04,916:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-08 02:31:05,269:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-08 02:31:05,797:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-08 02:31:06,577:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-08 02:31:06,674:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-08 02:31:13,613:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-08 02:31:13,812:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-08 02:31:13,953:INFO:Calculating mean and std
2024-02-08 02:31:13,954:INFO:Creating metrics dataframe
2024-02-08 02:31:13,959:INFO:Uploading results into container
2024-02-08 02:31:13,960:INFO:Uploading model into container now
2024-02-08 02:31:13,961:INFO:_master_model_container: 21
2024-02-08 02:31:13,961:INFO:_display_container: 2
2024-02-08 02:31:13,961:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=6300, solver='auto',
                tol=0.0001)
2024-02-08 02:31:13,962:INFO:create_model() successfully completed......................................
2024-02-08 02:31:14,091:INFO:SubProcess create_model() end ==================================
2024-02-08 02:31:14,091:INFO:Creating metrics dataframe
2024-02-08 02:31:14,101:INFO:Initializing Random Forest Classifier
2024-02-08 02:31:14,102:INFO:Total runtime is 4.309262867768606 minutes
2024-02-08 02:31:14,106:INFO:SubProcess create_model() called ==================================
2024-02-08 02:31:14,106:INFO:Initializing create_model()
2024-02-08 02:31:14,106:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E20F4CA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:31:14,106:INFO:Checking exceptions
2024-02-08 02:31:14,107:INFO:Importing libraries
2024-02-08 02:31:14,107:INFO:Copying training dataset
2024-02-08 02:31:14,186:INFO:Defining folds
2024-02-08 02:31:14,186:INFO:Declaring metric variables
2024-02-08 02:31:14,190:INFO:Importing untrained model
2024-02-08 02:31:14,198:INFO:Random Forest Classifier Imported successfully
2024-02-08 02:31:14,207:INFO:Starting cross validation
2024-02-08 02:31:14,220:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 02:33:10,554:INFO:Calculating mean and std
2024-02-08 02:33:10,556:INFO:Creating metrics dataframe
2024-02-08 02:33:10,560:INFO:Uploading results into container
2024-02-08 02:33:10,561:INFO:Uploading model into container now
2024-02-08 02:33:10,562:INFO:_master_model_container: 22
2024-02-08 02:33:10,562:INFO:_display_container: 2
2024-02-08 02:33:10,563:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6300, verbose=0, warm_start=False)
2024-02-08 02:33:10,563:INFO:create_model() successfully completed......................................
2024-02-08 02:33:10,723:INFO:SubProcess create_model() end ==================================
2024-02-08 02:33:10,724:INFO:Creating metrics dataframe
2024-02-08 02:33:10,738:INFO:Initializing Quadratic Discriminant Analysis
2024-02-08 02:33:10,738:INFO:Total runtime is 6.253186639149984 minutes
2024-02-08 02:33:10,742:INFO:SubProcess create_model() called ==================================
2024-02-08 02:33:10,742:INFO:Initializing create_model()
2024-02-08 02:33:10,742:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E20F4CA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:33:10,742:INFO:Checking exceptions
2024-02-08 02:33:10,742:INFO:Importing libraries
2024-02-08 02:33:10,743:INFO:Copying training dataset
2024-02-08 02:33:10,844:INFO:Defining folds
2024-02-08 02:33:10,844:INFO:Declaring metric variables
2024-02-08 02:33:10,848:INFO:Importing untrained model
2024-02-08 02:33:10,856:INFO:Quadratic Discriminant Analysis Imported successfully
2024-02-08 02:33:10,868:INFO:Starting cross validation
2024-02-08 02:33:10,883:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 02:33:45,285:INFO:Calculating mean and std
2024-02-08 02:33:45,287:INFO:Creating metrics dataframe
2024-02-08 02:33:45,292:INFO:Uploading results into container
2024-02-08 02:33:45,293:INFO:Uploading model into container now
2024-02-08 02:33:45,293:INFO:_master_model_container: 23
2024-02-08 02:33:45,294:INFO:_display_container: 2
2024-02-08 02:33:45,294:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-02-08 02:33:45,294:INFO:create_model() successfully completed......................................
2024-02-08 02:33:45,425:INFO:SubProcess create_model() end ==================================
2024-02-08 02:33:45,425:INFO:Creating metrics dataframe
2024-02-08 02:33:45,447:INFO:Initializing Ada Boost Classifier
2024-02-08 02:33:45,448:INFO:Total runtime is 6.831691896915436 minutes
2024-02-08 02:33:45,452:INFO:SubProcess create_model() called ==================================
2024-02-08 02:33:45,452:INFO:Initializing create_model()
2024-02-08 02:33:45,452:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E20F4CA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:33:45,452:INFO:Checking exceptions
2024-02-08 02:33:45,453:INFO:Importing libraries
2024-02-08 02:33:45,453:INFO:Copying training dataset
2024-02-08 02:33:45,566:INFO:Defining folds
2024-02-08 02:33:45,566:INFO:Declaring metric variables
2024-02-08 02:33:45,570:INFO:Importing untrained model
2024-02-08 02:33:45,577:INFO:Ada Boost Classifier Imported successfully
2024-02-08 02:33:45,587:INFO:Starting cross validation
2024-02-08 02:33:45,597:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 02:34:54,344:INFO:Calculating mean and std
2024-02-08 02:34:54,347:INFO:Creating metrics dataframe
2024-02-08 02:34:54,353:INFO:Uploading results into container
2024-02-08 02:34:54,354:INFO:Uploading model into container now
2024-02-08 02:34:54,354:INFO:_master_model_container: 24
2024-02-08 02:34:54,355:INFO:_display_container: 2
2024-02-08 02:34:54,355:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=6300)
2024-02-08 02:34:54,356:INFO:create_model() successfully completed......................................
2024-02-08 02:34:54,521:INFO:SubProcess create_model() end ==================================
2024-02-08 02:34:54,522:INFO:Creating metrics dataframe
2024-02-08 02:34:54,536:INFO:Initializing Gradient Boosting Classifier
2024-02-08 02:34:54,536:INFO:Total runtime is 7.983156907558442 minutes
2024-02-08 02:34:54,541:INFO:SubProcess create_model() called ==================================
2024-02-08 02:34:54,541:INFO:Initializing create_model()
2024-02-08 02:34:54,541:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E20F4CA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:34:54,541:INFO:Checking exceptions
2024-02-08 02:34:54,541:INFO:Importing libraries
2024-02-08 02:34:54,542:INFO:Copying training dataset
2024-02-08 02:34:54,654:INFO:Defining folds
2024-02-08 02:34:54,654:INFO:Declaring metric variables
2024-02-08 02:34:54,658:INFO:Importing untrained model
2024-02-08 02:34:54,667:INFO:Gradient Boosting Classifier Imported successfully
2024-02-08 02:34:54,680:INFO:Starting cross validation
2024-02-08 02:34:54,690:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 02:38:13,524:INFO:Calculating mean and std
2024-02-08 02:38:13,525:INFO:Creating metrics dataframe
2024-02-08 02:38:13,530:INFO:Uploading results into container
2024-02-08 02:38:13,531:INFO:Uploading model into container now
2024-02-08 02:38:13,531:INFO:_master_model_container: 25
2024-02-08 02:38:13,531:INFO:_display_container: 2
2024-02-08 02:38:13,532:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6300, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-02-08 02:38:13,532:INFO:create_model() successfully completed......................................
2024-02-08 02:38:13,679:INFO:SubProcess create_model() end ==================================
2024-02-08 02:38:13,679:INFO:Creating metrics dataframe
2024-02-08 02:38:13,697:INFO:Initializing Linear Discriminant Analysis
2024-02-08 02:38:13,697:INFO:Total runtime is 11.302500132719675 minutes
2024-02-08 02:38:13,703:INFO:SubProcess create_model() called ==================================
2024-02-08 02:38:13,704:INFO:Initializing create_model()
2024-02-08 02:38:13,705:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E20F4CA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:38:13,705:INFO:Checking exceptions
2024-02-08 02:38:13,705:INFO:Importing libraries
2024-02-08 02:38:13,705:INFO:Copying training dataset
2024-02-08 02:38:13,824:INFO:Defining folds
2024-02-08 02:38:13,825:INFO:Declaring metric variables
2024-02-08 02:38:13,829:INFO:Importing untrained model
2024-02-08 02:38:13,836:INFO:Linear Discriminant Analysis Imported successfully
2024-02-08 02:38:13,846:INFO:Starting cross validation
2024-02-08 02:38:13,862:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 02:38:45,596:INFO:Calculating mean and std
2024-02-08 02:38:45,597:INFO:Creating metrics dataframe
2024-02-08 02:38:45,603:INFO:Uploading results into container
2024-02-08 02:38:45,604:INFO:Uploading model into container now
2024-02-08 02:38:45,604:INFO:_master_model_container: 26
2024-02-08 02:38:45,604:INFO:_display_container: 2
2024-02-08 02:38:45,605:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-02-08 02:38:45,605:INFO:create_model() successfully completed......................................
2024-02-08 02:38:45,737:INFO:SubProcess create_model() end ==================================
2024-02-08 02:38:45,737:INFO:Creating metrics dataframe
2024-02-08 02:38:45,751:INFO:Initializing Extra Trees Classifier
2024-02-08 02:38:45,751:INFO:Total runtime is 11.836739285786946 minutes
2024-02-08 02:38:45,755:INFO:SubProcess create_model() called ==================================
2024-02-08 02:38:45,755:INFO:Initializing create_model()
2024-02-08 02:38:45,755:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E20F4CA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:38:45,755:INFO:Checking exceptions
2024-02-08 02:38:45,756:INFO:Importing libraries
2024-02-08 02:38:45,756:INFO:Copying training dataset
2024-02-08 02:38:45,867:INFO:Defining folds
2024-02-08 02:38:45,867:INFO:Declaring metric variables
2024-02-08 02:38:45,871:INFO:Importing untrained model
2024-02-08 02:38:45,877:INFO:Extra Trees Classifier Imported successfully
2024-02-08 02:38:45,886:INFO:Starting cross validation
2024-02-08 02:38:45,898:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 02:39:52,761:INFO:Calculating mean and std
2024-02-08 02:39:52,763:INFO:Creating metrics dataframe
2024-02-08 02:39:52,769:INFO:Uploading results into container
2024-02-08 02:39:52,770:INFO:Uploading model into container now
2024-02-08 02:39:52,770:INFO:_master_model_container: 27
2024-02-08 02:39:52,770:INFO:_display_container: 2
2024-02-08 02:39:52,771:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=6300, verbose=0, warm_start=False)
2024-02-08 02:39:52,771:INFO:create_model() successfully completed......................................
2024-02-08 02:39:52,916:INFO:SubProcess create_model() end ==================================
2024-02-08 02:39:52,916:INFO:Creating metrics dataframe
2024-02-08 02:39:52,931:INFO:Initializing Light Gradient Boosting Machine
2024-02-08 02:39:52,931:INFO:Total runtime is 12.956408309936522 minutes
2024-02-08 02:39:52,936:INFO:SubProcess create_model() called ==================================
2024-02-08 02:39:52,936:INFO:Initializing create_model()
2024-02-08 02:39:52,936:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E20F4CA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:39:52,936:INFO:Checking exceptions
2024-02-08 02:39:52,936:INFO:Importing libraries
2024-02-08 02:39:52,937:INFO:Copying training dataset
2024-02-08 02:39:53,057:INFO:Defining folds
2024-02-08 02:39:53,057:INFO:Declaring metric variables
2024-02-08 02:39:53,062:INFO:Importing untrained model
2024-02-08 02:39:53,068:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-08 02:39:53,078:INFO:Starting cross validation
2024-02-08 02:39:53,090:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 02:40:32,523:INFO:Calculating mean and std
2024-02-08 02:40:32,525:INFO:Creating metrics dataframe
2024-02-08 02:40:32,529:INFO:Uploading results into container
2024-02-08 02:40:32,530:INFO:Uploading model into container now
2024-02-08 02:40:32,530:INFO:_master_model_container: 28
2024-02-08 02:40:32,531:INFO:_display_container: 2
2024-02-08 02:40:32,531:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6300, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-02-08 02:40:32,531:INFO:create_model() successfully completed......................................
2024-02-08 02:40:32,676:INFO:SubProcess create_model() end ==================================
2024-02-08 02:40:32,676:INFO:Creating metrics dataframe
2024-02-08 02:40:32,697:INFO:Initializing CatBoost Classifier
2024-02-08 02:40:32,697:INFO:Total runtime is 13.619181064764657 minutes
2024-02-08 02:40:32,704:INFO:SubProcess create_model() called ==================================
2024-02-08 02:40:32,704:INFO:Initializing create_model()
2024-02-08 02:40:32,704:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E20F4CA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:40:32,704:INFO:Checking exceptions
2024-02-08 02:40:32,704:INFO:Importing libraries
2024-02-08 02:40:32,705:INFO:Copying training dataset
2024-02-08 02:40:32,827:INFO:Defining folds
2024-02-08 02:40:32,827:INFO:Declaring metric variables
2024-02-08 02:40:32,832:INFO:Importing untrained model
2024-02-08 02:40:32,838:INFO:CatBoost Classifier Imported successfully
2024-02-08 02:40:32,848:INFO:Starting cross validation
2024-02-08 02:40:32,861:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 02:43:19,175:INFO:Calculating mean and std
2024-02-08 02:43:19,177:INFO:Creating metrics dataframe
2024-02-08 02:43:19,181:INFO:Uploading results into container
2024-02-08 02:43:19,182:INFO:Uploading model into container now
2024-02-08 02:43:19,183:INFO:_master_model_container: 29
2024-02-08 02:43:19,183:INFO:_display_container: 2
2024-02-08 02:43:19,183:INFO:<catboost.core.CatBoostClassifier object at 0x000002369A89A980>
2024-02-08 02:43:19,183:INFO:create_model() successfully completed......................................
2024-02-08 02:43:19,326:INFO:SubProcess create_model() end ==================================
2024-02-08 02:43:19,327:INFO:Creating metrics dataframe
2024-02-08 02:43:19,340:INFO:Initializing Dummy Classifier
2024-02-08 02:43:19,340:INFO:Total runtime is 16.396550365289052 minutes
2024-02-08 02:43:19,345:INFO:SubProcess create_model() called ==================================
2024-02-08 02:43:19,345:INFO:Initializing create_model()
2024-02-08 02:43:19,345:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E20F4CA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:43:19,345:INFO:Checking exceptions
2024-02-08 02:43:19,345:INFO:Importing libraries
2024-02-08 02:43:19,346:INFO:Copying training dataset
2024-02-08 02:43:19,461:INFO:Defining folds
2024-02-08 02:43:19,461:INFO:Declaring metric variables
2024-02-08 02:43:19,467:INFO:Importing untrained model
2024-02-08 02:43:19,475:INFO:Dummy Classifier Imported successfully
2024-02-08 02:43:19,483:INFO:Starting cross validation
2024-02-08 02:43:19,496:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 02:43:42,457:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-08 02:43:42,730:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-08 02:43:43,174:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-08 02:43:43,293:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-08 02:43:43,662:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-08 02:43:46,060:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-08 02:43:46,380:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-08 02:43:46,517:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-08 02:43:52,389:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-08 02:43:52,522:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-08 02:43:52,650:INFO:Calculating mean and std
2024-02-08 02:43:52,652:INFO:Creating metrics dataframe
2024-02-08 02:43:52,662:INFO:Uploading results into container
2024-02-08 02:43:52,663:INFO:Uploading model into container now
2024-02-08 02:43:52,664:INFO:_master_model_container: 30
2024-02-08 02:43:52,665:INFO:_display_container: 2
2024-02-08 02:43:52,665:INFO:DummyClassifier(constant=None, random_state=6300, strategy='prior')
2024-02-08 02:43:52,665:INFO:create_model() successfully completed......................................
2024-02-08 02:43:52,798:INFO:SubProcess create_model() end ==================================
2024-02-08 02:43:52,798:INFO:Creating metrics dataframe
2024-02-08 02:43:52,824:INFO:Initializing create_model()
2024-02-08 02:43:52,824:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=<catboost.core.CatBoostClassifier object at 0x000002369A89A980>, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:43:52,824:INFO:Checking exceptions
2024-02-08 02:43:52,826:INFO:Importing libraries
2024-02-08 02:43:52,827:INFO:Copying training dataset
2024-02-08 02:43:52,919:INFO:Defining folds
2024-02-08 02:43:52,919:INFO:Declaring metric variables
2024-02-08 02:43:52,919:INFO:Importing untrained model
2024-02-08 02:43:52,919:INFO:Declaring custom model
2024-02-08 02:43:52,920:INFO:CatBoost Classifier Imported successfully
2024-02-08 02:43:52,929:INFO:Cross validation set to False
2024-02-08 02:43:52,929:INFO:Fitting Model
2024-02-08 02:43:55,931:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 02:43:55,988:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017389 seconds.
2024-02-08 02:43:55,989:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-02-08 02:43:55,989:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-02-08 02:43:55,989:INFO:[LightGBM] [Info] Total Bins 31359
2024-02-08 02:43:55,991:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 123
2024-02-08 02:43:55,992:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 02:44:18,092:INFO:<catboost.core.CatBoostClassifier object at 0x0000023698F22110>
2024-02-08 02:44:18,092:INFO:create_model() successfully completed......................................
2024-02-08 02:44:18,248:INFO:Initializing create_model()
2024-02-08 02:44:18,248:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6300, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:44:18,248:INFO:Checking exceptions
2024-02-08 02:44:18,251:INFO:Importing libraries
2024-02-08 02:44:18,251:INFO:Copying training dataset
2024-02-08 02:44:18,377:INFO:Defining folds
2024-02-08 02:44:18,377:INFO:Declaring metric variables
2024-02-08 02:44:18,378:INFO:Importing untrained model
2024-02-08 02:44:18,378:INFO:Declaring custom model
2024-02-08 02:44:18,379:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-08 02:44:18,391:INFO:Cross validation set to False
2024-02-08 02:44:18,391:INFO:Fitting Model
2024-02-08 02:44:21,699:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 02:44:21,793:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073440 seconds.
2024-02-08 02:44:21,793:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-08 02:44:21,796:INFO:[LightGBM] [Info] Total Bins 31359
2024-02-08 02:44:21,798:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 123
2024-02-08 02:44:21,799:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 02:44:24,049:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 02:44:24,063:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003652 seconds.
2024-02-08 02:44:24,063:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-02-08 02:44:24,063:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-02-08 02:44:24,063:INFO:[LightGBM] [Info] Total Bins 6375
2024-02-08 02:44:24,063:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 25
2024-02-08 02:44:24,064:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 02:44:24,600:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6300, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-02-08 02:44:24,600:INFO:create_model() successfully completed......................................
2024-02-08 02:44:24,776:INFO:Initializing create_model()
2024-02-08 02:44:24,776:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=6300, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:44:24,776:INFO:Checking exceptions
2024-02-08 02:44:24,780:INFO:Importing libraries
2024-02-08 02:44:24,780:INFO:Copying training dataset
2024-02-08 02:44:24,894:INFO:Defining folds
2024-02-08 02:44:24,895:INFO:Declaring metric variables
2024-02-08 02:44:24,895:INFO:Importing untrained model
2024-02-08 02:44:24,895:INFO:Declaring custom model
2024-02-08 02:44:24,896:INFO:Extra Trees Classifier Imported successfully
2024-02-08 02:44:24,904:INFO:Cross validation set to False
2024-02-08 02:44:24,904:INFO:Fitting Model
2024-02-08 02:44:28,627:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 02:44:28,726:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079275 seconds.
2024-02-08 02:44:28,726:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-08 02:44:28,729:INFO:[LightGBM] [Info] Total Bins 31359
2024-02-08 02:44:28,730:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 123
2024-02-08 02:44:28,731:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 02:44:35,090:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=6300, verbose=0, warm_start=False)
2024-02-08 02:44:35,090:INFO:create_model() successfully completed......................................
2024-02-08 02:44:35,229:INFO:Initializing create_model()
2024-02-08 02:44:35,230:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6300, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:44:35,230:INFO:Checking exceptions
2024-02-08 02:44:35,231:INFO:Importing libraries
2024-02-08 02:44:35,232:INFO:Copying training dataset
2024-02-08 02:44:35,338:INFO:Defining folds
2024-02-08 02:44:35,338:INFO:Declaring metric variables
2024-02-08 02:44:35,339:INFO:Importing untrained model
2024-02-08 02:44:35,339:INFO:Declaring custom model
2024-02-08 02:44:35,339:INFO:Random Forest Classifier Imported successfully
2024-02-08 02:44:35,348:INFO:Cross validation set to False
2024-02-08 02:44:35,348:INFO:Fitting Model
2024-02-08 02:44:38,577:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 02:44:38,667:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073432 seconds.
2024-02-08 02:44:38,667:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-08 02:44:38,669:INFO:[LightGBM] [Info] Total Bins 31359
2024-02-08 02:44:38,671:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 123
2024-02-08 02:44:38,672:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 02:44:52,044:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6300, verbose=0, warm_start=False)
2024-02-08 02:44:52,044:INFO:create_model() successfully completed......................................
2024-02-08 02:44:52,185:INFO:Initializing create_model()
2024-02-08 02:44:52,185:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6300, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 02:44:52,186:INFO:Checking exceptions
2024-02-08 02:44:52,188:INFO:Importing libraries
2024-02-08 02:44:52,188:INFO:Copying training dataset
2024-02-08 02:44:52,298:INFO:Defining folds
2024-02-08 02:44:52,298:INFO:Declaring metric variables
2024-02-08 02:44:52,298:INFO:Importing untrained model
2024-02-08 02:44:52,298:INFO:Declaring custom model
2024-02-08 02:44:52,299:INFO:Gradient Boosting Classifier Imported successfully
2024-02-08 02:44:52,307:INFO:Cross validation set to False
2024-02-08 02:44:52,307:INFO:Fitting Model
2024-02-08 02:44:55,316:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 02:44:55,383:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053763 seconds.
2024-02-08 02:44:55,383:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-08 02:44:55,384:INFO:[LightGBM] [Info] Total Bins 31359
2024-02-08 02:44:55,385:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 123
2024-02-08 02:44:55,386:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 02:45:53,469:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6300, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-02-08 02:45:53,469:INFO:create_model() successfully completed......................................
2024-02-08 02:45:53,678:INFO:_master_model_container: 30
2024-02-08 02:45:53,678:INFO:_display_container: 2
2024-02-08 02:45:53,680:INFO:[<catboost.core.CatBoostClassifier object at 0x0000023698F22110>, LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6300, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=6300, verbose=0, warm_start=False), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6300, verbose=0, warm_start=False), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6300, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)]
2024-02-08 02:45:53,680:INFO:compare_models() successfully completed......................................
2024-02-08 02:45:53,763:INFO:Initializing tune_model()
2024-02-08 02:45:53,764:INFO:tune_model(estimator=<catboost.core.CatBoostClassifier object at 0x0000023698F22110>, fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>)
2024-02-08 02:45:53,764:INFO:Checking exceptions
2024-02-08 02:45:53,838:INFO:Copying training dataset
2024-02-08 02:45:53,924:INFO:Checking base model
2024-02-08 02:45:53,924:INFO:Base model : CatBoost Classifier
2024-02-08 02:45:53,929:INFO:Declaring metric variables
2024-02-08 02:45:53,934:INFO:Defining Hyperparameters
2024-02-08 02:45:54,116:INFO:Tuning with n_jobs=-1
2024-02-08 02:45:54,116:INFO:Initializing RandomizedSearchCV
2024-02-08 03:10:25,871:INFO:best_params: {'actual_estimator__random_strength': 0.3, 'actual_estimator__n_estimators': 280, 'actual_estimator__l2_leaf_reg': 10, 'actual_estimator__eta': 0.1, 'actual_estimator__depth': 11}
2024-02-08 03:10:25,872:INFO:Hyperparameter search completed
2024-02-08 03:10:25,872:INFO:SubProcess create_model() called ==================================
2024-02-08 03:10:25,873:INFO:Initializing create_model()
2024-02-08 03:10:25,873:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=<catboost.core.CatBoostClassifier object at 0x000002369A8BC0A0>, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E2BFF3A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'random_strength': 0.3, 'n_estimators': 280, 'l2_leaf_reg': 10, 'eta': 0.1, 'depth': 11})
2024-02-08 03:10:25,873:INFO:Checking exceptions
2024-02-08 03:10:25,873:INFO:Importing libraries
2024-02-08 03:10:25,873:INFO:Copying training dataset
2024-02-08 03:10:25,979:INFO:Defining folds
2024-02-08 03:10:25,980:INFO:Declaring metric variables
2024-02-08 03:10:25,984:INFO:Importing untrained model
2024-02-08 03:10:25,984:INFO:Declaring custom model
2024-02-08 03:10:25,989:INFO:CatBoost Classifier Imported successfully
2024-02-08 03:10:25,996:INFO:Starting cross validation
2024-02-08 03:10:26,007:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 03:16:50,467:INFO:Calculating mean and std
2024-02-08 03:16:50,469:INFO:Creating metrics dataframe
2024-02-08 03:16:50,476:INFO:Finalizing model
2024-02-08 03:16:53,511:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 03:16:53,584:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057377 seconds.
2024-02-08 03:16:53,584:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-08 03:16:53,585:INFO:[LightGBM] [Info] Total Bins 31359
2024-02-08 03:16:53,586:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 123
2024-02-08 03:16:53,586:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 03:17:46,205:INFO:Uploading results into container
2024-02-08 03:17:46,207:INFO:Uploading model into container now
2024-02-08 03:17:46,207:INFO:_master_model_container: 31
2024-02-08 03:17:46,208:INFO:_display_container: 2
2024-02-08 03:17:46,208:INFO:<catboost.core.CatBoostClassifier object at 0x0000023698F8DC60>
2024-02-08 03:17:46,208:INFO:create_model() successfully completed......................................
2024-02-08 03:17:46,368:INFO:SubProcess create_model() end ==================================
2024-02-08 03:17:46,368:INFO:choose_better activated
2024-02-08 03:17:46,372:INFO:SubProcess create_model() called ==================================
2024-02-08 03:17:46,372:INFO:Initializing create_model()
2024-02-08 03:17:46,372:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=<catboost.core.CatBoostClassifier object at 0x0000023698F22110>, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 03:17:46,372:INFO:Checking exceptions
2024-02-08 03:17:46,374:INFO:Importing libraries
2024-02-08 03:17:46,374:INFO:Copying training dataset
2024-02-08 03:17:46,489:INFO:Defining folds
2024-02-08 03:17:46,490:INFO:Declaring metric variables
2024-02-08 03:17:46,490:INFO:Importing untrained model
2024-02-08 03:17:46,490:INFO:Declaring custom model
2024-02-08 03:17:46,490:INFO:CatBoost Classifier Imported successfully
2024-02-08 03:17:46,491:INFO:Starting cross validation
2024-02-08 03:17:46,502:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 03:20:33,399:INFO:Calculating mean and std
2024-02-08 03:20:33,399:INFO:Creating metrics dataframe
2024-02-08 03:20:33,402:INFO:Finalizing model
2024-02-08 03:20:36,983:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 03:20:37,072:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030307 seconds.
2024-02-08 03:20:37,072:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-02-08 03:20:37,072:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-02-08 03:20:37,073:INFO:[LightGBM] [Info] Total Bins 31359
2024-02-08 03:20:37,074:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 123
2024-02-08 03:20:37,075:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 03:20:58,848:INFO:Uploading results into container
2024-02-08 03:20:58,850:INFO:Uploading model into container now
2024-02-08 03:20:58,850:INFO:_master_model_container: 32
2024-02-08 03:20:58,850:INFO:_display_container: 3
2024-02-08 03:20:58,850:INFO:<catboost.core.CatBoostClassifier object at 0x000002369A8352A0>
2024-02-08 03:20:58,850:INFO:create_model() successfully completed......................................
2024-02-08 03:20:58,997:INFO:SubProcess create_model() end ==================================
2024-02-08 03:20:58,997:INFO:<catboost.core.CatBoostClassifier object at 0x000002369A8352A0> result for Accuracy is 0.9408
2024-02-08 03:20:58,998:INFO:<catboost.core.CatBoostClassifier object at 0x0000023698F8DC60> result for Accuracy is 0.9387
2024-02-08 03:20:58,998:INFO:<catboost.core.CatBoostClassifier object at 0x000002369A8352A0> is best model
2024-02-08 03:20:58,998:INFO:choose_better completed
2024-02-08 03:20:58,998:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-02-08 03:20:59,011:INFO:_master_model_container: 32
2024-02-08 03:20:59,011:INFO:_display_container: 2
2024-02-08 03:20:59,011:INFO:<catboost.core.CatBoostClassifier object at 0x000002369A8352A0>
2024-02-08 03:20:59,011:INFO:tune_model() successfully completed......................................
2024-02-08 03:20:59,183:INFO:Initializing tune_model()
2024-02-08 03:20:59,183:INFO:tune_model(estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6300, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>)
2024-02-08 03:20:59,183:INFO:Checking exceptions
2024-02-08 03:20:59,246:INFO:Copying training dataset
2024-02-08 03:20:59,334:INFO:Checking base model
2024-02-08 03:20:59,334:INFO:Base model : Light Gradient Boosting Machine
2024-02-08 03:20:59,338:INFO:Declaring metric variables
2024-02-08 03:20:59,342:INFO:Defining Hyperparameters
2024-02-08 03:20:59,505:INFO:Tuning with n_jobs=-1
2024-02-08 03:20:59,506:INFO:Initializing RandomizedSearchCV
2024-02-08 03:29:01,833:INFO:best_params: {'actual_estimator__reg_lambda': 0.3, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 8, 'actual_estimator__n_estimators': 170, 'actual_estimator__min_split_gain': 0.4, 'actual_estimator__min_child_samples': 21, 'actual_estimator__learning_rate': 0.3, 'actual_estimator__feature_fraction': 0.6, 'actual_estimator__bagging_freq': 6, 'actual_estimator__bagging_fraction': 0.8}
2024-02-08 03:29:01,834:INFO:Hyperparameter search completed
2024-02-08 03:29:01,835:INFO:SubProcess create_model() called ==================================
2024-02-08 03:29:01,836:INFO:Initializing create_model()
2024-02-08 03:29:01,836:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6300, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023698F22200>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 0.3, 'reg_alpha': 0.7, 'num_leaves': 8, 'n_estimators': 170, 'min_split_gain': 0.4, 'min_child_samples': 21, 'learning_rate': 0.3, 'feature_fraction': 0.6, 'bagging_freq': 6, 'bagging_fraction': 0.8})
2024-02-08 03:29:01,836:INFO:Checking exceptions
2024-02-08 03:29:01,836:INFO:Importing libraries
2024-02-08 03:29:01,836:INFO:Copying training dataset
2024-02-08 03:29:01,975:INFO:Defining folds
2024-02-08 03:29:01,976:INFO:Declaring metric variables
2024-02-08 03:29:01,980:INFO:Importing untrained model
2024-02-08 03:29:01,980:INFO:Declaring custom model
2024-02-08 03:29:01,985:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-08 03:29:01,993:INFO:Starting cross validation
2024-02-08 03:29:02,004:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 03:29:39,471:INFO:Calculating mean and std
2024-02-08 03:29:39,473:INFO:Creating metrics dataframe
2024-02-08 03:29:39,480:INFO:Finalizing model
2024-02-08 03:29:43,190:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 03:29:43,285:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030921 seconds.
2024-02-08 03:29:43,285:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-02-08 03:29:43,285:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-02-08 03:29:43,286:INFO:[LightGBM] [Info] Total Bins 31359
2024-02-08 03:29:43,286:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 123
2024-02-08 03:29:43,287:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 03:29:45,892:INFO:[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6
2024-02-08 03:29:45,892:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-02-08 03:29:45,892:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-02-08 03:29:45,979:INFO:[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6
2024-02-08 03:29:45,979:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-02-08 03:29:45,979:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-02-08 03:29:45,979:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 03:29:45,994:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003516 seconds.
2024-02-08 03:29:45,994:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-02-08 03:29:45,994:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-02-08 03:29:45,994:INFO:[LightGBM] [Info] Total Bins 6375
2024-02-08 03:29:45,995:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 25
2024-02-08 03:29:45,996:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 03:29:46,677:INFO:Uploading results into container
2024-02-08 03:29:46,678:INFO:Uploading model into container now
2024-02-08 03:29:46,679:INFO:_master_model_container: 33
2024-02-08 03:29:46,680:INFO:_display_container: 3
2024-02-08 03:29:46,682:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.6,
               importance_type='split', learning_rate=0.3, max_depth=-1,
               min_child_samples=21, min_child_weight=0.001, min_split_gain=0.4,
               n_estimators=170, n_jobs=-1, num_leaves=8, objective=None,
               random_state=6300, reg_alpha=0.7, reg_lambda=0.3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-02-08 03:29:46,683:INFO:create_model() successfully completed......................................
2024-02-08 03:29:46,848:INFO:SubProcess create_model() end ==================================
2024-02-08 03:29:46,848:INFO:choose_better activated
2024-02-08 03:29:46,852:INFO:SubProcess create_model() called ==================================
2024-02-08 03:29:46,853:INFO:Initializing create_model()
2024-02-08 03:29:46,853:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6300, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 03:29:46,853:INFO:Checking exceptions
2024-02-08 03:29:46,855:INFO:Importing libraries
2024-02-08 03:29:46,855:INFO:Copying training dataset
2024-02-08 03:29:46,961:INFO:Defining folds
2024-02-08 03:29:46,961:INFO:Declaring metric variables
2024-02-08 03:29:46,961:INFO:Importing untrained model
2024-02-08 03:29:46,961:INFO:Declaring custom model
2024-02-08 03:29:46,962:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-08 03:29:46,962:INFO:Starting cross validation
2024-02-08 03:29:46,971:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 03:30:27,095:INFO:Calculating mean and std
2024-02-08 03:30:27,096:INFO:Creating metrics dataframe
2024-02-08 03:30:27,098:INFO:Finalizing model
2024-02-08 03:30:30,754:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 03:30:30,847:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074865 seconds.
2024-02-08 03:30:30,847:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-08 03:30:30,850:INFO:[LightGBM] [Info] Total Bins 31359
2024-02-08 03:30:30,851:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 123
2024-02-08 03:30:30,852:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 03:30:33,624:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 03:30:33,635:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003687 seconds.
2024-02-08 03:30:33,636:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-02-08 03:30:33,636:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-02-08 03:30:33,636:INFO:[LightGBM] [Info] Total Bins 6375
2024-02-08 03:30:33,636:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 25
2024-02-08 03:30:33,637:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 03:30:34,155:INFO:Uploading results into container
2024-02-08 03:30:34,155:INFO:Uploading model into container now
2024-02-08 03:30:34,156:INFO:_master_model_container: 34
2024-02-08 03:30:34,156:INFO:_display_container: 4
2024-02-08 03:30:34,157:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6300, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-02-08 03:30:34,157:INFO:create_model() successfully completed......................................
2024-02-08 03:30:34,311:INFO:SubProcess create_model() end ==================================
2024-02-08 03:30:34,312:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6300, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.9376
2024-02-08 03:30:34,312:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.6,
               importance_type='split', learning_rate=0.3, max_depth=-1,
               min_child_samples=21, min_child_weight=0.001, min_split_gain=0.4,
               n_estimators=170, n_jobs=-1, num_leaves=8, objective=None,
               random_state=6300, reg_alpha=0.7, reg_lambda=0.3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.9388
2024-02-08 03:30:34,313:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.6,
               importance_type='split', learning_rate=0.3, max_depth=-1,
               min_child_samples=21, min_child_weight=0.001, min_split_gain=0.4,
               n_estimators=170, n_jobs=-1, num_leaves=8, objective=None,
               random_state=6300, reg_alpha=0.7, reg_lambda=0.3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2024-02-08 03:30:34,313:INFO:choose_better completed
2024-02-08 03:30:34,324:INFO:_master_model_container: 34
2024-02-08 03:30:34,324:INFO:_display_container: 3
2024-02-08 03:30:34,324:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.6,
               importance_type='split', learning_rate=0.3, max_depth=-1,
               min_child_samples=21, min_child_weight=0.001, min_split_gain=0.4,
               n_estimators=170, n_jobs=-1, num_leaves=8, objective=None,
               random_state=6300, reg_alpha=0.7, reg_lambda=0.3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-02-08 03:30:34,324:INFO:tune_model() successfully completed......................................
2024-02-08 03:30:34,499:INFO:Initializing ensemble_model()
2024-02-08 03:30:34,500:INFO:ensemble_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=<catboost.core.CatBoostClassifier object at 0x000002369A8352A0>, method=Bagging, fold=None, n_estimators=10, round=4, choose_better=True, optimize=Accuracy, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2024-02-08 03:30:34,500:INFO:Checking exceptions
2024-02-08 03:30:34,561:INFO:Importing libraries
2024-02-08 03:30:34,561:INFO:Copying training dataset
2024-02-08 03:30:34,561:INFO:Checking base model
2024-02-08 03:30:34,561:INFO:Base model : CatBoost Classifier
2024-02-08 03:30:34,569:INFO:Importing untrained ensembler
2024-02-08 03:30:34,569:INFO:Ensemble method set to Bagging
2024-02-08 03:30:34,569:INFO:SubProcess create_model() called ==================================
2024-02-08 03:30:34,570:INFO:Initializing create_model()
2024-02-08 03:30:34,570:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=BaggingClassifier(base_estimator='deprecated', bootstrap=True,
                  bootstrap_features=False,
                  estimator=<catboost.core.CatBoostClassifier object at 0x000002369A8352A0>,
                  max_features=1.0, max_samples=1.0, n_estimators=10,
                  n_jobs=None, oob_score=False, random_state=6300, verbose=0,
                  warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E27B2EC0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 03:30:34,570:INFO:Checking exceptions
2024-02-08 03:30:34,570:INFO:Importing libraries
2024-02-08 03:30:34,571:INFO:Copying training dataset
2024-02-08 03:30:34,664:INFO:Defining folds
2024-02-08 03:30:34,664:INFO:Declaring metric variables
2024-02-08 03:30:34,668:INFO:Importing untrained model
2024-02-08 03:30:34,668:INFO:Declaring custom model
2024-02-08 03:30:34,672:INFO:Bagging Classifier Imported successfully
2024-02-08 03:30:34,680:INFO:Starting cross validation
2024-02-08 03:30:34,689:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 03:52:45,868:INFO:Calculating mean and std
2024-02-08 03:52:45,871:INFO:Creating metrics dataframe
2024-02-08 03:52:45,880:INFO:Finalizing model
2024-02-08 03:52:49,003:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 03:52:49,082:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060653 seconds.
2024-02-08 03:52:49,082:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-08 03:52:49,084:INFO:[LightGBM] [Info] Total Bins 31359
2024-02-08 03:52:49,084:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 123
2024-02-08 03:52:49,085:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 03:55:57,891:INFO:Uploading results into container
2024-02-08 03:55:57,892:INFO:Uploading model into container now
2024-02-08 03:55:57,893:INFO:_master_model_container: 35
2024-02-08 03:55:57,894:INFO:_display_container: 3
2024-02-08 03:55:57,894:INFO:BaggingClassifier(base_estimator='deprecated', bootstrap=True,
                  bootstrap_features=False,
                  estimator=<catboost.core.CatBoostClassifier object at 0x000002369A894340>,
                  max_features=1.0, max_samples=1.0, n_estimators=10,
                  n_jobs=None, oob_score=False, random_state=6300, verbose=0,
                  warm_start=False)
2024-02-08 03:55:57,895:INFO:create_model() successfully completed......................................
2024-02-08 03:55:58,031:INFO:SubProcess create_model() end ==================================
2024-02-08 03:55:58,032:INFO:choose_better activated
2024-02-08 03:55:58,035:INFO:SubProcess create_model() called ==================================
2024-02-08 03:55:58,036:INFO:Initializing create_model()
2024-02-08 03:55:58,036:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=<catboost.core.CatBoostClassifier object at 0x000002369A8352A0>, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 03:55:58,036:INFO:Checking exceptions
2024-02-08 03:55:58,038:INFO:Importing libraries
2024-02-08 03:55:58,038:INFO:Copying training dataset
2024-02-08 03:55:58,151:INFO:Defining folds
2024-02-08 03:55:58,151:INFO:Declaring metric variables
2024-02-08 03:55:58,151:INFO:Importing untrained model
2024-02-08 03:55:58,151:INFO:Declaring custom model
2024-02-08 03:55:58,151:INFO:CatBoost Classifier Imported successfully
2024-02-08 03:55:58,152:INFO:Starting cross validation
2024-02-08 03:55:58,164:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 03:58:53,890:INFO:Calculating mean and std
2024-02-08 03:58:53,891:INFO:Creating metrics dataframe
2024-02-08 03:58:53,894:INFO:Finalizing model
2024-02-08 03:58:56,821:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 03:58:56,886:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049613 seconds.
2024-02-08 03:58:56,886:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-08 03:58:56,888:INFO:[LightGBM] [Info] Total Bins 31359
2024-02-08 03:58:56,890:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 123
2024-02-08 03:58:56,891:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 03:59:19,396:INFO:Uploading results into container
2024-02-08 03:59:19,397:INFO:Uploading model into container now
2024-02-08 03:59:19,397:INFO:_master_model_container: 36
2024-02-08 03:59:19,397:INFO:_display_container: 4
2024-02-08 03:59:19,397:INFO:<catboost.core.CatBoostClassifier object at 0x000002369A8BF2B0>
2024-02-08 03:59:19,397:INFO:create_model() successfully completed......................................
2024-02-08 03:59:19,540:INFO:SubProcess create_model() end ==================================
2024-02-08 03:59:19,541:INFO:<catboost.core.CatBoostClassifier object at 0x000002369A8BF2B0> result for Accuracy is 0.9404
2024-02-08 03:59:19,541:INFO:BaggingClassifier(base_estimator='deprecated', bootstrap=True,
                  bootstrap_features=False,
                  estimator=<catboost.core.CatBoostClassifier object at 0x000002369A894340>,
                  max_features=1.0, max_samples=1.0, n_estimators=10,
                  n_jobs=None, oob_score=False, random_state=6300, verbose=0,
                  warm_start=False) result for Accuracy is 0.9405
2024-02-08 03:59:19,542:INFO:BaggingClassifier(base_estimator='deprecated', bootstrap=True,
                  bootstrap_features=False,
                  estimator=<catboost.core.CatBoostClassifier object at 0x000002369A894340>,
                  max_features=1.0, max_samples=1.0, n_estimators=10,
                  n_jobs=None, oob_score=False, random_state=6300, verbose=0,
                  warm_start=False) is best model
2024-02-08 03:59:19,542:INFO:choose_better completed
2024-02-08 03:59:19,554:INFO:_master_model_container: 36
2024-02-08 03:59:19,555:INFO:_display_container: 3
2024-02-08 03:59:19,555:INFO:BaggingClassifier(base_estimator='deprecated', bootstrap=True,
                  bootstrap_features=False,
                  estimator=<catboost.core.CatBoostClassifier object at 0x000002369A894340>,
                  max_features=1.0, max_samples=1.0, n_estimators=10,
                  n_jobs=None, oob_score=False, random_state=6300, verbose=0,
                  warm_start=False)
2024-02-08 03:59:19,555:INFO:ensemble_model() successfully completed......................................
2024-02-08 03:59:19,688:INFO:Initializing ensemble_model()
2024-02-08 03:59:19,688:INFO:ensemble_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.6,
               importance_type='split', learning_rate=0.3, max_depth=-1,
               min_child_samples=21, min_child_weight=0.001, min_split_gain=0.4,
               n_estimators=170, n_jobs=-1, num_leaves=8, objective=None,
               random_state=6300, reg_alpha=0.7, reg_lambda=0.3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), method=Bagging, fold=None, n_estimators=10, round=4, choose_better=True, optimize=Accuracy, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2024-02-08 03:59:19,688:INFO:Checking exceptions
2024-02-08 03:59:19,741:INFO:Importing libraries
2024-02-08 03:59:19,741:INFO:Copying training dataset
2024-02-08 03:59:19,742:INFO:Checking base model
2024-02-08 03:59:19,742:INFO:Base model : Light Gradient Boosting Machine
2024-02-08 03:59:19,750:INFO:Importing untrained ensembler
2024-02-08 03:59:19,750:INFO:Ensemble method set to Bagging
2024-02-08 03:59:19,751:INFO:SubProcess create_model() called ==================================
2024-02-08 03:59:19,752:INFO:Initializing create_model()
2024-02-08 03:59:19,753:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=BaggingClassifier(base_estimator='deprecated', bootstrap=True,
                  bootstrap_features=False,
                  estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=6,
                                           boosting_type='gbdt',
                                           class_weight=None,
                                           colsample_bytree=1.0,
                                           feature_fraction=0.6,
                                           importance_type='split',
                                           learning_rate=0.3, max_depth=-1,
                                           min_child_samples=21,
                                           min_child_weight=0.001,
                                           min_split_gain=0.4, n_estimators=170,
                                           n_jobs=-1, num_leaves=8,
                                           objective=None, random_state=6300,
                                           reg_alpha=0.7, reg_lambda=0.3,
                                           subsample=1.0,
                                           subsample_for_bin=200000,
                                           subsample_freq=0),
                  max_features=1.0, max_samples=1.0, n_estimators=10,
                  n_jobs=None, oob_score=False, random_state=6300, verbose=0,
                  warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023698F220E0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 03:59:19,753:INFO:Checking exceptions
2024-02-08 03:59:19,753:INFO:Importing libraries
2024-02-08 03:59:19,753:INFO:Copying training dataset
2024-02-08 03:59:19,854:INFO:Defining folds
2024-02-08 03:59:19,854:INFO:Declaring metric variables
2024-02-08 03:59:19,858:INFO:Importing untrained model
2024-02-08 03:59:19,858:INFO:Declaring custom model
2024-02-08 03:59:19,863:INFO:Bagging Classifier Imported successfully
2024-02-08 03:59:19,871:INFO:Starting cross validation
2024-02-08 03:59:19,886:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 04:01:12,250:INFO:Calculating mean and std
2024-02-08 04:01:12,252:INFO:Creating metrics dataframe
2024-02-08 04:01:12,265:INFO:Finalizing model
2024-02-08 04:01:17,748:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 04:01:17,840:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071331 seconds.
2024-02-08 04:01:17,840:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-08 04:01:17,842:INFO:[LightGBM] [Info] Total Bins 31359
2024-02-08 04:01:17,844:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 123
2024-02-08 04:01:17,845:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 04:01:20,151:INFO:[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6
2024-02-08 04:01:20,152:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-02-08 04:01:20,152:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-02-08 04:01:20,247:INFO:[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6
2024-02-08 04:01:20,247:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-02-08 04:01:20,247:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-02-08 04:01:20,247:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 04:01:20,262:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011669 seconds.
2024-02-08 04:01:20,262:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-08 04:01:20,263:INFO:[LightGBM] [Info] Total Bins 6375
2024-02-08 04:01:20,263:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 25
2024-02-08 04:01:20,264:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499682 -> initscore=-0.001273
2024-02-08 04:01:20,264:INFO:[LightGBM] [Info] Start training from score -0.001273
2024-02-08 04:01:20,965:INFO:[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6
2024-02-08 04:01:20,965:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-02-08 04:01:20,965:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-02-08 04:01:21,094:INFO:[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6
2024-02-08 04:01:21,095:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-02-08 04:01:21,095:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-02-08 04:01:21,095:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 04:01:21,117:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004545 seconds.
2024-02-08 04:01:21,117:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-02-08 04:01:21,117:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-02-08 04:01:21,117:INFO:[LightGBM] [Info] Total Bins 6375
2024-02-08 04:01:21,117:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 25
2024-02-08 04:01:21,119:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501045 -> initscore=0.004181
2024-02-08 04:01:21,120:INFO:[LightGBM] [Info] Start training from score 0.004181
2024-02-08 04:01:21,871:INFO:[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6
2024-02-08 04:01:21,871:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-02-08 04:01:21,871:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-02-08 04:01:21,974:INFO:[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6
2024-02-08 04:01:21,974:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-02-08 04:01:21,974:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-02-08 04:01:21,975:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 04:01:21,993:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003590 seconds.
2024-02-08 04:01:21,993:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-02-08 04:01:21,993:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-02-08 04:01:21,993:INFO:[LightGBM] [Info] Total Bins 6375
2024-02-08 04:01:21,994:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 25
2024-02-08 04:01:21,995:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502511 -> initscore=0.010044
2024-02-08 04:01:21,995:INFO:[LightGBM] [Info] Start training from score 0.010044
2024-02-08 04:01:22,964:INFO:[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6
2024-02-08 04:01:22,964:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-02-08 04:01:22,964:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-02-08 04:01:23,077:INFO:[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6
2024-02-08 04:01:23,077:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-02-08 04:01:23,077:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-02-08 04:01:23,077:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 04:01:23,094:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005243 seconds.
2024-02-08 04:01:23,094:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-02-08 04:01:23,094:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-02-08 04:01:23,095:INFO:[LightGBM] [Info] Total Bins 6375
2024-02-08 04:01:23,095:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 25
2024-02-08 04:01:23,096:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863
2024-02-08 04:01:23,097:INFO:[LightGBM] [Info] Start training from score -0.001863
2024-02-08 04:01:24,121:INFO:[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6
2024-02-08 04:01:24,121:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-02-08 04:01:24,121:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-02-08 04:01:24,261:INFO:[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6
2024-02-08 04:01:24,261:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-02-08 04:01:24,262:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-02-08 04:01:24,262:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 04:01:24,282:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004959 seconds.
2024-02-08 04:01:24,282:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-02-08 04:01:24,282:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-02-08 04:01:24,283:INFO:[LightGBM] [Info] Total Bins 6375
2024-02-08 04:01:24,283:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 25
2024-02-08 04:01:24,285:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498455 -> initscore=-0.006181
2024-02-08 04:01:24,285:INFO:[LightGBM] [Info] Start training from score -0.006181
2024-02-08 04:01:25,359:INFO:[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6
2024-02-08 04:01:25,359:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-02-08 04:01:25,359:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-02-08 04:01:25,483:INFO:[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6
2024-02-08 04:01:25,483:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-02-08 04:01:25,483:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-02-08 04:01:25,484:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 04:01:25,502:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014411 seconds.
2024-02-08 04:01:25,503:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-08 04:01:25,503:INFO:[LightGBM] [Info] Total Bins 6375
2024-02-08 04:01:25,503:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 25
2024-02-08 04:01:25,505:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498637 -> initscore=-0.005454
2024-02-08 04:01:25,505:INFO:[LightGBM] [Info] Start training from score -0.005454
2024-02-08 04:01:26,385:INFO:[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6
2024-02-08 04:01:26,385:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-02-08 04:01:26,385:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-02-08 04:01:26,532:INFO:[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6
2024-02-08 04:01:26,534:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-02-08 04:01:26,534:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-02-08 04:01:26,534:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 04:01:26,554:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004720 seconds.
2024-02-08 04:01:26,554:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-02-08 04:01:26,554:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-02-08 04:01:26,555:INFO:[LightGBM] [Info] Total Bins 6375
2024-02-08 04:01:26,555:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 25
2024-02-08 04:01:26,557:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499875 -> initscore=-0.000500
2024-02-08 04:01:26,557:INFO:[LightGBM] [Info] Start training from score -0.000500
2024-02-08 04:01:27,606:INFO:[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6
2024-02-08 04:01:27,607:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-02-08 04:01:27,607:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-02-08 04:01:27,722:INFO:[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6
2024-02-08 04:01:27,722:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-02-08 04:01:27,722:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-02-08 04:01:27,722:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 04:01:27,747:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005579 seconds.
2024-02-08 04:01:27,747:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-02-08 04:01:27,747:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-02-08 04:01:27,748:INFO:[LightGBM] [Info] Total Bins 6375
2024-02-08 04:01:27,748:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 25
2024-02-08 04:01:27,750:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498205 -> initscore=-0.007181
2024-02-08 04:01:27,750:INFO:[LightGBM] [Info] Start training from score -0.007181
2024-02-08 04:01:28,783:INFO:[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6
2024-02-08 04:01:28,783:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-02-08 04:01:28,783:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-02-08 04:01:28,929:INFO:[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6
2024-02-08 04:01:28,930:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-02-08 04:01:28,930:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-02-08 04:01:28,930:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 04:01:28,953:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018617 seconds.
2024-02-08 04:01:28,953:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-08 04:01:28,954:INFO:[LightGBM] [Info] Total Bins 6375
2024-02-08 04:01:28,954:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 25
2024-02-08 04:01:28,956:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498057 -> initscore=-0.007772
2024-02-08 04:01:28,956:INFO:[LightGBM] [Info] Start training from score -0.007772
2024-02-08 04:01:29,776:INFO:[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6
2024-02-08 04:01:29,776:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-02-08 04:01:29,776:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-02-08 04:01:29,912:INFO:[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6
2024-02-08 04:01:29,912:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-02-08 04:01:29,912:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-02-08 04:01:29,912:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 04:01:29,933:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005730 seconds.
2024-02-08 04:01:29,933:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-02-08 04:01:29,933:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-02-08 04:01:29,933:INFO:[LightGBM] [Info] Total Bins 6375
2024-02-08 04:01:29,933:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 25
2024-02-08 04:01:29,936:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500545 -> initscore=0.002181
2024-02-08 04:01:29,936:INFO:[LightGBM] [Info] Start training from score 0.002181
2024-02-08 04:01:30,909:INFO:Uploading results into container
2024-02-08 04:01:30,912:INFO:Uploading model into container now
2024-02-08 04:01:30,913:INFO:_master_model_container: 37
2024-02-08 04:01:30,913:INFO:_display_container: 4
2024-02-08 04:01:30,917:INFO:BaggingClassifier(base_estimator='deprecated', bootstrap=True,
                  bootstrap_features=False,
                  estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=6,
                                           boosting_type='gbdt',
                                           class_weight=None,
                                           colsample_bytree=1.0,
                                           feature_fraction=0.6,
                                           importance_type='split',
                                           learning_rate=0.3, max_depth=-1,
                                           min_child_samples=21,
                                           min_child_weight=0.001,
                                           min_split_gain=0.4, n_estimators=170,
                                           n_jobs=-1, num_leaves=8,
                                           objective=None, random_state=6300,
                                           reg_alpha=0.7, reg_lambda=0.3,
                                           subsample=1.0,
                                           subsample_for_bin=200000,
                                           subsample_freq=0),
                  max_features=1.0, max_samples=1.0, n_estimators=10,
                  n_jobs=None, oob_score=False, random_state=6300, verbose=0,
                  warm_start=False)
2024-02-08 04:01:30,917:INFO:create_model() successfully completed......................................
2024-02-08 04:01:31,085:INFO:SubProcess create_model() end ==================================
2024-02-08 04:01:31,085:INFO:choose_better activated
2024-02-08 04:01:31,090:INFO:SubProcess create_model() called ==================================
2024-02-08 04:01:31,091:INFO:Initializing create_model()
2024-02-08 04:01:31,092:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.6,
               importance_type='split', learning_rate=0.3, max_depth=-1,
               min_child_samples=21, min_child_weight=0.001, min_split_gain=0.4,
               n_estimators=170, n_jobs=-1, num_leaves=8, objective=None,
               random_state=6300, reg_alpha=0.7, reg_lambda=0.3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 04:01:31,092:INFO:Checking exceptions
2024-02-08 04:01:31,094:INFO:Importing libraries
2024-02-08 04:01:31,094:INFO:Copying training dataset
2024-02-08 04:01:31,213:INFO:Defining folds
2024-02-08 04:01:31,213:INFO:Declaring metric variables
2024-02-08 04:01:31,213:INFO:Importing untrained model
2024-02-08 04:01:31,213:INFO:Declaring custom model
2024-02-08 04:01:31,214:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-08 04:01:31,215:INFO:Starting cross validation
2024-02-08 04:01:31,225:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 04:02:10,472:INFO:Calculating mean and std
2024-02-08 04:02:10,473:INFO:Creating metrics dataframe
2024-02-08 04:02:10,477:INFO:Finalizing model
2024-02-08 04:02:13,629:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 04:02:13,701:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054768 seconds.
2024-02-08 04:02:13,701:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-08 04:02:13,703:INFO:[LightGBM] [Info] Total Bins 31359
2024-02-08 04:02:13,704:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 123
2024-02-08 04:02:13,705:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 04:02:16,221:INFO:[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6
2024-02-08 04:02:16,221:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-02-08 04:02:16,221:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-02-08 04:02:16,328:INFO:[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6
2024-02-08 04:02:16,328:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-02-08 04:02:16,329:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-02-08 04:02:16,329:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 04:02:16,349:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005255 seconds.
2024-02-08 04:02:16,350:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-02-08 04:02:16,350:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-02-08 04:02:16,350:INFO:[LightGBM] [Info] Total Bins 6375
2024-02-08 04:02:16,350:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 25
2024-02-08 04:02:16,352:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 04:02:17,336:INFO:Uploading results into container
2024-02-08 04:02:17,337:INFO:Uploading model into container now
2024-02-08 04:02:17,338:INFO:_master_model_container: 38
2024-02-08 04:02:17,338:INFO:_display_container: 5
2024-02-08 04:02:17,339:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.6,
               importance_type='split', learning_rate=0.3, max_depth=-1,
               min_child_samples=21, min_child_weight=0.001, min_split_gain=0.4,
               n_estimators=170, n_jobs=-1, num_leaves=8, objective=None,
               random_state=6300, reg_alpha=0.7, reg_lambda=0.3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-02-08 04:02:17,340:INFO:create_model() successfully completed......................................
2024-02-08 04:02:17,504:INFO:SubProcess create_model() end ==================================
2024-02-08 04:02:17,505:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.6,
               importance_type='split', learning_rate=0.3, max_depth=-1,
               min_child_samples=21, min_child_weight=0.001, min_split_gain=0.4,
               n_estimators=170, n_jobs=-1, num_leaves=8, objective=None,
               random_state=6300, reg_alpha=0.7, reg_lambda=0.3, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.9387
2024-02-08 04:02:17,507:INFO:BaggingClassifier(base_estimator='deprecated', bootstrap=True,
                  bootstrap_features=False,
                  estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=6,
                                           boosting_type='gbdt',
                                           class_weight=None,
                                           colsample_bytree=1.0,
                                           feature_fraction=0.6,
                                           importance_type='split',
                                           learning_rate=0.3, max_depth=-1,
                                           min_child_samples=21,
                                           min_child_weight=0.001,
                                           min_split_gain=0.4, n_estimators=170,
                                           n_jobs=-1, num_leaves=8,
                                           objective=None, random_state=6300,
                                           reg_alpha=0.7, reg_lambda=0.3,
                                           subsample=1.0,
                                           subsample_for_bin=200000,
                                           subsample_freq=0),
                  max_features=1.0, max_samples=1.0, n_estimators=10,
                  n_jobs=None, oob_score=False, random_state=6300, verbose=0,
                  warm_start=False) result for Accuracy is 0.9401
2024-02-08 04:02:17,509:INFO:BaggingClassifier(base_estimator='deprecated', bootstrap=True,
                  bootstrap_features=False,
                  estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=6,
                                           boosting_type='gbdt',
                                           class_weight=None,
                                           colsample_bytree=1.0,
                                           feature_fraction=0.6,
                                           importance_type='split',
                                           learning_rate=0.3, max_depth=-1,
                                           min_child_samples=21,
                                           min_child_weight=0.001,
                                           min_split_gain=0.4, n_estimators=170,
                                           n_jobs=-1, num_leaves=8,
                                           objective=None, random_state=6300,
                                           reg_alpha=0.7, reg_lambda=0.3,
                                           subsample=1.0,
                                           subsample_for_bin=200000,
                                           subsample_freq=0),
                  max_features=1.0, max_samples=1.0, n_estimators=10,
                  n_jobs=None, oob_score=False, random_state=6300, verbose=0,
                  warm_start=False) is best model
2024-02-08 04:02:17,509:INFO:choose_better completed
2024-02-08 04:02:17,520:INFO:_master_model_container: 38
2024-02-08 04:02:17,520:INFO:_display_container: 4
2024-02-08 04:02:17,522:INFO:BaggingClassifier(base_estimator='deprecated', bootstrap=True,
                  bootstrap_features=False,
                  estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=6,
                                           boosting_type='gbdt',
                                           class_weight=None,
                                           colsample_bytree=1.0,
                                           feature_fraction=0.6,
                                           importance_type='split',
                                           learning_rate=0.3, max_depth=-1,
                                           min_child_samples=21,
                                           min_child_weight=0.001,
                                           min_split_gain=0.4, n_estimators=170,
                                           n_jobs=-1, num_leaves=8,
                                           objective=None, random_state=6300,
                                           reg_alpha=0.7, reg_lambda=0.3,
                                           subsample=1.0,
                                           subsample_for_bin=200000,
                                           subsample_freq=0),
                  max_features=1.0, max_samples=1.0, n_estimators=10,
                  n_jobs=None, oob_score=False, random_state=6300, verbose=0,
                  warm_start=False)
2024-02-08 04:02:17,522:INFO:ensemble_model() successfully completed......................................
2024-02-08 04:02:17,700:INFO:Initializing blend_models()
2024-02-08 04:02:17,700:INFO:blend_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator_list=[<catboost.core.CatBoostClassifier object at 0x0000023698F22110>, LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6300, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)], fold=None, round=4, choose_better=True, optimize=Accuracy, method=auto, weights=None, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2024-02-08 04:02:17,701:INFO:Checking exceptions
2024-02-08 04:02:17,769:INFO:Importing libraries
2024-02-08 04:02:17,770:INFO:Copying training dataset
2024-02-08 04:02:17,775:INFO:Getting model names
2024-02-08 04:02:17,781:INFO:SubProcess create_model() called ==================================
2024-02-08 04:02:17,784:INFO:Initializing create_model()
2024-02-08 04:02:17,784:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=VotingClassifier(estimators=[('CatBoost Classifier',
                              <catboost.core.CatBoostClassifier object at 0x0000023698F22110>),
                             ('Light Gradient Boosting Machine',
                              LGBMClassifier(boosting_type='gbdt',
                                             class_weight=None,
                                             colsample_bytree=1.0,
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=6300, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002369A81BD30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 04:02:17,784:INFO:Checking exceptions
2024-02-08 04:02:17,784:INFO:Importing libraries
2024-02-08 04:02:17,784:INFO:Copying training dataset
2024-02-08 04:02:17,912:INFO:Defining folds
2024-02-08 04:02:17,912:INFO:Declaring metric variables
2024-02-08 04:02:17,916:INFO:Importing untrained model
2024-02-08 04:02:17,916:INFO:Declaring custom model
2024-02-08 04:02:17,922:INFO:Voting Classifier Imported successfully
2024-02-08 04:02:17,931:INFO:Starting cross validation
2024-02-08 04:02:17,942:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 04:05:06,997:INFO:Calculating mean and std
2024-02-08 04:05:06,998:INFO:Creating metrics dataframe
2024-02-08 04:05:07,005:INFO:Finalizing model
2024-02-08 04:05:10,641:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 04:05:10,738:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.078696 seconds.
2024-02-08 04:05:10,738:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-08 04:05:10,739:INFO:[LightGBM] [Info] Total Bins 31359
2024-02-08 04:05:10,740:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 123
2024-02-08 04:05:10,741:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 04:05:32,683:INFO:Uploading results into container
2024-02-08 04:05:32,684:INFO:Uploading model into container now
2024-02-08 04:05:32,685:INFO:_master_model_container: 39
2024-02-08 04:05:32,685:INFO:_display_container: 4
2024-02-08 04:05:32,691:INFO:VotingClassifier(estimators=[('CatBoost Classifier',
                              <catboost.core.CatBoostClassifier object at 0x000002369A8C4FA0>),
                             ('Light Gradient Boosting Machine',
                              LGBMClassifier(boosting_type='gbdt',
                                             class_weight=None,
                                             colsample_bytree=1.0,
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=6300, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None)
2024-02-08 04:05:32,691:INFO:create_model() successfully completed......................................
2024-02-08 04:05:32,841:INFO:SubProcess create_model() end ==================================
2024-02-08 04:05:32,841:INFO:choose_better activated
2024-02-08 04:05:32,847:INFO:VotingClassifier(estimators=[('CatBoost Classifier',
                              <catboost.core.CatBoostClassifier object at 0x000002369A8C4FA0>),
                             ('Light Gradient Boosting Machine',
                              LGBMClassifier(boosting_type='gbdt',
                                             class_weight=None,
                                             colsample_bytree=1.0,
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=6300, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None) result for Accuracy is 0.9403
2024-02-08 04:05:32,848:INFO:SubProcess create_model() called ==================================
2024-02-08 04:05:32,848:INFO:Initializing create_model()
2024-02-08 04:05:32,848:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=<catboost.core.CatBoostClassifier object at 0x0000023698F22110>, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 04:05:32,848:INFO:Checking exceptions
2024-02-08 04:05:32,850:INFO:Importing libraries
2024-02-08 04:05:32,850:INFO:Copying training dataset
2024-02-08 04:05:32,963:INFO:Defining folds
2024-02-08 04:05:32,963:INFO:Declaring metric variables
2024-02-08 04:05:32,963:INFO:Importing untrained model
2024-02-08 04:05:32,964:INFO:Declaring custom model
2024-02-08 04:05:32,964:INFO:CatBoost Classifier Imported successfully
2024-02-08 04:05:32,965:INFO:Starting cross validation
2024-02-08 04:05:32,976:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 04:08:21,534:INFO:Calculating mean and std
2024-02-08 04:08:21,535:INFO:Creating metrics dataframe
2024-02-08 04:08:21,537:INFO:Finalizing model
2024-02-08 04:08:24,561:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 04:08:24,648:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024688 seconds.
2024-02-08 04:08:24,648:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-02-08 04:08:24,648:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-02-08 04:08:24,648:INFO:[LightGBM] [Info] Total Bins 31359
2024-02-08 04:08:24,649:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 123
2024-02-08 04:08:24,650:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 04:08:46,065:INFO:Uploading results into container
2024-02-08 04:08:46,066:INFO:Uploading model into container now
2024-02-08 04:08:46,066:INFO:_master_model_container: 40
2024-02-08 04:08:46,066:INFO:_display_container: 5
2024-02-08 04:08:46,066:INFO:<catboost.core.CatBoostClassifier object at 0x000002369A837B50>
2024-02-08 04:08:46,066:INFO:create_model() successfully completed......................................
2024-02-08 04:08:46,220:INFO:SubProcess create_model() end ==================================
2024-02-08 04:08:46,220:INFO:<catboost.core.CatBoostClassifier object at 0x000002369A837B50> result for Accuracy is 0.9411
2024-02-08 04:08:46,220:INFO:SubProcess create_model() called ==================================
2024-02-08 04:08:46,221:INFO:Initializing create_model()
2024-02-08 04:08:46,221:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6300, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 04:08:46,221:INFO:Checking exceptions
2024-02-08 04:08:46,223:INFO:Importing libraries
2024-02-08 04:08:46,223:INFO:Copying training dataset
2024-02-08 04:08:46,335:INFO:Defining folds
2024-02-08 04:08:46,335:INFO:Declaring metric variables
2024-02-08 04:08:46,335:INFO:Importing untrained model
2024-02-08 04:08:46,335:INFO:Declaring custom model
2024-02-08 04:08:46,336:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-08 04:08:46,336:INFO:Starting cross validation
2024-02-08 04:08:46,349:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 04:09:26,098:INFO:Calculating mean and std
2024-02-08 04:09:26,098:INFO:Creating metrics dataframe
2024-02-08 04:09:26,103:INFO:Finalizing model
2024-02-08 04:09:29,033:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 04:09:29,089:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043452 seconds.
2024-02-08 04:09:29,089:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-08 04:09:29,090:INFO:[LightGBM] [Info] Total Bins 31359
2024-02-08 04:09:29,091:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 123
2024-02-08 04:09:29,092:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 04:09:31,069:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 04:09:31,088:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005192 seconds.
2024-02-08 04:09:31,088:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-02-08 04:09:31,088:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-02-08 04:09:31,088:INFO:[LightGBM] [Info] Total Bins 6375
2024-02-08 04:09:31,089:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 25
2024-02-08 04:09:31,089:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 04:09:31,798:INFO:Uploading results into container
2024-02-08 04:09:31,799:INFO:Uploading model into container now
2024-02-08 04:09:31,800:INFO:_master_model_container: 41
2024-02-08 04:09:31,800:INFO:_display_container: 5
2024-02-08 04:09:31,801:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6300, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-02-08 04:09:31,801:INFO:create_model() successfully completed......................................
2024-02-08 04:09:31,962:INFO:SubProcess create_model() end ==================================
2024-02-08 04:09:31,963:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6300, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.9374
2024-02-08 04:09:31,963:INFO:<catboost.core.CatBoostClassifier object at 0x000002369A837B50> is best model
2024-02-08 04:09:31,963:INFO:choose_better completed
2024-02-08 04:09:31,963:INFO:Original model was better than the blended model, hence it will be returned. NOTE: The display metrics are for the blended model (not the original one).
2024-02-08 04:09:31,976:INFO:_master_model_container: 41
2024-02-08 04:09:31,976:INFO:_display_container: 4
2024-02-08 04:09:31,976:INFO:<catboost.core.CatBoostClassifier object at 0x000002369A837B50>
2024-02-08 04:09:31,976:INFO:blend_models() successfully completed......................................
2024-02-08 04:09:32,175:INFO:Initializing stack_models()
2024-02-08 04:09:32,175:INFO:stack_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator_list=[<catboost.core.CatBoostClassifier object at 0x0000023698F22110>, LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6300, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)], meta_model=None, meta_model_fold=5, fold=None, round=4, method=auto, restack=True, choose_better=True, optimize=Accuracy, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2024-02-08 04:09:32,175:INFO:Checking exceptions
2024-02-08 04:09:32,232:INFO:Defining meta model
2024-02-08 04:09:32,260:INFO:Getting model names
2024-02-08 04:09:32,260:INFO:[('CatBoost Classifier', <catboost.core.CatBoostClassifier object at 0x0000023698F22110>), ('Light Gradient Boosting Machine', LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6300, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0))]
2024-02-08 04:09:32,267:INFO:SubProcess create_model() called ==================================
2024-02-08 04:09:32,273:INFO:Initializing create_model()
2024-02-08 04:09:32,273:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=StackingClassifier(cv=5,
                   estimators=[('CatBoost Classifier',
                                <catboost.core.CatBoostClassifier object at 0x0000023698F22110>),
                               ('Light Gradient Boosting Machine',
                                LGBMClassifier(boosting_type='gbdt',
                                               class_weight=None,
                                               colsample_bytree=1.0,
                                               importance_type='split',
                                               learning_rate=0.1, max_depth=-1,
                                               min_child_samples=20,
                                               min_child_weight=0.001,
                                               min_split_gain=0.0,
                                               n_est...
                                               subsample_for_bin=200000,
                                               subsample_freq=0))],
                   final_estimator=LogisticRegression(C=1.0, class_weight=None,
                                                      dual=False,
                                                      fit_intercept=True,
                                                      intercept_scaling=1,
                                                      l1_ratio=None,
                                                      max_iter=1000,
                                                      multi_class='auto',
                                                      n_jobs=None, penalty='l2',
                                                      random_state=6300,
                                                      solver='lbfgs',
                                                      tol=0.0001, verbose=0,
                                                      warm_start=False),
                   n_jobs=-1, passthrough=True, stack_method='auto', verbose=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000236E27B2EC0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 04:09:32,273:INFO:Checking exceptions
2024-02-08 04:09:32,273:INFO:Importing libraries
2024-02-08 04:09:32,274:INFO:Copying training dataset
2024-02-08 04:09:32,408:INFO:Defining folds
2024-02-08 04:09:32,408:INFO:Declaring metric variables
2024-02-08 04:09:32,412:INFO:Importing untrained model
2024-02-08 04:09:32,412:INFO:Declaring custom model
2024-02-08 04:09:32,418:INFO:Stacking Classifier Imported successfully
2024-02-08 04:09:32,426:INFO:Starting cross validation
2024-02-08 04:09:32,439:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 04:19:22,140:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 04:19:22,285:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 04:19:22,290:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 04:19:25,305:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 04:19:37,844:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 04:19:40,462:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 04:19:47,330:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 04:19:53,722:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 04:22:31,158:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 04:22:33,869:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-08 04:22:34,190:INFO:Calculating mean and std
2024-02-08 04:22:34,191:INFO:Creating metrics dataframe
2024-02-08 04:22:34,197:INFO:Finalizing model
2024-02-08 04:22:37,618:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 04:22:37,683:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051123 seconds.
2024-02-08 04:22:37,683:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-08 04:22:37,685:INFO:[LightGBM] [Info] Total Bins 31359
2024-02-08 04:22:37,686:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 123
2024-02-08 04:22:37,687:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 04:24:10,126:INFO:Uploading results into container
2024-02-08 04:24:10,127:INFO:Uploading model into container now
2024-02-08 04:24:10,129:INFO:_master_model_container: 42
2024-02-08 04:24:10,129:INFO:_display_container: 4
2024-02-08 04:24:10,133:INFO:StackingClassifier(cv=5,
                   estimators=[('CatBoost Classifier',
                                <catboost.core.CatBoostClassifier object at 0x000002369A894580>),
                               ('Light Gradient Boosting Machine',
                                LGBMClassifier(boosting_type='gbdt',
                                               class_weight=None,
                                               colsample_bytree=1.0,
                                               importance_type='split',
                                               learning_rate=0.1, max_depth=-1,
                                               min_child_samples=20,
                                               min_child_weight=0.001,
                                               min_split_gain=0.0,
                                               n_est...
                                               subsample_for_bin=200000,
                                               subsample_freq=0))],
                   final_estimator=LogisticRegression(C=1.0, class_weight=None,
                                                      dual=False,
                                                      fit_intercept=True,
                                                      intercept_scaling=1,
                                                      l1_ratio=None,
                                                      max_iter=1000,
                                                      multi_class='auto',
                                                      n_jobs=None, penalty='l2',
                                                      random_state=6300,
                                                      solver='lbfgs',
                                                      tol=0.0001, verbose=0,
                                                      warm_start=False),
                   n_jobs=-1, passthrough=True, stack_method='auto', verbose=0)
2024-02-08 04:24:10,133:INFO:create_model() successfully completed......................................
2024-02-08 04:24:10,296:INFO:SubProcess create_model() end ==================================
2024-02-08 04:24:10,297:INFO:choose_better activated
2024-02-08 04:24:10,304:INFO:StackingClassifier(cv=5,
                   estimators=[('CatBoost Classifier',
                                <catboost.core.CatBoostClassifier object at 0x000002369A894580>),
                               ('Light Gradient Boosting Machine',
                                LGBMClassifier(boosting_type='gbdt',
                                               class_weight=None,
                                               colsample_bytree=1.0,
                                               importance_type='split',
                                               learning_rate=0.1, max_depth=-1,
                                               min_child_samples=20,
                                               min_child_weight=0.001,
                                               min_split_gain=0.0,
                                               n_est...
                                               subsample_for_bin=200000,
                                               subsample_freq=0))],
                   final_estimator=LogisticRegression(C=1.0, class_weight=None,
                                                      dual=False,
                                                      fit_intercept=True,
                                                      intercept_scaling=1,
                                                      l1_ratio=None,
                                                      max_iter=1000,
                                                      multi_class='auto',
                                                      n_jobs=None, penalty='l2',
                                                      random_state=6300,
                                                      solver='lbfgs',
                                                      tol=0.0001, verbose=0,
                                                      warm_start=False),
                   n_jobs=-1, passthrough=True, stack_method='auto', verbose=0) result for Accuracy is 0.9061
2024-02-08 04:24:10,304:INFO:SubProcess create_model() called ==================================
2024-02-08 04:24:10,304:INFO:Initializing create_model()
2024-02-08 04:24:10,305:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=<catboost.core.CatBoostClassifier object at 0x0000023698F22110>, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 04:24:10,305:INFO:Checking exceptions
2024-02-08 04:24:10,307:INFO:Importing libraries
2024-02-08 04:24:10,307:INFO:Copying training dataset
2024-02-08 04:24:10,412:INFO:Defining folds
2024-02-08 04:24:10,412:INFO:Declaring metric variables
2024-02-08 04:24:10,412:INFO:Importing untrained model
2024-02-08 04:24:10,412:INFO:Declaring custom model
2024-02-08 04:24:10,412:INFO:CatBoost Classifier Imported successfully
2024-02-08 04:24:10,413:INFO:Starting cross validation
2024-02-08 04:24:10,425:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 04:26:57,415:INFO:Calculating mean and std
2024-02-08 04:26:57,416:INFO:Creating metrics dataframe
2024-02-08 04:26:57,419:INFO:Finalizing model
2024-02-08 04:27:00,976:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 04:27:01,074:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079684 seconds.
2024-02-08 04:27:01,075:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-08 04:27:01,077:INFO:[LightGBM] [Info] Total Bins 31359
2024-02-08 04:27:01,078:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 123
2024-02-08 04:27:01,078:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 04:27:22,624:INFO:Uploading results into container
2024-02-08 04:27:22,625:INFO:Uploading model into container now
2024-02-08 04:27:22,625:INFO:_master_model_container: 43
2024-02-08 04:27:22,625:INFO:_display_container: 5
2024-02-08 04:27:22,625:INFO:<catboost.core.CatBoostClassifier object at 0x000002369A899720>
2024-02-08 04:27:22,625:INFO:create_model() successfully completed......................................
2024-02-08 04:27:22,762:INFO:SubProcess create_model() end ==================================
2024-02-08 04:27:22,762:INFO:<catboost.core.CatBoostClassifier object at 0x000002369A899720> result for Accuracy is 0.9405
2024-02-08 04:27:22,762:INFO:SubProcess create_model() called ==================================
2024-02-08 04:27:22,763:INFO:Initializing create_model()
2024-02-08 04:27:22,763:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6300, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 04:27:22,763:INFO:Checking exceptions
2024-02-08 04:27:22,765:INFO:Importing libraries
2024-02-08 04:27:22,765:INFO:Copying training dataset
2024-02-08 04:27:22,880:INFO:Defining folds
2024-02-08 04:27:22,880:INFO:Declaring metric variables
2024-02-08 04:27:22,880:INFO:Importing untrained model
2024-02-08 04:27:22,880:INFO:Declaring custom model
2024-02-08 04:27:22,881:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-08 04:27:22,881:INFO:Starting cross validation
2024-02-08 04:27:22,893:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-08 04:28:04,586:INFO:Calculating mean and std
2024-02-08 04:28:04,587:INFO:Creating metrics dataframe
2024-02-08 04:28:04,589:INFO:Finalizing model
2024-02-08 04:28:08,131:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 04:28:08,227:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076413 seconds.
2024-02-08 04:28:08,227:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-08 04:28:08,229:INFO:[LightGBM] [Info] Total Bins 31359
2024-02-08 04:28:08,230:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 123
2024-02-08 04:28:08,231:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 04:28:11,046:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 04:28:11,060:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003470 seconds.
2024-02-08 04:28:11,060:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-02-08 04:28:11,061:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-02-08 04:28:11,061:INFO:[LightGBM] [Info] Total Bins 6375
2024-02-08 04:28:11,061:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 25
2024-02-08 04:28:11,061:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 04:28:11,538:INFO:Uploading results into container
2024-02-08 04:28:11,539:INFO:Uploading model into container now
2024-02-08 04:28:11,539:INFO:_master_model_container: 44
2024-02-08 04:28:11,540:INFO:_display_container: 5
2024-02-08 04:28:11,540:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6300, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-02-08 04:28:11,540:INFO:create_model() successfully completed......................................
2024-02-08 04:28:11,701:INFO:SubProcess create_model() end ==================================
2024-02-08 04:28:11,701:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6300, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.9375
2024-02-08 04:28:11,701:INFO:<catboost.core.CatBoostClassifier object at 0x000002369A899720> is best model
2024-02-08 04:28:11,701:INFO:choose_better completed
2024-02-08 04:28:11,702:INFO:Original model was better than the stacked model, hence it will be returned. NOTE: The display metrics are for the stacked model (not the original one).
2024-02-08 04:28:11,714:INFO:_master_model_container: 44
2024-02-08 04:28:11,714:INFO:_display_container: 4
2024-02-08 04:28:11,714:INFO:<catboost.core.CatBoostClassifier object at 0x000002369A899720>
2024-02-08 04:28:11,714:INFO:stack_models() successfully completed......................................
2024-02-08 04:28:11,876:INFO:Initializing automl()
2024-02-08 04:28:11,876:INFO:automl(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, optimize=Accuracy, use_holdout=False, turbo=True, return_train_score=False)
2024-02-08 04:28:11,876:INFO:Model Selection Basis : CV Results on Training set
2024-02-08 04:28:11,876:INFO:Checking model 0
2024-02-08 04:28:11,876:INFO:Checking model 1
2024-02-08 04:28:11,876:INFO:Checking model 2
2024-02-08 04:28:11,877:INFO:Checking model 3
2024-02-08 04:28:11,877:INFO:Checking model 4
2024-02-08 04:28:11,877:INFO:Checking model 5
2024-02-08 04:28:11,877:INFO:Checking model 6
2024-02-08 04:28:11,877:INFO:Checking model 7
2024-02-08 04:28:11,878:INFO:Checking model 8
2024-02-08 04:28:11,878:INFO:Checking model 9
2024-02-08 04:28:11,878:INFO:Checking model 10
2024-02-08 04:28:11,878:INFO:Checking model 11
2024-02-08 04:28:11,878:INFO:Checking model 12
2024-02-08 04:28:11,879:INFO:Checking model 13
2024-02-08 04:28:11,879:INFO:Checking model 14
2024-02-08 04:28:11,879:INFO:Checking model 15
2024-02-08 04:28:11,879:INFO:Checking model 16
2024-02-08 04:28:11,879:INFO:Checking model 17
2024-02-08 04:28:11,879:INFO:Checking model 18
2024-02-08 04:28:11,880:INFO:Checking model 19
2024-02-08 04:28:11,880:INFO:Checking model 20
2024-02-08 04:28:11,880:INFO:Checking model 21
2024-02-08 04:28:11,880:INFO:Checking model 22
2024-02-08 04:28:11,880:INFO:Checking model 23
2024-02-08 04:28:11,881:INFO:Checking model 24
2024-02-08 04:28:11,881:INFO:Checking model 25
2024-02-08 04:28:11,881:INFO:Checking model 26
2024-02-08 04:28:11,881:INFO:Checking model 27
2024-02-08 04:28:11,881:INFO:Checking model 28
2024-02-08 04:28:11,882:INFO:Checking model 29
2024-02-08 04:28:11,882:INFO:Checking model 30
2024-02-08 04:28:11,882:INFO:Checking model 31
2024-02-08 04:28:11,882:INFO:Checking model 32
2024-02-08 04:28:11,882:INFO:Checking model 33
2024-02-08 04:28:11,882:INFO:Checking model 34
2024-02-08 04:28:11,883:INFO:Checking model 35
2024-02-08 04:28:11,883:INFO:Checking model 36
2024-02-08 04:28:11,883:INFO:Checking model 37
2024-02-08 04:28:11,884:INFO:Checking model 38
2024-02-08 04:28:11,884:INFO:Checking model 39
2024-02-08 04:28:11,884:INFO:Checking model 40
2024-02-08 04:28:11,884:INFO:Checking model 41
2024-02-08 04:28:11,885:INFO:Checking model 42
2024-02-08 04:28:11,885:INFO:Checking model 43
2024-02-08 04:28:11,885:INFO:Initializing create_model()
2024-02-08 04:28:11,885:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=<catboost.core.CatBoostClassifier object at 0x000002369A837B50>, fold=None, round=4, cross_validation=False, predict=False, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 04:28:11,885:INFO:Checking exceptions
2024-02-08 04:28:11,887:INFO:Importing libraries
2024-02-08 04:28:11,887:INFO:Copying training dataset
2024-02-08 04:28:11,989:INFO:Defining folds
2024-02-08 04:28:11,989:INFO:Declaring metric variables
2024-02-08 04:28:11,989:INFO:Importing untrained model
2024-02-08 04:28:11,989:INFO:Declaring custom model
2024-02-08 04:28:11,990:INFO:CatBoost Classifier Imported successfully
2024-02-08 04:28:12,000:INFO:Cross validation set to False
2024-02-08 04:28:12,000:INFO:Fitting Model
2024-02-08 04:28:15,078:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-08 04:28:15,179:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076674 seconds.
2024-02-08 04:28:15,179:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-08 04:28:15,181:INFO:[LightGBM] [Info] Total Bins 31359
2024-02-08 04:28:15,182:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 123
2024-02-08 04:28:15,183:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 04:28:37,702:INFO:<catboost.core.CatBoostClassifier object at 0x000002369A8944C0>
2024-02-08 04:28:37,702:INFO:create_model() successfully completed......................................
2024-02-08 04:28:37,985:INFO:<catboost.core.CatBoostClassifier object at 0x000002369A8944C0>
2024-02-08 04:28:37,985:INFO:automl() successfully completed......................................
2024-02-08 04:28:38,007:INFO:Initializing predict_model()
2024-02-08 04:28:38,007:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=<catboost.core.CatBoostClassifier object at 0x000002369A8944C0>, probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002369A8AA290>)
2024-02-08 04:28:38,007:INFO:Checking exceptions
2024-02-08 04:28:38,007:INFO:Preloading libraries
2024-02-08 04:28:38,548:INFO:Initializing finalize_model()
2024-02-08 04:28:38,548:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=<catboost.core.CatBoostClassifier object at 0x000002369A8944C0>, fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-02-08 04:28:38,548:INFO:Finalizing <catboost.core.CatBoostClassifier object at 0x000002369A8944C0>
2024-02-08 04:28:38,620:INFO:Initializing create_model()
2024-02-08 04:28:38,620:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=<catboost.core.CatBoostClassifier object at 0x000002369A8944C0>, fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 04:28:38,620:INFO:Checking exceptions
2024-02-08 04:28:38,622:INFO:Importing libraries
2024-02-08 04:28:38,622:INFO:Copying training dataset
2024-02-08 04:28:38,630:INFO:Defining folds
2024-02-08 04:28:38,630:INFO:Declaring metric variables
2024-02-08 04:28:38,630:INFO:Importing untrained model
2024-02-08 04:28:38,631:INFO:Declaring custom model
2024-02-08 04:28:38,631:INFO:CatBoost Classifier Imported successfully
2024-02-08 04:28:38,643:INFO:Cross validation set to False
2024-02-08 04:28:38,643:INFO:Fitting Model
2024-02-08 04:28:42,966:INFO:[LightGBM] [Info] Number of positive: 62867, number of negative: 62867
2024-02-08 04:28:43,081:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088156 seconds.
2024-02-08 04:28:43,081:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-08 04:28:43,083:INFO:[LightGBM] [Info] Total Bins 31358
2024-02-08 04:28:43,084:INFO:[LightGBM] [Info] Number of data points in the train set: 125734, number of used features: 123
2024-02-08 04:28:43,085:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 04:29:10,229:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id', 'last_date_of_month_7',
                                             'last_date_of_month_8', 'arpu_6',
                                             'arpu_7', 'arpu_8', 'onnet_mou_6',
                                             'onnet_mou_7', 'onnet_mou_8',
                                             'offnet_mou_6', 'offnet_mou_7',
                                             'offnet_mou_8', 'roam_ic_mou_6',
                                             'roam_ic_mou_7', 'roam_ic_mou_8',
                                             'roam_og_mou_6', 'roam_og_mou_7...
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=None,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=25,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf))),
                ('actual_estimator',
                 <catboost.core.CatBoostClassifier object at 0x000002369A8970D0>)],
         verbose=False)
2024-02-08 04:29:10,229:INFO:create_model() successfully completed......................................
2024-02-08 04:29:10,355:INFO:_master_model_container: 44
2024-02-08 04:29:10,355:INFO:_display_container: 4
2024-02-08 04:29:10,370:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id', 'last_date_of_month_7',
                                             'last_date_of_month_8', 'arpu_6',
                                             'arpu_7', 'arpu_8', 'onnet_mou_6',
                                             'onnet_mou_7', 'onnet_mou_8',
                                             'offnet_mou_6', 'offnet_mou_7',
                                             'offnet_mou_8', 'roam_ic_mou_6',
                                             'roam_ic_mou_7', 'roam_ic_mou_8',
                                             'roam_og_mou_6', 'roam_og_mou_7...
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=None,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=25,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf))),
                ('actual_estimator',
                 <catboost.core.CatBoostClassifier object at 0x000002369A8970D0>)],
         verbose=False)
2024-02-08 04:29:10,370:INFO:finalize_model() successfully completed......................................
2024-02-08 04:29:10,515:INFO:Initializing predict_model()
2024-02-08 04:29:10,516:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id', 'last_date_of_month_7',
                                             'last_date_of_month_8', 'arpu_6',
                                             'arpu_7', 'arpu_8', 'onnet_mou_6',
                                             'onnet_mou_7', 'onnet_mou_8',
                                             'offnet_mou_6', 'offnet_mou_7',
                                             'offnet_mou_8', 'roam_ic_mou_6',
                                             'roam_ic_mou_7', 'roam_ic_mou_8',
                                             'roam_og_mou_6', 'roam_og_mou_7...
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=None,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=25,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf))),
                ('actual_estimator',
                 <catboost.core.CatBoostClassifier object at 0x000002369A8970D0>)],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002369A8A92D0>)
2024-02-08 04:29:10,516:INFO:Checking exceptions
2024-02-08 04:29:10,516:INFO:Preloading libraries
2024-02-08 09:04:22,024:INFO:Initializing finalize_model()
2024-02-08 09:04:22,025:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=<catboost.core.CatBoostClassifier object at 0x000002369A8944C0>, fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-02-08 09:04:22,025:INFO:Finalizing <catboost.core.CatBoostClassifier object at 0x000002369A8944C0>
2024-02-08 09:04:22,093:INFO:Initializing create_model()
2024-02-08 09:04:22,093:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=<catboost.core.CatBoostClassifier object at 0x000002369A8944C0>, fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-02-08 09:04:22,093:INFO:Checking exceptions
2024-02-08 09:04:22,095:INFO:Importing libraries
2024-02-08 09:04:22,096:INFO:Copying training dataset
2024-02-08 09:04:22,104:INFO:Defining folds
2024-02-08 09:04:22,104:INFO:Declaring metric variables
2024-02-08 09:04:22,105:INFO:Importing untrained model
2024-02-08 09:04:22,105:INFO:Declaring custom model
2024-02-08 09:04:22,105:INFO:CatBoost Classifier Imported successfully
2024-02-08 09:04:22,115:INFO:Cross validation set to False
2024-02-08 09:04:22,116:INFO:Fitting Model
2024-02-08 09:04:26,680:INFO:[LightGBM] [Info] Number of positive: 62867, number of negative: 62867
2024-02-08 09:04:26,770:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060136 seconds.
2024-02-08 09:04:26,771:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-08 09:04:26,772:INFO:[LightGBM] [Info] Total Bins 31358
2024-02-08 09:04:26,773:INFO:[LightGBM] [Info] Number of data points in the train set: 125734, number of used features: 123
2024-02-08 09:04:26,774:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-08 09:04:47,162:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id', 'last_date_of_month_7',
                                             'last_date_of_month_8', 'arpu_6',
                                             'arpu_7', 'arpu_8', 'onnet_mou_6',
                                             'onnet_mou_7', 'onnet_mou_8',
                                             'offnet_mou_6', 'offnet_mou_7',
                                             'offnet_mou_8', 'roam_ic_mou_6',
                                             'roam_ic_mou_7', 'roam_ic_mou_8',
                                             'roam_og_mou_6', 'roam_og_mou_7...
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=None,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=25,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf))),
                ('actual_estimator',
                 <catboost.core.CatBoostClassifier object at 0x000002369A8988E0>)],
         verbose=False)
2024-02-08 09:04:47,162:INFO:create_model() successfully completed......................................
2024-02-08 09:04:47,298:INFO:_master_model_container: 44
2024-02-08 09:04:47,298:INFO:_display_container: 4
2024-02-08 09:04:47,313:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id', 'last_date_of_month_7',
                                             'last_date_of_month_8', 'arpu_6',
                                             'arpu_7', 'arpu_8', 'onnet_mou_6',
                                             'onnet_mou_7', 'onnet_mou_8',
                                             'offnet_mou_6', 'offnet_mou_7',
                                             'offnet_mou_8', 'roam_ic_mou_6',
                                             'roam_ic_mou_7', 'roam_ic_mou_8',
                                             'roam_og_mou_6', 'roam_og_mou_7...
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=None,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=25,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf))),
                ('actual_estimator',
                 <catboost.core.CatBoostClassifier object at 0x000002369A8988E0>)],
         verbose=False)
2024-02-08 09:04:47,313:INFO:finalize_model() successfully completed......................................
2024-02-08 09:04:52,888:INFO:Initializing predict_model()
2024-02-08 09:04:52,888:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id', 'last_date_of_month_7',
                                             'last_date_of_month_8', 'arpu_6',
                                             'arpu_7', 'arpu_8', 'onnet_mou_6',
                                             'onnet_mou_7', 'onnet_mou_8',
                                             'offnet_mou_6', 'offnet_mou_7',
                                             'offnet_mou_8', 'roam_ic_mou_6',
                                             'roam_ic_mou_7', 'roam_ic_mou_8',
                                             'roam_og_mou_6', 'roam_og_mou_7...
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=None,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=25,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf))),
                ('actual_estimator',
                 <catboost.core.CatBoostClassifier object at 0x000002369A8988E0>)],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000236829BB0A0>)
2024-02-08 09:04:52,889:INFO:Checking exceptions
2024-02-08 09:04:52,889:INFO:Preloading libraries
2024-02-08 09:05:01,709:INFO:Initializing predict_model()
2024-02-08 09:05:01,709:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id', 'last_date_of_month_7',
                                             'last_date_of_month_8', 'arpu_6',
                                             'arpu_7', 'arpu_8', 'onnet_mou_6',
                                             'onnet_mou_7', 'onnet_mou_8',
                                             'offnet_mou_6', 'offnet_mou_7',
                                             'offnet_mou_8', 'roam_ic_mou_6',
                                             'roam_ic_mou_7', 'roam_ic_mou_8',
                                             'roam_og_mou_6', 'roam_og_mou_7...
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=None,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=25,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf))),
                ('actual_estimator',
                 <catboost.core.CatBoostClassifier object at 0x000002369A8988E0>)],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002369AA97A30>)
2024-02-08 09:05:01,709:INFO:Checking exceptions
2024-02-08 09:05:01,709:INFO:Preloading libraries
2024-02-08 09:05:01,712:INFO:Set up data.
2024-02-08 09:05:01,801:INFO:Set up index.
2024-02-08 09:07:27,050:INFO:Initializing predict_model()
2024-02-08 09:07:27,050:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=<catboost.core.CatBoostClassifier object at 0x000002369A8944C0>, probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000236FBA6BBE0>)
2024-02-08 09:07:27,050:INFO:Checking exceptions
2024-02-08 09:07:27,051:INFO:Preloading libraries
2024-02-08 09:07:27,054:INFO:Set up data.
2024-02-08 09:07:27,149:INFO:Set up index.
2024-02-08 09:13:52,493:INFO:Initializing predict_model()
2024-02-08 09:13:52,494:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id', 'last_date_of_month_7',
                                             'last_date_of_month_8', 'arpu_6',
                                             'arpu_7', 'arpu_8', 'onnet_mou_6',
                                             'onnet_mou_7', 'onnet_mou_8',
                                             'offnet_mou_6', 'offnet_mou_7',
                                             'offnet_mou_8', 'roam_ic_mou_6',
                                             'roam_ic_mou_7', 'roam_ic_mou_8',
                                             'roam_og_mou_6', 'roam_og_mou_7...
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=None,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=25,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf))),
                ('actual_estimator',
                 <catboost.core.CatBoostClassifier object at 0x000002369A8988E0>)],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000236FBA6A0E0>)
2024-02-08 09:13:52,494:INFO:Checking exceptions
2024-02-08 09:13:52,494:INFO:Preloading libraries
2024-02-08 09:13:52,496:INFO:Set up data.
2024-02-08 09:13:52,609:INFO:Set up index.
2024-02-08 09:14:23,319:INFO:Initializing predict_model()
2024-02-08 09:14:23,319:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id', 'last_date_of_month_7',
                                             'last_date_of_month_8', 'arpu_6',
                                             'arpu_7', 'arpu_8', 'onnet_mou_6',
                                             'onnet_mou_7', 'onnet_mou_8',
                                             'offnet_mou_6', 'offnet_mou_7',
                                             'offnet_mou_8', 'roam_ic_mou_6',
                                             'roam_ic_mou_7', 'roam_ic_mou_8',
                                             'roam_og_mou_6', 'roam_og_mou_7...
                                                                                         min_split_gain=0.0,
                                                                                         n_estimators=100,
                                                                                         n_jobs=None,
                                                                                         num_leaves=31,
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=25,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf))),
                ('actual_estimator',
                 <catboost.core.CatBoostClassifier object at 0x000002369A8988E0>)],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000023704BBFE20>)
2024-02-08 09:14:23,319:INFO:Checking exceptions
2024-02-08 09:14:23,319:INFO:Preloading libraries
2024-02-08 09:14:23,322:INFO:Set up data.
2024-02-08 09:14:23,408:INFO:Set up index.
2024-02-08 09:14:56,108:INFO:Initializing predict_model()
2024-02-08 09:14:56,108:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000236E1BC6FE0>, estimator=<catboost.core.CatBoostClassifier object at 0x000002369A8944C0>, probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000236FBA6B880>)
2024-02-08 09:14:56,109:INFO:Checking exceptions
2024-02-08 09:14:56,109:INFO:Preloading libraries
2024-02-08 09:14:56,112:INFO:Set up data.
2024-02-08 09:14:56,201:INFO:Set up index.
2024-02-09 01:22:38,642:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-09 01:22:38,642:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-09 01:22:38,642:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-09 01:22:38,642:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-09 01:22:49,791:INFO:PyCaret ClassificationExperiment
2024-02-09 01:22:49,792:INFO:Logging name: clf-default-name
2024-02-09 01:22:49,792:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-02-09 01:22:49,792:INFO:version 3.2.0
2024-02-09 01:22:49,792:INFO:Initializing setup()
2024-02-09 01:22:49,792:INFO:self.USI: d46a
2024-02-09 01:22:49,792:INFO:self._variable_keys: {'fold_shuffle_param', 'y_test', '_available_plots', 'idx', 'X', 'fix_imbalance', 'USI', 'pipeline', 'n_jobs_param', 'exp_name_log', 'memory', 'fold_generator', 'log_plots_param', 'y_train', 'seed', '_ml_usecase', 'gpu_param', 'logging_param', 'data', 'X_train', 'target_param', 'html_param', 'exp_id', 'fold_groups_param', 'is_multiclass', 'gpu_n_jobs_param', 'X_test', 'y'}
2024-02-09 01:22:49,792:INFO:Checking environment
2024-02-09 01:22:49,792:INFO:python_version: 3.10.9
2024-02-09 01:22:49,792:INFO:python_build: ('main', 'Jan 11 2023 15:15:40')
2024-02-09 01:22:49,792:INFO:machine: AMD64
2024-02-09 01:22:49,792:INFO:platform: Windows-10-10.0.19045-SP0
2024-02-09 01:22:49,792:INFO:Memory: svmem(total=16856182784, available=10071699456, percent=40.2, used=6784483328, free=10071699456)
2024-02-09 01:22:49,792:INFO:Physical Core: 4
2024-02-09 01:22:49,792:INFO:Logical Core: 8
2024-02-09 01:22:49,792:INFO:Checking libraries
2024-02-09 01:22:49,792:INFO:System:
2024-02-09 01:22:49,792:INFO:    python: 3.10.9 | packaged by conda-forge | (main, Jan 11 2023, 15:15:40) [MSC v.1916 64 bit (AMD64)]
2024-02-09 01:22:49,793:INFO:executable: C:\ProgramData\miniconda3\python.exe
2024-02-09 01:22:49,793:INFO:   machine: Windows-10-10.0.19045-SP0
2024-02-09 01:22:49,793:INFO:PyCaret required dependencies:
2024-02-09 01:22:49,918:INFO:                 pip: 22.3.1
2024-02-09 01:22:49,918:INFO:          setuptools: 65.6.3
2024-02-09 01:22:49,918:INFO:             pycaret: 3.2.0
2024-02-09 01:22:49,918:INFO:             IPython: 8.20.0
2024-02-09 01:22:49,918:INFO:          ipywidgets: 8.0.4
2024-02-09 01:22:49,918:INFO:                tqdm: 4.64.1
2024-02-09 01:22:49,918:INFO:               numpy: 1.25.2
2024-02-09 01:22:49,918:INFO:              pandas: 1.5.3
2024-02-09 01:22:49,918:INFO:              jinja2: 3.1.3
2024-02-09 01:22:49,918:INFO:               scipy: 1.10.1
2024-02-09 01:22:49,918:INFO:              joblib: 1.3.2
2024-02-09 01:22:49,918:INFO:             sklearn: 1.2.2
2024-02-09 01:22:49,918:INFO:                pyod: 1.1.2
2024-02-09 01:22:49,918:INFO:            imblearn: 0.12.0
2024-02-09 01:22:49,918:INFO:   category_encoders: 2.6.3
2024-02-09 01:22:49,918:INFO:            lightgbm: 4.3.0
2024-02-09 01:22:49,918:INFO:               numba: 0.59.0
2024-02-09 01:22:49,918:INFO:            requests: 2.31.0
2024-02-09 01:22:49,918:INFO:          matplotlib: 3.6.0
2024-02-09 01:22:49,919:INFO:          scikitplot: 0.3.7
2024-02-09 01:22:49,919:INFO:         yellowbrick: 1.5
2024-02-09 01:22:49,919:INFO:              plotly: 5.18.0
2024-02-09 01:22:49,919:INFO:    plotly-resampler: Not installed
2024-02-09 01:22:49,919:INFO:             kaleido: 0.2.1
2024-02-09 01:22:49,919:INFO:           schemdraw: 0.15
2024-02-09 01:22:49,919:INFO:         statsmodels: 0.14.1
2024-02-09 01:22:49,919:INFO:              sktime: 0.21.1
2024-02-09 01:22:49,919:INFO:               tbats: 1.1.3
2024-02-09 01:22:49,919:INFO:            pmdarima: 2.0.4
2024-02-09 01:22:49,919:INFO:              psutil: 5.9.0
2024-02-09 01:22:49,919:INFO:          markupsafe: 2.1.3
2024-02-09 01:22:49,919:INFO:             pickle5: Not installed
2024-02-09 01:22:49,919:INFO:         cloudpickle: 3.0.0
2024-02-09 01:22:49,919:INFO:         deprecation: 2.1.0
2024-02-09 01:22:49,919:INFO:              xxhash: 3.4.1
2024-02-09 01:22:49,919:INFO:           wurlitzer: Not installed
2024-02-09 01:22:49,919:INFO:PyCaret optional dependencies:
2024-02-09 01:22:49,930:INFO:                shap: 0.44.1
2024-02-09 01:22:49,930:INFO:           interpret: Not installed
2024-02-09 01:22:49,930:INFO:                umap: Not installed
2024-02-09 01:22:49,930:INFO:     ydata_profiling: Not installed
2024-02-09 01:22:49,930:INFO:  explainerdashboard: 0.4.5
2024-02-09 01:22:49,930:INFO:             autoviz: Not installed
2024-02-09 01:22:49,930:INFO:           fairlearn: Not installed
2024-02-09 01:22:49,930:INFO:          deepchecks: Not installed
2024-02-09 01:22:49,930:INFO:             xgboost: Not installed
2024-02-09 01:22:49,930:INFO:            catboost: 1.2.2
2024-02-09 01:22:49,931:INFO:              kmodes: Not installed
2024-02-09 01:22:49,931:INFO:             mlxtend: Not installed
2024-02-09 01:22:49,931:INFO:       statsforecast: Not installed
2024-02-09 01:22:49,931:INFO:        tune_sklearn: Not installed
2024-02-09 01:22:49,931:INFO:                 ray: Not installed
2024-02-09 01:22:49,931:INFO:            hyperopt: Not installed
2024-02-09 01:22:49,931:INFO:              optuna: Not installed
2024-02-09 01:22:49,931:INFO:               skopt: Not installed
2024-02-09 01:22:49,931:INFO:              mlflow: 2.10.0
2024-02-09 01:22:49,931:INFO:              gradio: Not installed
2024-02-09 01:22:49,931:INFO:             fastapi: Not installed
2024-02-09 01:22:49,931:INFO:             uvicorn: Not installed
2024-02-09 01:22:49,931:INFO:              m2cgen: Not installed
2024-02-09 01:22:49,931:INFO:           evidently: Not installed
2024-02-09 01:22:49,931:INFO:               fugue: Not installed
2024-02-09 01:22:49,931:INFO:           streamlit: Not installed
2024-02-09 01:22:49,931:INFO:             prophet: Not installed
2024-02-09 01:22:49,931:INFO:None
2024-02-09 01:22:49,931:INFO:Set up data.
2024-02-09 01:22:50,064:INFO:Set up folding strategy.
2024-02-09 01:22:50,064:INFO:Set up train/test split.
2024-02-09 01:22:50,172:INFO:Set up index.
2024-02-09 01:22:50,174:INFO:Assigning column types.
2024-02-09 01:22:50,259:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-02-09 01:22:50,299:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-09 01:22:50,303:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-02-09 01:22:50,339:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-09 01:22:50,339:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-09 01:22:50,610:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-09 01:22:50,611:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-02-09 01:22:50,637:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-09 01:22:50,638:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-09 01:22:50,638:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-02-09 01:22:50,695:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-02-09 01:22:50,729:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-09 01:22:50,729:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-09 01:22:50,778:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-02-09 01:22:50,806:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-09 01:22:50,807:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-09 01:22:50,807:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-02-09 01:22:50,876:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-09 01:22:50,876:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-09 01:22:50,943:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-09 01:22:50,943:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-09 01:22:50,949:INFO:Preparing preprocessing pipeline...
2024-02-09 01:22:50,959:INFO:Set up simple imputation.
2024-02-09 01:22:50,959:INFO:Set up imbalanced handling.
2024-02-09 01:22:51,270:INFO:Finished creating preprocessing pipeline.
2024-02-09 01:22:51,281:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\user\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id', 'last_date_of_month_7',
                                             'last_date_of_month_8', 'arpu_6',
                                             'arpu_7', 'arpu_8', 'onnet_mou_6',
                                             'onnet_mou_7', 'onnet_mou_8',
                                             'offnet_mou_6', 'offnet_mou_7',
                                             'offnet_mou_8', 'roam_ic_mou_6',
                                             'roam_ic_mou_7', 'roam_i...
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              n_jobs=None,
                                                                              random_state=None,
                                                                              sampling_strategy='auto'))))],
         verbose=False)
2024-02-09 01:22:51,281:INFO:Creating final display dataframe.
2024-02-09 01:22:53,337:INFO:Setup _display_container:                     Description              Value
0                    Session id               6467
1                        Target  churn_probability
2                   Target type             Binary
3           Original data shape       (69999, 129)
4        Transformed data shape      (109014, 129)
5   Transformed train set shape       (88014, 129)
6    Transformed test set shape       (21000, 129)
7              Numeric features                128
8                    Preprocess               True
9               Imputation type             simple
10           Numeric imputation               mean
11       Categorical imputation               mode
12                Fix imbalance               True
13         Fix imbalance method              SMOTE
14               Fold Generator    StratifiedKFold
15                  Fold Number                 20
16                     CPU Jobs                 -1
17                      Use GPU              False
18               Log Experiment              False
19              Experiment Name   clf-default-name
20                          USI               d46a
2024-02-09 01:22:53,416:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-09 01:22:53,417:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-09 01:22:53,486:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-09 01:22:53,487:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-09 01:22:53,488:INFO:setup() successfully completed in 3.73s...............
2024-02-09 01:23:37,202:INFO:Initializing compare_models()
2024-02-09 01:23:37,202:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, include=None, fold=20, round=4, cross_validation=True, sort=Accuracy, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, 'include': None, 'exclude': None, 'fold': 20, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-02-09 01:23:37,202:INFO:Checking exceptions
2024-02-09 01:23:37,271:INFO:Preparing display monitor
2024-02-09 01:23:37,297:INFO:Initializing Logistic Regression
2024-02-09 01:23:37,297:INFO:Total runtime is 0.0 minutes
2024-02-09 01:23:37,303:INFO:SubProcess create_model() called ==================================
2024-02-09 01:23:37,303:INFO:Initializing create_model()
2024-02-09 01:23:37,303:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=lr, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82DA80370>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 01:23:37,303:INFO:Checking exceptions
2024-02-09 01:23:37,303:INFO:Importing libraries
2024-02-09 01:23:37,303:INFO:Copying training dataset
2024-02-09 01:23:37,406:INFO:Defining folds
2024-02-09 01:23:37,407:INFO:Declaring metric variables
2024-02-09 01:23:37,409:INFO:Importing untrained model
2024-02-09 01:23:37,413:INFO:Logistic Regression Imported successfully
2024-02-09 01:23:37,419:INFO:Starting cross validation
2024-02-09 01:23:37,422:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 01:24:36,832:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 01:24:36,834:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 01:24:37,047:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 01:24:37,380:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 01:24:37,403:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 01:24:37,438:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 01:24:37,865:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 01:24:38,376:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 01:25:30,485:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 01:25:31,242:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 01:25:31,276:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 01:25:31,371:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 01:25:32,087:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 01:25:32,236:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 01:25:32,850:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 01:25:32,977:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 01:26:03,228:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 01:26:03,727:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 01:26:03,901:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 01:26:04,148:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 01:26:04,505:INFO:Calculating mean and std
2024-02-09 01:26:04,506:INFO:Creating metrics dataframe
2024-02-09 01:26:04,511:INFO:Uploading results into container
2024-02-09 01:26:04,512:INFO:Uploading model into container now
2024-02-09 01:26:04,513:INFO:_master_model_container: 1
2024-02-09 01:26:04,513:INFO:_display_container: 2
2024-02-09 01:26:04,513:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=6467, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-02-09 01:26:04,514:INFO:create_model() successfully completed......................................
2024-02-09 01:26:04,594:INFO:SubProcess create_model() end ==================================
2024-02-09 01:26:04,594:INFO:Creating metrics dataframe
2024-02-09 01:26:04,605:INFO:Initializing K Neighbors Classifier
2024-02-09 01:26:04,605:INFO:Total runtime is 2.4551305055618284 minutes
2024-02-09 01:26:04,609:INFO:SubProcess create_model() called ==================================
2024-02-09 01:26:04,610:INFO:Initializing create_model()
2024-02-09 01:26:04,610:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=knn, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82DA80370>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 01:26:04,610:INFO:Checking exceptions
2024-02-09 01:26:04,610:INFO:Importing libraries
2024-02-09 01:26:04,610:INFO:Copying training dataset
2024-02-09 01:26:04,703:INFO:Defining folds
2024-02-09 01:26:04,703:INFO:Declaring metric variables
2024-02-09 01:26:04,708:INFO:Importing untrained model
2024-02-09 01:26:04,713:INFO:K Neighbors Classifier Imported successfully
2024-02-09 01:26:04,720:INFO:Starting cross validation
2024-02-09 01:26:04,724:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 01:26:40,877:INFO:Calculating mean and std
2024-02-09 01:26:40,880:INFO:Creating metrics dataframe
2024-02-09 01:26:40,887:INFO:Uploading results into container
2024-02-09 01:26:40,888:INFO:Uploading model into container now
2024-02-09 01:26:40,888:INFO:_master_model_container: 2
2024-02-09 01:26:40,888:INFO:_display_container: 2
2024-02-09 01:26:40,889:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-02-09 01:26:40,889:INFO:create_model() successfully completed......................................
2024-02-09 01:26:40,973:INFO:SubProcess create_model() end ==================================
2024-02-09 01:26:40,973:INFO:Creating metrics dataframe
2024-02-09 01:26:40,991:INFO:Initializing Naive Bayes
2024-02-09 01:26:40,991:INFO:Total runtime is 3.0615537683169043 minutes
2024-02-09 01:26:40,995:INFO:SubProcess create_model() called ==================================
2024-02-09 01:26:40,995:INFO:Initializing create_model()
2024-02-09 01:26:40,995:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=nb, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82DA80370>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 01:26:40,996:INFO:Checking exceptions
2024-02-09 01:26:40,996:INFO:Importing libraries
2024-02-09 01:26:40,996:INFO:Copying training dataset
2024-02-09 01:26:41,079:INFO:Defining folds
2024-02-09 01:26:41,079:INFO:Declaring metric variables
2024-02-09 01:26:41,083:INFO:Importing untrained model
2024-02-09 01:26:41,089:INFO:Naive Bayes Imported successfully
2024-02-09 01:26:41,096:INFO:Starting cross validation
2024-02-09 01:26:41,100:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 01:26:49,145:INFO:Calculating mean and std
2024-02-09 01:26:49,147:INFO:Creating metrics dataframe
2024-02-09 01:26:49,150:INFO:Uploading results into container
2024-02-09 01:26:49,151:INFO:Uploading model into container now
2024-02-09 01:26:49,151:INFO:_master_model_container: 3
2024-02-09 01:26:49,151:INFO:_display_container: 2
2024-02-09 01:26:49,152:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-02-09 01:26:49,152:INFO:create_model() successfully completed......................................
2024-02-09 01:26:49,230:INFO:SubProcess create_model() end ==================================
2024-02-09 01:26:49,230:INFO:Creating metrics dataframe
2024-02-09 01:26:49,241:INFO:Initializing Decision Tree Classifier
2024-02-09 01:26:49,241:INFO:Total runtime is 3.199052802721659 minutes
2024-02-09 01:26:49,247:INFO:SubProcess create_model() called ==================================
2024-02-09 01:26:49,247:INFO:Initializing create_model()
2024-02-09 01:26:49,248:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=dt, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82DA80370>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 01:26:49,248:INFO:Checking exceptions
2024-02-09 01:26:49,248:INFO:Importing libraries
2024-02-09 01:26:49,248:INFO:Copying training dataset
2024-02-09 01:26:49,347:INFO:Defining folds
2024-02-09 01:26:49,347:INFO:Declaring metric variables
2024-02-09 01:26:49,351:INFO:Importing untrained model
2024-02-09 01:26:49,357:INFO:Decision Tree Classifier Imported successfully
2024-02-09 01:26:49,366:INFO:Starting cross validation
2024-02-09 01:26:49,369:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 01:28:02,256:INFO:Calculating mean and std
2024-02-09 01:28:02,257:INFO:Creating metrics dataframe
2024-02-09 01:28:02,261:INFO:Uploading results into container
2024-02-09 01:28:02,262:INFO:Uploading model into container now
2024-02-09 01:28:02,262:INFO:_master_model_container: 4
2024-02-09 01:28:02,262:INFO:_display_container: 2
2024-02-09 01:28:02,263:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=6467, splitter='best')
2024-02-09 01:28:02,263:INFO:create_model() successfully completed......................................
2024-02-09 01:28:02,357:INFO:SubProcess create_model() end ==================================
2024-02-09 01:28:02,357:INFO:Creating metrics dataframe
2024-02-09 01:28:02,366:INFO:Initializing SVM - Linear Kernel
2024-02-09 01:28:02,366:INFO:Total runtime is 4.417811465263366 minutes
2024-02-09 01:28:02,369:INFO:SubProcess create_model() called ==================================
2024-02-09 01:28:02,370:INFO:Initializing create_model()
2024-02-09 01:28:02,370:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=svm, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82DA80370>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 01:28:02,370:INFO:Checking exceptions
2024-02-09 01:28:02,370:INFO:Importing libraries
2024-02-09 01:28:02,370:INFO:Copying training dataset
2024-02-09 01:28:02,453:INFO:Defining folds
2024-02-09 01:28:02,453:INFO:Declaring metric variables
2024-02-09 01:28:02,457:INFO:Importing untrained model
2024-02-09 01:28:02,463:INFO:SVM - Linear Kernel Imported successfully
2024-02-09 01:28:02,472:INFO:Starting cross validation
2024-02-09 01:28:02,475:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 01:28:20,855:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 01:28:23,430:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 01:28:25,663:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 01:28:26,215:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 01:28:27,266:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 01:28:27,710:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 01:28:27,751:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 01:28:29,990:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 01:28:43,585:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 01:28:45,182:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 01:28:47,894:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 01:28:49,124:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 01:28:51,873:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 01:28:52,320:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 01:28:53,300:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 01:29:02,858:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 01:29:07,594:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 01:29:09,144:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 01:29:09,358:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 01:29:10,315:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 01:29:10,444:INFO:Calculating mean and std
2024-02-09 01:29:10,448:INFO:Creating metrics dataframe
2024-02-09 01:29:10,455:INFO:Uploading results into container
2024-02-09 01:29:10,455:INFO:Uploading model into container now
2024-02-09 01:29:10,456:INFO:_master_model_container: 5
2024-02-09 01:29:10,456:INFO:_display_container: 2
2024-02-09 01:29:10,456:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=6467, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-02-09 01:29:10,456:INFO:create_model() successfully completed......................................
2024-02-09 01:29:10,530:INFO:SubProcess create_model() end ==================================
2024-02-09 01:29:10,530:INFO:Creating metrics dataframe
2024-02-09 01:29:10,540:INFO:Initializing Ridge Classifier
2024-02-09 01:29:10,540:INFO:Total runtime is 5.554043622811634 minutes
2024-02-09 01:29:10,545:INFO:SubProcess create_model() called ==================================
2024-02-09 01:29:10,546:INFO:Initializing create_model()
2024-02-09 01:29:10,546:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=ridge, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82DA80370>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 01:29:10,546:INFO:Checking exceptions
2024-02-09 01:29:10,546:INFO:Importing libraries
2024-02-09 01:29:10,546:INFO:Copying training dataset
2024-02-09 01:29:10,634:INFO:Defining folds
2024-02-09 01:29:10,635:INFO:Declaring metric variables
2024-02-09 01:29:10,639:INFO:Importing untrained model
2024-02-09 01:29:10,649:INFO:Ridge Classifier Imported successfully
2024-02-09 01:29:10,661:INFO:Starting cross validation
2024-02-09 01:29:10,664:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 01:29:13,688:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 01:29:13,690:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 01:29:13,704:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 01:29:13,714:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 01:29:13,729:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 01:29:13,730:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 01:29:13,743:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 01:29:13,758:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 01:29:16,886:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 01:29:16,931:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 01:29:16,945:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 01:29:16,965:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 01:29:16,976:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 01:29:17,000:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 01:29:17,003:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 01:29:17,036:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 01:29:18,851:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 01:29:18,924:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 01:29:18,925:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 01:29:18,941:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 01:29:19,075:INFO:Calculating mean and std
2024-02-09 01:29:19,076:INFO:Creating metrics dataframe
2024-02-09 01:29:19,081:INFO:Uploading results into container
2024-02-09 01:29:19,082:INFO:Uploading model into container now
2024-02-09 01:29:19,082:INFO:_master_model_container: 6
2024-02-09 01:29:19,082:INFO:_display_container: 2
2024-02-09 01:29:19,083:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=6467, solver='auto',
                tol=0.0001)
2024-02-09 01:29:19,083:INFO:create_model() successfully completed......................................
2024-02-09 01:29:19,165:INFO:SubProcess create_model() end ==================================
2024-02-09 01:29:19,165:INFO:Creating metrics dataframe
2024-02-09 01:29:19,177:INFO:Initializing Random Forest Classifier
2024-02-09 01:29:19,178:INFO:Total runtime is 5.698017597198485 minutes
2024-02-09 01:29:19,187:INFO:SubProcess create_model() called ==================================
2024-02-09 01:29:19,187:INFO:Initializing create_model()
2024-02-09 01:29:19,188:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=rf, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82DA80370>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 01:29:19,188:INFO:Checking exceptions
2024-02-09 01:29:19,189:INFO:Importing libraries
2024-02-09 01:29:19,189:INFO:Copying training dataset
2024-02-09 01:29:19,293:INFO:Defining folds
2024-02-09 01:29:19,293:INFO:Declaring metric variables
2024-02-09 01:29:19,298:INFO:Importing untrained model
2024-02-09 01:29:19,307:INFO:Random Forest Classifier Imported successfully
2024-02-09 01:29:19,316:INFO:Starting cross validation
2024-02-09 01:29:19,320:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 01:36:13,964:INFO:Calculating mean and std
2024-02-09 01:36:13,967:INFO:Creating metrics dataframe
2024-02-09 01:36:13,974:INFO:Uploading results into container
2024-02-09 01:36:13,975:INFO:Uploading model into container now
2024-02-09 01:36:13,975:INFO:_master_model_container: 7
2024-02-09 01:36:13,976:INFO:_display_container: 2
2024-02-09 01:36:13,977:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6467, verbose=0, warm_start=False)
2024-02-09 01:36:13,977:INFO:create_model() successfully completed......................................
2024-02-09 01:36:14,109:INFO:SubProcess create_model() end ==================================
2024-02-09 01:36:14,109:INFO:Creating metrics dataframe
2024-02-09 01:36:14,125:INFO:Initializing Quadratic Discriminant Analysis
2024-02-09 01:36:14,125:INFO:Total runtime is 12.613790063063302 minutes
2024-02-09 01:36:14,132:INFO:SubProcess create_model() called ==================================
2024-02-09 01:36:14,132:INFO:Initializing create_model()
2024-02-09 01:36:14,132:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=qda, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82DA80370>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 01:36:14,132:INFO:Checking exceptions
2024-02-09 01:36:14,132:INFO:Importing libraries
2024-02-09 01:36:14,132:INFO:Copying training dataset
2024-02-09 01:36:14,252:INFO:Defining folds
2024-02-09 01:36:14,252:INFO:Declaring metric variables
2024-02-09 01:36:14,258:INFO:Importing untrained model
2024-02-09 01:36:14,267:INFO:Quadratic Discriminant Analysis Imported successfully
2024-02-09 01:36:14,277:INFO:Starting cross validation
2024-02-09 01:36:14,283:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 01:36:20,673:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 01:36:20,801:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 01:36:20,991:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 01:36:21,509:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 01:36:22,154:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 01:36:22,253:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 01:36:22,326:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 01:36:22,413:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 01:36:30,312:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 01:36:30,760:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 01:36:31,464:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 01:36:34,621:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 01:36:36,071:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 01:36:36,743:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 01:36:36,774:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 01:36:36,779:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 01:36:40,827:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 01:36:42,028:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 01:36:43,474:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 01:36:44,350:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 01:36:45,718:INFO:Calculating mean and std
2024-02-09 01:36:45,719:INFO:Creating metrics dataframe
2024-02-09 01:36:45,723:INFO:Uploading results into container
2024-02-09 01:36:45,724:INFO:Uploading model into container now
2024-02-09 01:36:45,724:INFO:_master_model_container: 8
2024-02-09 01:36:45,724:INFO:_display_container: 2
2024-02-09 01:36:45,725:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-02-09 01:36:45,725:INFO:create_model() successfully completed......................................
2024-02-09 01:36:45,811:INFO:SubProcess create_model() end ==================================
2024-02-09 01:36:45,811:INFO:Creating metrics dataframe
2024-02-09 01:36:45,824:INFO:Initializing Ada Boost Classifier
2024-02-09 01:36:45,824:INFO:Total runtime is 13.14210390249888 minutes
2024-02-09 01:36:45,827:INFO:SubProcess create_model() called ==================================
2024-02-09 01:36:45,828:INFO:Initializing create_model()
2024-02-09 01:36:45,828:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=ada, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82DA80370>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 01:36:45,828:INFO:Checking exceptions
2024-02-09 01:36:45,828:INFO:Importing libraries
2024-02-09 01:36:45,828:INFO:Copying training dataset
2024-02-09 01:36:45,920:INFO:Defining folds
2024-02-09 01:36:45,920:INFO:Declaring metric variables
2024-02-09 01:36:45,925:INFO:Importing untrained model
2024-02-09 01:36:45,932:INFO:Ada Boost Classifier Imported successfully
2024-02-09 01:36:45,940:INFO:Starting cross validation
2024-02-09 01:36:45,944:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 01:41:37,460:INFO:Calculating mean and std
2024-02-09 01:41:37,461:INFO:Creating metrics dataframe
2024-02-09 01:41:37,464:INFO:Uploading results into container
2024-02-09 01:41:37,465:INFO:Uploading model into container now
2024-02-09 01:41:37,466:INFO:_master_model_container: 9
2024-02-09 01:41:37,466:INFO:_display_container: 2
2024-02-09 01:41:37,466:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=6467)
2024-02-09 01:41:37,466:INFO:create_model() successfully completed......................................
2024-02-09 01:41:37,544:INFO:SubProcess create_model() end ==================================
2024-02-09 01:41:37,544:INFO:Creating metrics dataframe
2024-02-09 01:41:37,556:INFO:Initializing Gradient Boosting Classifier
2024-02-09 01:41:37,556:INFO:Total runtime is 18.00431038935979 minutes
2024-02-09 01:41:37,559:INFO:SubProcess create_model() called ==================================
2024-02-09 01:41:37,559:INFO:Initializing create_model()
2024-02-09 01:41:37,560:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=gbc, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82DA80370>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 01:41:37,560:INFO:Checking exceptions
2024-02-09 01:41:37,560:INFO:Importing libraries
2024-02-09 01:41:37,560:INFO:Copying training dataset
2024-02-09 01:41:37,652:INFO:Defining folds
2024-02-09 01:41:37,652:INFO:Declaring metric variables
2024-02-09 01:41:37,655:INFO:Importing untrained model
2024-02-09 01:41:37,661:INFO:Gradient Boosting Classifier Imported successfully
2024-02-09 01:41:37,670:INFO:Starting cross validation
2024-02-09 01:41:37,674:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 02:09:01,070:INFO:Calculating mean and std
2024-02-09 02:09:01,074:INFO:Creating metrics dataframe
2024-02-09 02:09:01,085:INFO:Uploading results into container
2024-02-09 02:09:01,086:INFO:Uploading model into container now
2024-02-09 02:09:01,087:INFO:_master_model_container: 10
2024-02-09 02:09:01,087:INFO:_display_container: 2
2024-02-09 02:09:01,088:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6467, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-02-09 02:09:01,088:INFO:create_model() successfully completed......................................
2024-02-09 02:09:01,195:INFO:SubProcess create_model() end ==================================
2024-02-09 02:09:01,195:INFO:Creating metrics dataframe
2024-02-09 02:09:01,235:INFO:Initializing Linear Discriminant Analysis
2024-02-09 02:09:01,235:INFO:Total runtime is 45.39896542628606 minutes
2024-02-09 02:09:01,247:INFO:SubProcess create_model() called ==================================
2024-02-09 02:09:01,248:INFO:Initializing create_model()
2024-02-09 02:09:01,248:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=lda, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82DA80370>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 02:09:01,248:INFO:Checking exceptions
2024-02-09 02:09:01,248:INFO:Importing libraries
2024-02-09 02:09:01,248:INFO:Copying training dataset
2024-02-09 02:09:01,383:INFO:Defining folds
2024-02-09 02:09:01,384:INFO:Declaring metric variables
2024-02-09 02:09:01,389:INFO:Importing untrained model
2024-02-09 02:09:01,396:INFO:Linear Discriminant Analysis Imported successfully
2024-02-09 02:09:01,410:INFO:Starting cross validation
2024-02-09 02:09:01,416:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 02:09:37,314:INFO:Calculating mean and std
2024-02-09 02:09:37,317:INFO:Creating metrics dataframe
2024-02-09 02:09:37,326:INFO:Uploading results into container
2024-02-09 02:09:37,328:INFO:Uploading model into container now
2024-02-09 02:09:37,329:INFO:_master_model_container: 11
2024-02-09 02:09:37,329:INFO:_display_container: 2
2024-02-09 02:09:37,330:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-02-09 02:09:37,331:INFO:create_model() successfully completed......................................
2024-02-09 02:09:37,465:INFO:SubProcess create_model() end ==================================
2024-02-09 02:09:37,465:INFO:Creating metrics dataframe
2024-02-09 02:09:37,515:INFO:Initializing Extra Trees Classifier
2024-02-09 02:09:37,516:INFO:Total runtime is 46.003645265102385 minutes
2024-02-09 02:09:37,528:INFO:SubProcess create_model() called ==================================
2024-02-09 02:09:37,529:INFO:Initializing create_model()
2024-02-09 02:09:37,530:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=et, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82DA80370>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 02:09:37,530:INFO:Checking exceptions
2024-02-09 02:09:37,531:INFO:Importing libraries
2024-02-09 02:09:37,531:INFO:Copying training dataset
2024-02-09 02:09:37,678:INFO:Defining folds
2024-02-09 02:09:37,678:INFO:Declaring metric variables
2024-02-09 02:09:37,684:INFO:Importing untrained model
2024-02-09 02:09:37,692:INFO:Extra Trees Classifier Imported successfully
2024-02-09 02:09:37,713:INFO:Starting cross validation
2024-02-09 02:09:37,722:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 02:12:44,838:INFO:Calculating mean and std
2024-02-09 02:12:44,840:INFO:Creating metrics dataframe
2024-02-09 02:12:44,848:INFO:Uploading results into container
2024-02-09 02:12:44,850:INFO:Uploading model into container now
2024-02-09 02:12:44,851:INFO:_master_model_container: 12
2024-02-09 02:12:44,851:INFO:_display_container: 2
2024-02-09 02:12:44,853:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=6467, verbose=0, warm_start=False)
2024-02-09 02:12:44,853:INFO:create_model() successfully completed......................................
2024-02-09 02:12:44,977:INFO:SubProcess create_model() end ==================================
2024-02-09 02:12:44,977:INFO:Creating metrics dataframe
2024-02-09 02:12:45,010:INFO:Initializing Light Gradient Boosting Machine
2024-02-09 02:12:45,010:INFO:Total runtime is 49.12854929367701 minutes
2024-02-09 02:12:45,020:INFO:SubProcess create_model() called ==================================
2024-02-09 02:12:45,021:INFO:Initializing create_model()
2024-02-09 02:12:45,021:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=lightgbm, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82DA80370>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 02:12:45,022:INFO:Checking exceptions
2024-02-09 02:12:45,022:INFO:Importing libraries
2024-02-09 02:12:45,022:INFO:Copying training dataset
2024-02-09 02:12:45,223:INFO:Defining folds
2024-02-09 02:12:45,224:INFO:Declaring metric variables
2024-02-09 02:12:45,234:INFO:Importing untrained model
2024-02-09 02:12:45,245:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-09 02:12:45,266:INFO:Starting cross validation
2024-02-09 02:12:45,273:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 02:13:45,197:INFO:Calculating mean and std
2024-02-09 02:13:45,200:INFO:Creating metrics dataframe
2024-02-09 02:13:45,211:INFO:Uploading results into container
2024-02-09 02:13:45,212:INFO:Uploading model into container now
2024-02-09 02:13:45,214:INFO:_master_model_container: 13
2024-02-09 02:13:45,214:INFO:_display_container: 2
2024-02-09 02:13:45,216:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6467, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-02-09 02:13:45,217:INFO:create_model() successfully completed......................................
2024-02-09 02:13:45,356:INFO:SubProcess create_model() end ==================================
2024-02-09 02:13:45,357:INFO:Creating metrics dataframe
2024-02-09 02:13:45,391:INFO:Initializing CatBoost Classifier
2024-02-09 02:13:45,391:INFO:Total runtime is 50.134897673130034 minutes
2024-02-09 02:13:45,397:INFO:SubProcess create_model() called ==================================
2024-02-09 02:13:45,397:INFO:Initializing create_model()
2024-02-09 02:13:45,398:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=catboost, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82DA80370>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 02:13:45,398:INFO:Checking exceptions
2024-02-09 02:13:45,398:INFO:Importing libraries
2024-02-09 02:13:45,398:INFO:Copying training dataset
2024-02-09 02:13:45,624:INFO:Defining folds
2024-02-09 02:13:45,625:INFO:Declaring metric variables
2024-02-09 02:13:45,637:INFO:Importing untrained model
2024-02-09 02:13:45,648:INFO:CatBoost Classifier Imported successfully
2024-02-09 02:13:45,668:INFO:Starting cross validation
2024-02-09 02:13:45,674:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 02:25:27,094:INFO:Calculating mean and std
2024-02-09 02:25:27,096:INFO:Creating metrics dataframe
2024-02-09 02:25:27,101:INFO:Uploading results into container
2024-02-09 02:25:27,102:INFO:Uploading model into container now
2024-02-09 02:25:27,102:INFO:_master_model_container: 14
2024-02-09 02:25:27,102:INFO:_display_container: 2
2024-02-09 02:25:27,102:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82D609E40>
2024-02-09 02:25:27,102:INFO:create_model() successfully completed......................................
2024-02-09 02:25:27,178:INFO:SubProcess create_model() end ==================================
2024-02-09 02:25:27,178:INFO:Creating metrics dataframe
2024-02-09 02:25:27,195:INFO:Initializing Dummy Classifier
2024-02-09 02:25:27,195:INFO:Total runtime is 61.83161997795105 minutes
2024-02-09 02:25:27,203:INFO:SubProcess create_model() called ==================================
2024-02-09 02:25:27,203:INFO:Initializing create_model()
2024-02-09 02:25:27,203:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=dummy, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82DA80370>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 02:25:27,203:INFO:Checking exceptions
2024-02-09 02:25:27,203:INFO:Importing libraries
2024-02-09 02:25:27,204:INFO:Copying training dataset
2024-02-09 02:25:27,315:INFO:Defining folds
2024-02-09 02:25:27,315:INFO:Declaring metric variables
2024-02-09 02:25:27,321:INFO:Importing untrained model
2024-02-09 02:25:27,326:INFO:Dummy Classifier Imported successfully
2024-02-09 02:25:27,334:INFO:Starting cross validation
2024-02-09 02:25:27,338:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 02:25:29,542:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 02:25:29,554:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 02:25:29,566:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 02:25:29,573:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 02:25:29,606:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 02:25:29,617:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 02:25:29,624:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 02:25:29,634:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 02:25:31,765:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 02:25:31,821:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 02:25:31,839:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 02:25:31,878:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 02:25:31,880:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 02:25:31,893:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 02:25:31,903:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 02:25:31,926:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 02:25:33,270:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 02:25:33,284:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 02:25:33,347:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 02:25:33,367:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 02:25:33,485:INFO:Calculating mean and std
2024-02-09 02:25:33,487:INFO:Creating metrics dataframe
2024-02-09 02:25:33,491:INFO:Uploading results into container
2024-02-09 02:25:33,491:INFO:Uploading model into container now
2024-02-09 02:25:33,492:INFO:_master_model_container: 15
2024-02-09 02:25:33,492:INFO:_display_container: 2
2024-02-09 02:25:33,492:INFO:DummyClassifier(constant=None, random_state=6467, strategy='prior')
2024-02-09 02:25:33,492:INFO:create_model() successfully completed......................................
2024-02-09 02:25:33,577:INFO:SubProcess create_model() end ==================================
2024-02-09 02:25:33,578:INFO:Creating metrics dataframe
2024-02-09 02:25:33,607:INFO:Initializing create_model()
2024-02-09 02:25:33,607:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=<catboost.core.CatBoostClassifier object at 0x000001D82D609E40>, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 02:25:33,608:INFO:Checking exceptions
2024-02-09 02:25:33,610:INFO:Importing libraries
2024-02-09 02:25:33,610:INFO:Copying training dataset
2024-02-09 02:25:33,711:INFO:Defining folds
2024-02-09 02:25:33,711:INFO:Declaring metric variables
2024-02-09 02:25:33,712:INFO:Importing untrained model
2024-02-09 02:25:33,712:INFO:Declaring custom model
2024-02-09 02:25:33,712:INFO:CatBoost Classifier Imported successfully
2024-02-09 02:25:33,714:INFO:Cross validation set to False
2024-02-09 02:25:33,715:INFO:Fitting Model
2024-02-09 02:26:17,606:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82D5D8B20>
2024-02-09 02:26:17,606:INFO:create_model() successfully completed......................................
2024-02-09 02:26:17,700:INFO:Initializing create_model()
2024-02-09 02:26:17,700:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6467, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 02:26:17,702:INFO:Checking exceptions
2024-02-09 02:26:17,705:INFO:Importing libraries
2024-02-09 02:26:17,705:INFO:Copying training dataset
2024-02-09 02:26:17,815:INFO:Defining folds
2024-02-09 02:26:17,815:INFO:Declaring metric variables
2024-02-09 02:26:17,816:INFO:Importing untrained model
2024-02-09 02:26:17,816:INFO:Declaring custom model
2024-02-09 02:26:17,817:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-09 02:26:17,819:INFO:Cross validation set to False
2024-02-09 02:26:17,820:INFO:Fitting Model
2024-02-09 02:26:19,032:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-09 02:26:19,128:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.078375 seconds.
2024-02-09 02:26:19,128:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-09 02:26:19,130:INFO:[LightGBM] [Info] Total Bins 32382
2024-02-09 02:26:19,132:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 127
2024-02-09 02:26:19,134:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-09 02:26:21,824:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6467, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-02-09 02:26:21,824:INFO:create_model() successfully completed......................................
2024-02-09 02:26:21,932:INFO:Initializing create_model()
2024-02-09 02:26:21,932:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=6467, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 02:26:21,932:INFO:Checking exceptions
2024-02-09 02:26:21,936:INFO:Importing libraries
2024-02-09 02:26:21,937:INFO:Copying training dataset
2024-02-09 02:26:22,057:INFO:Defining folds
2024-02-09 02:26:22,058:INFO:Declaring metric variables
2024-02-09 02:26:22,058:INFO:Importing untrained model
2024-02-09 02:26:22,058:INFO:Declaring custom model
2024-02-09 02:26:22,059:INFO:Extra Trees Classifier Imported successfully
2024-02-09 02:26:22,062:INFO:Cross validation set to False
2024-02-09 02:26:22,062:INFO:Fitting Model
2024-02-09 02:26:31,864:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=6467, verbose=0, warm_start=False)
2024-02-09 02:26:31,865:INFO:create_model() successfully completed......................................
2024-02-09 02:26:31,947:INFO:Initializing create_model()
2024-02-09 02:26:31,947:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6467, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 02:26:31,948:INFO:Checking exceptions
2024-02-09 02:26:31,951:INFO:Importing libraries
2024-02-09 02:26:31,952:INFO:Copying training dataset
2024-02-09 02:26:32,059:INFO:Defining folds
2024-02-09 02:26:32,060:INFO:Declaring metric variables
2024-02-09 02:26:32,060:INFO:Importing untrained model
2024-02-09 02:26:32,060:INFO:Declaring custom model
2024-02-09 02:26:32,061:INFO:Random Forest Classifier Imported successfully
2024-02-09 02:26:32,063:INFO:Cross validation set to False
2024-02-09 02:26:32,063:INFO:Fitting Model
2024-02-09 02:26:56,206:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6467, verbose=0, warm_start=False)
2024-02-09 02:26:56,206:INFO:create_model() successfully completed......................................
2024-02-09 02:26:56,292:INFO:Initializing create_model()
2024-02-09 02:26:56,292:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6467, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 02:26:56,293:INFO:Checking exceptions
2024-02-09 02:26:56,298:INFO:Importing libraries
2024-02-09 02:26:56,298:INFO:Copying training dataset
2024-02-09 02:26:56,416:INFO:Defining folds
2024-02-09 02:26:56,416:INFO:Declaring metric variables
2024-02-09 02:26:56,416:INFO:Importing untrained model
2024-02-09 02:26:56,416:INFO:Declaring custom model
2024-02-09 02:26:56,417:INFO:Gradient Boosting Classifier Imported successfully
2024-02-09 02:26:56,420:INFO:Cross validation set to False
2024-02-09 02:26:56,420:INFO:Fitting Model
2024-02-09 02:31:35,444:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6467, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-02-09 02:31:35,445:INFO:create_model() successfully completed......................................
2024-02-09 02:31:35,554:INFO:_master_model_container: 15
2024-02-09 02:31:35,554:INFO:_display_container: 2
2024-02-09 02:31:35,556:INFO:[<catboost.core.CatBoostClassifier object at 0x000001D82D5D8B20>, LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6467, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=6467, verbose=0, warm_start=False), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6467, verbose=0, warm_start=False), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6467, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)]
2024-02-09 02:31:35,556:INFO:compare_models() successfully completed......................................
2024-02-09 02:36:47,897:INFO:Initializing predict_model()
2024-02-09 02:36:47,897:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=<catboost.core.CatBoostClassifier object at 0x000001D82D5D8B20>, probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D82DDECCA0>)
2024-02-09 02:36:47,897:INFO:Checking exceptions
2024-02-09 02:36:47,897:INFO:Preloading libraries
2024-02-09 02:36:47,900:INFO:Set up data.
2024-02-09 02:36:47,988:INFO:Set up index.
2024-02-09 02:38:20,035:INFO:Initializing compare_models()
2024-02-09 02:38:20,035:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, include=None, fold=20, round=4, cross_validation=True, sort=Accuracy, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, 'include': None, 'exclude': None, 'fold': 20, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-02-09 02:38:20,035:INFO:Checking exceptions
2024-02-09 02:38:20,090:INFO:Preparing display monitor
2024-02-09 02:38:20,147:INFO:Initializing Logistic Regression
2024-02-09 02:38:20,147:INFO:Total runtime is 0.0 minutes
2024-02-09 02:38:20,152:INFO:SubProcess create_model() called ==================================
2024-02-09 02:38:20,153:INFO:Initializing create_model()
2024-02-09 02:38:20,153:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=lr, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82D5D89A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 02:38:20,153:INFO:Checking exceptions
2024-02-09 02:38:20,153:INFO:Importing libraries
2024-02-09 02:38:20,153:INFO:Copying training dataset
2024-02-09 02:38:20,284:INFO:Defining folds
2024-02-09 02:38:20,285:INFO:Declaring metric variables
2024-02-09 02:38:20,289:INFO:Importing untrained model
2024-02-09 02:38:20,296:INFO:Logistic Regression Imported successfully
2024-02-09 02:38:20,307:INFO:Starting cross validation
2024-02-09 02:38:20,311:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 02:39:18,481:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 02:39:19,013:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 02:39:19,150:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 02:39:19,611:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 02:39:19,732:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 02:39:19,793:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 02:39:20,277:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 02:39:20,630:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 02:40:12,366:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 02:40:13,033:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 02:40:13,312:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 02:40:13,578:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 02:40:13,638:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 02:40:13,991:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 02:40:14,288:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 02:40:14,449:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 02:40:44,088:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 02:40:44,291:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 02:40:44,457:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 02:40:44,516:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 02:40:44,665:INFO:Calculating mean and std
2024-02-09 02:40:44,668:INFO:Creating metrics dataframe
2024-02-09 02:40:44,676:INFO:Uploading results into container
2024-02-09 02:40:44,676:INFO:Uploading model into container now
2024-02-09 02:40:44,677:INFO:_master_model_container: 16
2024-02-09 02:40:44,677:INFO:_display_container: 3
2024-02-09 02:40:44,678:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=6467, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-02-09 02:40:44,678:INFO:create_model() successfully completed......................................
2024-02-09 02:40:44,751:INFO:SubProcess create_model() end ==================================
2024-02-09 02:40:44,751:INFO:Creating metrics dataframe
2024-02-09 02:40:44,761:INFO:Initializing K Neighbors Classifier
2024-02-09 02:40:44,761:INFO:Total runtime is 2.4102333982785544 minutes
2024-02-09 02:40:44,764:INFO:SubProcess create_model() called ==================================
2024-02-09 02:40:44,765:INFO:Initializing create_model()
2024-02-09 02:40:44,765:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=knn, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82D5D89A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 02:40:44,765:INFO:Checking exceptions
2024-02-09 02:40:44,765:INFO:Importing libraries
2024-02-09 02:40:44,765:INFO:Copying training dataset
2024-02-09 02:40:44,850:INFO:Defining folds
2024-02-09 02:40:44,851:INFO:Declaring metric variables
2024-02-09 02:40:44,854:INFO:Importing untrained model
2024-02-09 02:40:44,860:INFO:K Neighbors Classifier Imported successfully
2024-02-09 02:40:44,867:INFO:Starting cross validation
2024-02-09 02:40:44,870:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 02:41:21,215:INFO:Calculating mean and std
2024-02-09 02:41:21,216:INFO:Creating metrics dataframe
2024-02-09 02:41:21,219:INFO:Uploading results into container
2024-02-09 02:41:21,220:INFO:Uploading model into container now
2024-02-09 02:41:21,220:INFO:_master_model_container: 17
2024-02-09 02:41:21,220:INFO:_display_container: 3
2024-02-09 02:41:21,221:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-02-09 02:41:21,221:INFO:create_model() successfully completed......................................
2024-02-09 02:41:21,298:INFO:SubProcess create_model() end ==================================
2024-02-09 02:41:21,298:INFO:Creating metrics dataframe
2024-02-09 02:41:21,307:INFO:Initializing Naive Bayes
2024-02-09 02:41:21,307:INFO:Total runtime is 3.019342855612437 minutes
2024-02-09 02:41:21,311:INFO:SubProcess create_model() called ==================================
2024-02-09 02:41:21,311:INFO:Initializing create_model()
2024-02-09 02:41:21,311:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=nb, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82D5D89A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 02:41:21,311:INFO:Checking exceptions
2024-02-09 02:41:21,311:INFO:Importing libraries
2024-02-09 02:41:21,311:INFO:Copying training dataset
2024-02-09 02:41:21,392:INFO:Defining folds
2024-02-09 02:41:21,393:INFO:Declaring metric variables
2024-02-09 02:41:21,397:INFO:Importing untrained model
2024-02-09 02:41:21,402:INFO:Naive Bayes Imported successfully
2024-02-09 02:41:21,412:INFO:Starting cross validation
2024-02-09 02:41:21,416:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 02:41:29,507:INFO:Calculating mean and std
2024-02-09 02:41:29,509:INFO:Creating metrics dataframe
2024-02-09 02:41:29,512:INFO:Uploading results into container
2024-02-09 02:41:29,513:INFO:Uploading model into container now
2024-02-09 02:41:29,514:INFO:_master_model_container: 18
2024-02-09 02:41:29,514:INFO:_display_container: 3
2024-02-09 02:41:29,514:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-02-09 02:41:29,514:INFO:create_model() successfully completed......................................
2024-02-09 02:41:29,589:INFO:SubProcess create_model() end ==================================
2024-02-09 02:41:29,590:INFO:Creating metrics dataframe
2024-02-09 02:41:29,603:INFO:Initializing Decision Tree Classifier
2024-02-09 02:41:29,604:INFO:Total runtime is 3.15762345790863 minutes
2024-02-09 02:41:29,609:INFO:SubProcess create_model() called ==================================
2024-02-09 02:41:29,610:INFO:Initializing create_model()
2024-02-09 02:41:29,610:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=dt, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82D5D89A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 02:41:29,610:INFO:Checking exceptions
2024-02-09 02:41:29,610:INFO:Importing libraries
2024-02-09 02:41:29,610:INFO:Copying training dataset
2024-02-09 02:41:29,708:INFO:Defining folds
2024-02-09 02:41:29,708:INFO:Declaring metric variables
2024-02-09 02:41:29,712:INFO:Importing untrained model
2024-02-09 02:41:29,719:INFO:Decision Tree Classifier Imported successfully
2024-02-09 02:41:29,727:INFO:Starting cross validation
2024-02-09 02:41:29,731:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 02:42:42,563:INFO:Calculating mean and std
2024-02-09 02:42:42,567:INFO:Creating metrics dataframe
2024-02-09 02:42:42,571:INFO:Uploading results into container
2024-02-09 02:42:42,573:INFO:Uploading model into container now
2024-02-09 02:42:42,573:INFO:_master_model_container: 19
2024-02-09 02:42:42,573:INFO:_display_container: 3
2024-02-09 02:42:42,574:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=6467, splitter='best')
2024-02-09 02:42:42,574:INFO:create_model() successfully completed......................................
2024-02-09 02:42:42,655:INFO:SubProcess create_model() end ==================================
2024-02-09 02:42:42,655:INFO:Creating metrics dataframe
2024-02-09 02:42:42,665:INFO:Initializing SVM - Linear Kernel
2024-02-09 02:42:42,665:INFO:Total runtime is 4.375299803415934 minutes
2024-02-09 02:42:42,668:INFO:SubProcess create_model() called ==================================
2024-02-09 02:42:42,668:INFO:Initializing create_model()
2024-02-09 02:42:42,668:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=svm, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82D5D89A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 02:42:42,668:INFO:Checking exceptions
2024-02-09 02:42:42,669:INFO:Importing libraries
2024-02-09 02:42:42,669:INFO:Copying training dataset
2024-02-09 02:42:42,747:INFO:Defining folds
2024-02-09 02:42:42,748:INFO:Declaring metric variables
2024-02-09 02:42:42,751:INFO:Importing untrained model
2024-02-09 02:42:42,757:INFO:SVM - Linear Kernel Imported successfully
2024-02-09 02:42:42,765:INFO:Starting cross validation
2024-02-09 02:42:42,769:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 02:43:02,151:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 02:43:03,273:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 02:43:05,094:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 02:43:09,081:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 02:43:10,633:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 02:43:10,859:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 02:43:11,754:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 02:43:15,764:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 02:43:22,889:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 02:43:24,857:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 02:43:31,214:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 02:43:31,760:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 02:43:32,611:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 02:43:34,839:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 02:43:41,227:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 02:43:42,219:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 02:43:42,672:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 02:43:48,573:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 02:43:49,682:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 02:43:50,935:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-09 02:43:51,065:INFO:Calculating mean and std
2024-02-09 02:43:51,068:INFO:Creating metrics dataframe
2024-02-09 02:43:51,074:INFO:Uploading results into container
2024-02-09 02:43:51,074:INFO:Uploading model into container now
2024-02-09 02:43:51,075:INFO:_master_model_container: 20
2024-02-09 02:43:51,075:INFO:_display_container: 3
2024-02-09 02:43:51,075:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=6467, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-02-09 02:43:51,075:INFO:create_model() successfully completed......................................
2024-02-09 02:43:51,144:INFO:SubProcess create_model() end ==================================
2024-02-09 02:43:51,144:INFO:Creating metrics dataframe
2024-02-09 02:43:51,158:INFO:Initializing Ridge Classifier
2024-02-09 02:43:51,158:INFO:Total runtime is 5.516853121916453 minutes
2024-02-09 02:43:51,165:INFO:SubProcess create_model() called ==================================
2024-02-09 02:43:51,165:INFO:Initializing create_model()
2024-02-09 02:43:51,165:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=ridge, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82D5D89A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 02:43:51,165:INFO:Checking exceptions
2024-02-09 02:43:51,165:INFO:Importing libraries
2024-02-09 02:43:51,165:INFO:Copying training dataset
2024-02-09 02:43:51,261:INFO:Defining folds
2024-02-09 02:43:51,262:INFO:Declaring metric variables
2024-02-09 02:43:51,266:INFO:Importing untrained model
2024-02-09 02:43:51,271:INFO:Ridge Classifier Imported successfully
2024-02-09 02:43:51,280:INFO:Starting cross validation
2024-02-09 02:43:51,283:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 02:43:54,177:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 02:43:54,345:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 02:43:54,346:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 02:43:54,349:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 02:43:54,357:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 02:43:54,358:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 02:43:54,367:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 02:43:54,407:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 02:43:57,132:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 02:43:57,413:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 02:43:57,480:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 02:43:57,495:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 02:43:57,526:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 02:43:57,605:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 02:43:57,605:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 02:43:57,628:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 02:43:59,273:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 02:43:59,450:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 02:43:59,484:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 02:43:59,523:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-09 02:43:59,661:INFO:Calculating mean and std
2024-02-09 02:43:59,663:INFO:Creating metrics dataframe
2024-02-09 02:43:59,668:INFO:Uploading results into container
2024-02-09 02:43:59,669:INFO:Uploading model into container now
2024-02-09 02:43:59,670:INFO:_master_model_container: 21
2024-02-09 02:43:59,670:INFO:_display_container: 3
2024-02-09 02:43:59,671:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=6467, solver='auto',
                tol=0.0001)
2024-02-09 02:43:59,672:INFO:create_model() successfully completed......................................
2024-02-09 02:43:59,762:INFO:SubProcess create_model() end ==================================
2024-02-09 02:43:59,762:INFO:Creating metrics dataframe
2024-02-09 02:43:59,774:INFO:Initializing Random Forest Classifier
2024-02-09 02:43:59,774:INFO:Total runtime is 5.660463666915894 minutes
2024-02-09 02:43:59,780:INFO:SubProcess create_model() called ==================================
2024-02-09 02:43:59,781:INFO:Initializing create_model()
2024-02-09 02:43:59,781:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=rf, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82D5D89A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 02:43:59,781:INFO:Checking exceptions
2024-02-09 02:43:59,781:INFO:Importing libraries
2024-02-09 02:43:59,781:INFO:Copying training dataset
2024-02-09 02:43:59,882:INFO:Defining folds
2024-02-09 02:43:59,883:INFO:Declaring metric variables
2024-02-09 02:43:59,887:INFO:Importing untrained model
2024-02-09 02:43:59,893:INFO:Random Forest Classifier Imported successfully
2024-02-09 02:43:59,903:INFO:Starting cross validation
2024-02-09 02:43:59,906:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 02:50:58,450:INFO:Calculating mean and std
2024-02-09 02:50:58,452:INFO:Creating metrics dataframe
2024-02-09 02:50:58,455:INFO:Uploading results into container
2024-02-09 02:50:58,456:INFO:Uploading model into container now
2024-02-09 02:50:58,457:INFO:_master_model_container: 22
2024-02-09 02:50:58,457:INFO:_display_container: 3
2024-02-09 02:50:58,457:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6467, verbose=0, warm_start=False)
2024-02-09 02:50:58,457:INFO:create_model() successfully completed......................................
2024-02-09 02:50:58,547:INFO:SubProcess create_model() end ==================================
2024-02-09 02:50:58,548:INFO:Creating metrics dataframe
2024-02-09 02:50:58,569:INFO:Initializing Quadratic Discriminant Analysis
2024-02-09 02:50:58,570:INFO:Total runtime is 12.640387523174287 minutes
2024-02-09 02:50:58,576:INFO:SubProcess create_model() called ==================================
2024-02-09 02:50:58,577:INFO:Initializing create_model()
2024-02-09 02:50:58,577:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=qda, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82D5D89A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 02:50:58,577:INFO:Checking exceptions
2024-02-09 02:50:58,578:INFO:Importing libraries
2024-02-09 02:50:58,578:INFO:Copying training dataset
2024-02-09 02:50:58,679:INFO:Defining folds
2024-02-09 02:50:58,679:INFO:Declaring metric variables
2024-02-09 02:50:58,686:INFO:Importing untrained model
2024-02-09 02:50:58,692:INFO:Quadratic Discriminant Analysis Imported successfully
2024-02-09 02:50:58,701:INFO:Starting cross validation
2024-02-09 02:50:58,705:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 02:51:05,967:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 02:51:05,989:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 02:51:06,046:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 02:51:06,107:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 02:51:06,319:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 02:51:06,379:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 02:51:06,457:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 02:51:06,476:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 02:51:18,192:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 02:51:18,326:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 02:51:18,547:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 02:51:18,784:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 02:51:19,078:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 02:51:19,123:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 02:51:19,295:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 02:51:19,317:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 02:51:27,974:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 02:51:28,121:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 02:51:28,196:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 02:51:28,260:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-09 02:51:31,001:INFO:Calculating mean and std
2024-02-09 02:51:31,003:INFO:Creating metrics dataframe
2024-02-09 02:51:31,007:INFO:Uploading results into container
2024-02-09 02:51:31,008:INFO:Uploading model into container now
2024-02-09 02:51:31,009:INFO:_master_model_container: 23
2024-02-09 02:51:31,009:INFO:_display_container: 3
2024-02-09 02:51:31,009:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-02-09 02:51:31,010:INFO:create_model() successfully completed......................................
2024-02-09 02:51:31,098:INFO:SubProcess create_model() end ==================================
2024-02-09 02:51:31,098:INFO:Creating metrics dataframe
2024-02-09 02:51:31,111:INFO:Initializing Ada Boost Classifier
2024-02-09 02:51:31,111:INFO:Total runtime is 13.18274836142858 minutes
2024-02-09 02:51:31,115:INFO:SubProcess create_model() called ==================================
2024-02-09 02:51:31,116:INFO:Initializing create_model()
2024-02-09 02:51:31,116:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=ada, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82D5D89A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 02:51:31,116:INFO:Checking exceptions
2024-02-09 02:51:31,116:INFO:Importing libraries
2024-02-09 02:51:31,117:INFO:Copying training dataset
2024-02-09 02:51:31,218:INFO:Defining folds
2024-02-09 02:51:31,219:INFO:Declaring metric variables
2024-02-09 02:51:31,225:INFO:Importing untrained model
2024-02-09 02:51:31,230:INFO:Ada Boost Classifier Imported successfully
2024-02-09 02:51:31,241:INFO:Starting cross validation
2024-02-09 02:51:31,244:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 02:56:32,168:INFO:Calculating mean and std
2024-02-09 02:56:32,170:INFO:Creating metrics dataframe
2024-02-09 02:56:32,174:INFO:Uploading results into container
2024-02-09 02:56:32,174:INFO:Uploading model into container now
2024-02-09 02:56:32,175:INFO:_master_model_container: 24
2024-02-09 02:56:32,175:INFO:_display_container: 3
2024-02-09 02:56:32,175:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=6467)
2024-02-09 02:56:32,175:INFO:create_model() successfully completed......................................
2024-02-09 02:56:32,274:INFO:SubProcess create_model() end ==================================
2024-02-09 02:56:32,274:INFO:Creating metrics dataframe
2024-02-09 02:56:32,292:INFO:Initializing Gradient Boosting Classifier
2024-02-09 02:56:32,293:INFO:Total runtime is 18.202435580889386 minutes
2024-02-09 02:56:32,299:INFO:SubProcess create_model() called ==================================
2024-02-09 02:56:32,300:INFO:Initializing create_model()
2024-02-09 02:56:32,300:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=gbc, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82D5D89A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 02:56:32,300:INFO:Checking exceptions
2024-02-09 02:56:32,301:INFO:Importing libraries
2024-02-09 02:56:32,301:INFO:Copying training dataset
2024-02-09 02:56:32,435:INFO:Defining folds
2024-02-09 02:56:32,435:INFO:Declaring metric variables
2024-02-09 02:56:32,442:INFO:Importing untrained model
2024-02-09 02:56:32,447:INFO:Gradient Boosting Classifier Imported successfully
2024-02-09 02:56:32,457:INFO:Starting cross validation
2024-02-09 02:56:32,460:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 03:22:05,326:INFO:Calculating mean and std
2024-02-09 03:22:05,327:INFO:Creating metrics dataframe
2024-02-09 03:22:05,331:INFO:Uploading results into container
2024-02-09 03:22:05,332:INFO:Uploading model into container now
2024-02-09 03:22:05,332:INFO:_master_model_container: 25
2024-02-09 03:22:05,332:INFO:_display_container: 3
2024-02-09 03:22:05,333:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6467, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-02-09 03:22:05,333:INFO:create_model() successfully completed......................................
2024-02-09 03:22:05,409:INFO:SubProcess create_model() end ==================================
2024-02-09 03:22:05,410:INFO:Creating metrics dataframe
2024-02-09 03:22:05,423:INFO:Initializing Linear Discriminant Analysis
2024-02-09 03:22:05,424:INFO:Total runtime is 43.75462181568146 minutes
2024-02-09 03:22:05,430:INFO:SubProcess create_model() called ==================================
2024-02-09 03:22:05,431:INFO:Initializing create_model()
2024-02-09 03:22:05,431:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=lda, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82D5D89A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 03:22:05,431:INFO:Checking exceptions
2024-02-09 03:22:05,431:INFO:Importing libraries
2024-02-09 03:22:05,431:INFO:Copying training dataset
2024-02-09 03:22:05,526:INFO:Defining folds
2024-02-09 03:22:05,526:INFO:Declaring metric variables
2024-02-09 03:22:05,530:INFO:Importing untrained model
2024-02-09 03:22:05,535:INFO:Linear Discriminant Analysis Imported successfully
2024-02-09 03:22:05,544:INFO:Starting cross validation
2024-02-09 03:22:05,547:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 03:22:42,238:INFO:Calculating mean and std
2024-02-09 03:22:42,240:INFO:Creating metrics dataframe
2024-02-09 03:22:42,244:INFO:Uploading results into container
2024-02-09 03:22:42,245:INFO:Uploading model into container now
2024-02-09 03:22:42,245:INFO:_master_model_container: 26
2024-02-09 03:22:42,245:INFO:_display_container: 3
2024-02-09 03:22:42,245:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-02-09 03:22:42,245:INFO:create_model() successfully completed......................................
2024-02-09 03:22:42,326:INFO:SubProcess create_model() end ==================================
2024-02-09 03:22:42,326:INFO:Creating metrics dataframe
2024-02-09 03:22:42,347:INFO:Initializing Extra Trees Classifier
2024-02-09 03:22:42,348:INFO:Total runtime is 44.37001970211665 minutes
2024-02-09 03:22:42,353:INFO:SubProcess create_model() called ==================================
2024-02-09 03:22:42,354:INFO:Initializing create_model()
2024-02-09 03:22:42,354:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=et, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82D5D89A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 03:22:42,355:INFO:Checking exceptions
2024-02-09 03:22:42,355:INFO:Importing libraries
2024-02-09 03:22:42,355:INFO:Copying training dataset
2024-02-09 03:22:42,468:INFO:Defining folds
2024-02-09 03:22:42,468:INFO:Declaring metric variables
2024-02-09 03:22:42,472:INFO:Importing untrained model
2024-02-09 03:22:42,480:INFO:Extra Trees Classifier Imported successfully
2024-02-09 03:22:42,495:INFO:Starting cross validation
2024-02-09 03:22:42,498:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 03:25:49,592:INFO:Calculating mean and std
2024-02-09 03:25:49,594:INFO:Creating metrics dataframe
2024-02-09 03:25:49,598:INFO:Uploading results into container
2024-02-09 03:25:49,599:INFO:Uploading model into container now
2024-02-09 03:25:49,600:INFO:_master_model_container: 27
2024-02-09 03:25:49,600:INFO:_display_container: 3
2024-02-09 03:25:49,601:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=6467, verbose=0, warm_start=False)
2024-02-09 03:25:49,601:INFO:create_model() successfully completed......................................
2024-02-09 03:25:49,692:INFO:SubProcess create_model() end ==================================
2024-02-09 03:25:49,692:INFO:Creating metrics dataframe
2024-02-09 03:25:49,706:INFO:Initializing Light Gradient Boosting Machine
2024-02-09 03:25:49,706:INFO:Total runtime is 47.492654450734456 minutes
2024-02-09 03:25:49,709:INFO:SubProcess create_model() called ==================================
2024-02-09 03:25:49,710:INFO:Initializing create_model()
2024-02-09 03:25:49,710:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=lightgbm, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82D5D89A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 03:25:49,710:INFO:Checking exceptions
2024-02-09 03:25:49,710:INFO:Importing libraries
2024-02-09 03:25:49,710:INFO:Copying training dataset
2024-02-09 03:25:49,808:INFO:Defining folds
2024-02-09 03:25:49,809:INFO:Declaring metric variables
2024-02-09 03:25:49,813:INFO:Importing untrained model
2024-02-09 03:25:49,820:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-09 03:25:49,831:INFO:Starting cross validation
2024-02-09 03:25:49,834:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 03:26:47,958:INFO:Calculating mean and std
2024-02-09 03:26:47,960:INFO:Creating metrics dataframe
2024-02-09 03:26:47,964:INFO:Uploading results into container
2024-02-09 03:26:47,965:INFO:Uploading model into container now
2024-02-09 03:26:47,966:INFO:_master_model_container: 28
2024-02-09 03:26:47,966:INFO:_display_container: 3
2024-02-09 03:26:47,967:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6467, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-02-09 03:26:47,967:INFO:create_model() successfully completed......................................
2024-02-09 03:26:48,076:INFO:SubProcess create_model() end ==================================
2024-02-09 03:26:48,077:INFO:Creating metrics dataframe
2024-02-09 03:26:48,094:INFO:Initializing CatBoost Classifier
2024-02-09 03:26:48,094:INFO:Total runtime is 48.4657958428065 minutes
2024-02-09 03:26:48,100:INFO:SubProcess create_model() called ==================================
2024-02-09 03:26:48,100:INFO:Initializing create_model()
2024-02-09 03:26:48,100:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=catboost, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82D5D89A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 03:26:48,100:INFO:Checking exceptions
2024-02-09 03:26:48,101:INFO:Importing libraries
2024-02-09 03:26:48,101:INFO:Copying training dataset
2024-02-09 03:26:48,229:INFO:Defining folds
2024-02-09 03:26:48,229:INFO:Declaring metric variables
2024-02-09 03:26:48,233:INFO:Importing untrained model
2024-02-09 03:26:48,240:INFO:CatBoost Classifier Imported successfully
2024-02-09 03:26:48,249:INFO:Starting cross validation
2024-02-09 03:26:48,254:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 03:38:17,353:INFO:Calculating mean and std
2024-02-09 03:38:17,355:INFO:Creating metrics dataframe
2024-02-09 03:38:17,359:INFO:Uploading results into container
2024-02-09 03:38:17,360:INFO:Uploading model into container now
2024-02-09 03:38:17,360:INFO:_master_model_container: 29
2024-02-09 03:38:17,360:INFO:_display_container: 3
2024-02-09 03:38:17,360:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82DE74760>
2024-02-09 03:38:17,361:INFO:create_model() successfully completed......................................
2024-02-09 03:38:17,485:INFO:SubProcess create_model() end ==================================
2024-02-09 03:38:17,485:INFO:Creating metrics dataframe
2024-02-09 03:38:17,502:INFO:Initializing Dummy Classifier
2024-02-09 03:38:17,502:INFO:Total runtime is 59.955923048655194 minutes
2024-02-09 03:38:17,507:INFO:SubProcess create_model() called ==================================
2024-02-09 03:38:17,507:INFO:Initializing create_model()
2024-02-09 03:38:17,507:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=dummy, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82D5D89A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 03:38:17,507:INFO:Checking exceptions
2024-02-09 03:38:17,508:INFO:Importing libraries
2024-02-09 03:38:17,508:INFO:Copying training dataset
2024-02-09 03:38:17,620:INFO:Defining folds
2024-02-09 03:38:17,620:INFO:Declaring metric variables
2024-02-09 03:38:17,625:INFO:Importing untrained model
2024-02-09 03:38:17,631:INFO:Dummy Classifier Imported successfully
2024-02-09 03:38:17,643:INFO:Starting cross validation
2024-02-09 03:38:17,649:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 03:38:19,892:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 03:38:20,058:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 03:38:20,084:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 03:38:20,086:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 03:38:20,106:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 03:38:20,148:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 03:38:20,176:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 03:38:20,206:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 03:38:22,098:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 03:38:22,359:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 03:38:22,395:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 03:38:22,426:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 03:38:22,464:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 03:38:22,554:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 03:38:22,618:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 03:38:22,624:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 03:38:23,818:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 03:38:23,889:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 03:38:23,944:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 03:38:23,984:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-09 03:38:24,108:INFO:Calculating mean and std
2024-02-09 03:38:24,109:INFO:Creating metrics dataframe
2024-02-09 03:38:24,114:INFO:Uploading results into container
2024-02-09 03:38:24,115:INFO:Uploading model into container now
2024-02-09 03:38:24,115:INFO:_master_model_container: 30
2024-02-09 03:38:24,115:INFO:_display_container: 3
2024-02-09 03:38:24,116:INFO:DummyClassifier(constant=None, random_state=6467, strategy='prior')
2024-02-09 03:38:24,116:INFO:create_model() successfully completed......................................
2024-02-09 03:38:24,202:INFO:SubProcess create_model() end ==================================
2024-02-09 03:38:24,202:INFO:Creating metrics dataframe
2024-02-09 03:38:24,237:INFO:Initializing create_model()
2024-02-09 03:38:24,237:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=<catboost.core.CatBoostClassifier object at 0x000001D82DE74760>, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 03:38:24,237:INFO:Checking exceptions
2024-02-09 03:38:24,240:INFO:Importing libraries
2024-02-09 03:38:24,241:INFO:Copying training dataset
2024-02-09 03:38:24,347:INFO:Defining folds
2024-02-09 03:38:24,347:INFO:Declaring metric variables
2024-02-09 03:38:24,347:INFO:Importing untrained model
2024-02-09 03:38:24,348:INFO:Declaring custom model
2024-02-09 03:38:24,348:INFO:CatBoost Classifier Imported successfully
2024-02-09 03:38:24,351:INFO:Cross validation set to False
2024-02-09 03:38:24,351:INFO:Fitting Model
2024-02-09 03:39:08,325:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82DE76A40>
2024-02-09 03:39:08,325:INFO:create_model() successfully completed......................................
2024-02-09 03:39:08,415:INFO:Initializing create_model()
2024-02-09 03:39:08,416:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6467, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 03:39:08,417:INFO:Checking exceptions
2024-02-09 03:39:08,420:INFO:Importing libraries
2024-02-09 03:39:08,420:INFO:Copying training dataset
2024-02-09 03:39:08,528:INFO:Defining folds
2024-02-09 03:39:08,529:INFO:Declaring metric variables
2024-02-09 03:39:08,529:INFO:Importing untrained model
2024-02-09 03:39:08,529:INFO:Declaring custom model
2024-02-09 03:39:08,530:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-09 03:39:08,533:INFO:Cross validation set to False
2024-02-09 03:39:08,533:INFO:Fitting Model
2024-02-09 03:39:09,757:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-09 03:39:09,849:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069649 seconds.
2024-02-09 03:39:09,849:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-09 03:39:09,851:INFO:[LightGBM] [Info] Total Bins 32382
2024-02-09 03:39:09,853:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 127
2024-02-09 03:39:09,855:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-09 03:39:12,605:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6467, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-02-09 03:39:12,606:INFO:create_model() successfully completed......................................
2024-02-09 03:39:12,715:INFO:Initializing create_model()
2024-02-09 03:39:12,715:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=6467, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 03:39:12,716:INFO:Checking exceptions
2024-02-09 03:39:12,719:INFO:Importing libraries
2024-02-09 03:39:12,720:INFO:Copying training dataset
2024-02-09 03:39:12,841:INFO:Defining folds
2024-02-09 03:39:12,841:INFO:Declaring metric variables
2024-02-09 03:39:12,842:INFO:Importing untrained model
2024-02-09 03:39:12,842:INFO:Declaring custom model
2024-02-09 03:39:12,843:INFO:Extra Trees Classifier Imported successfully
2024-02-09 03:39:12,846:INFO:Cross validation set to False
2024-02-09 03:39:12,846:INFO:Fitting Model
2024-02-09 03:39:22,789:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=6467, verbose=0, warm_start=False)
2024-02-09 03:39:22,789:INFO:create_model() successfully completed......................................
2024-02-09 03:39:22,876:INFO:Initializing create_model()
2024-02-09 03:39:22,877:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6467, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 03:39:22,877:INFO:Checking exceptions
2024-02-09 03:39:22,881:INFO:Importing libraries
2024-02-09 03:39:22,881:INFO:Copying training dataset
2024-02-09 03:39:22,989:INFO:Defining folds
2024-02-09 03:39:22,989:INFO:Declaring metric variables
2024-02-09 03:39:22,990:INFO:Importing untrained model
2024-02-09 03:39:22,990:INFO:Declaring custom model
2024-02-09 03:39:22,991:INFO:Random Forest Classifier Imported successfully
2024-02-09 03:39:22,993:INFO:Cross validation set to False
2024-02-09 03:39:22,993:INFO:Fitting Model
2024-02-09 03:39:47,818:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6467, verbose=0, warm_start=False)
2024-02-09 03:39:47,818:INFO:create_model() successfully completed......................................
2024-02-09 03:39:47,897:INFO:Initializing create_model()
2024-02-09 03:39:47,898:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6467, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 03:39:47,898:INFO:Checking exceptions
2024-02-09 03:39:47,900:INFO:Importing libraries
2024-02-09 03:39:47,900:INFO:Copying training dataset
2024-02-09 03:39:47,994:INFO:Defining folds
2024-02-09 03:39:47,994:INFO:Declaring metric variables
2024-02-09 03:39:47,994:INFO:Importing untrained model
2024-02-09 03:39:47,994:INFO:Declaring custom model
2024-02-09 03:39:47,995:INFO:Gradient Boosting Classifier Imported successfully
2024-02-09 03:39:47,997:INFO:Cross validation set to False
2024-02-09 03:39:47,997:INFO:Fitting Model
2024-02-09 03:44:23,729:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6467, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-02-09 03:44:23,729:INFO:create_model() successfully completed......................................
2024-02-09 03:44:23,845:INFO:_master_model_container: 30
2024-02-09 03:44:23,845:INFO:_display_container: 3
2024-02-09 03:44:23,847:INFO:[<catboost.core.CatBoostClassifier object at 0x000001D82DE76A40>, LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6467, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=6467, verbose=0, warm_start=False), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6467, verbose=0, warm_start=False), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6467, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)]
2024-02-09 03:44:23,847:INFO:compare_models() successfully completed......................................
2024-02-09 03:44:23,939:INFO:Initializing tune_model()
2024-02-09 03:44:23,939:INFO:tune_model(estimator=<catboost.core.CatBoostClassifier object at 0x000001D82DE76A40>, fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>)
2024-02-09 03:44:23,939:INFO:Checking exceptions
2024-02-09 03:44:24,000:INFO:Copying training dataset
2024-02-09 03:44:24,069:INFO:Checking base model
2024-02-09 03:44:24,069:INFO:Base model : CatBoost Classifier
2024-02-09 03:44:24,073:INFO:Declaring metric variables
2024-02-09 03:44:24,076:INFO:Defining Hyperparameters
2024-02-09 03:44:24,154:INFO:Tuning with n_jobs=-1
2024-02-09 03:44:24,154:INFO:Initializing RandomizedSearchCV
2024-02-09 04:57:29,951:INFO:best_params: {'actual_estimator__random_strength': 0.7, 'actual_estimator__n_estimators': 290, 'actual_estimator__l2_leaf_reg': 4, 'actual_estimator__eta': 0.1, 'actual_estimator__depth': 8}
2024-02-09 04:57:29,952:INFO:Hyperparameter search completed
2024-02-09 04:57:29,953:INFO:SubProcess create_model() called ==================================
2024-02-09 04:57:29,953:INFO:Initializing create_model()
2024-02-09 04:57:29,953:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=<catboost.core.CatBoostClassifier object at 0x000001D82D5DB610>, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82D609720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'random_strength': 0.7, 'n_estimators': 290, 'l2_leaf_reg': 4, 'eta': 0.1, 'depth': 8})
2024-02-09 04:57:29,954:INFO:Checking exceptions
2024-02-09 04:57:29,954:INFO:Importing libraries
2024-02-09 04:57:29,954:INFO:Copying training dataset
2024-02-09 04:57:30,092:INFO:Defining folds
2024-02-09 04:57:30,092:INFO:Declaring metric variables
2024-02-09 04:57:30,100:INFO:Importing untrained model
2024-02-09 04:57:30,101:INFO:Declaring custom model
2024-02-09 04:57:30,109:INFO:CatBoost Classifier Imported successfully
2024-02-09 04:57:30,121:INFO:Starting cross validation
2024-02-09 04:57:30,127:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 05:04:53,554:INFO:Calculating mean and std
2024-02-09 05:04:53,557:INFO:Creating metrics dataframe
2024-02-09 05:04:53,567:INFO:Finalizing model
2024-02-09 05:05:20,924:INFO:Uploading results into container
2024-02-09 05:05:20,925:INFO:Uploading model into container now
2024-02-09 05:05:20,927:INFO:_master_model_container: 31
2024-02-09 05:05:20,927:INFO:_display_container: 3
2024-02-09 05:05:20,927:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82D3EA2F0>
2024-02-09 05:05:20,927:INFO:create_model() successfully completed......................................
2024-02-09 05:05:21,024:INFO:SubProcess create_model() end ==================================
2024-02-09 05:05:21,024:INFO:choose_better activated
2024-02-09 05:05:21,028:INFO:SubProcess create_model() called ==================================
2024-02-09 05:05:21,028:INFO:Initializing create_model()
2024-02-09 05:05:21,029:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=<catboost.core.CatBoostClassifier object at 0x000001D82DE76A40>, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 05:05:21,029:INFO:Checking exceptions
2024-02-09 05:05:21,031:INFO:Importing libraries
2024-02-09 05:05:21,031:INFO:Copying training dataset
2024-02-09 05:05:21,131:INFO:Defining folds
2024-02-09 05:05:21,131:INFO:Declaring metric variables
2024-02-09 05:05:21,131:INFO:Importing untrained model
2024-02-09 05:05:21,131:INFO:Declaring custom model
2024-02-09 05:05:21,132:INFO:CatBoost Classifier Imported successfully
2024-02-09 05:05:21,132:INFO:Starting cross validation
2024-02-09 05:05:21,134:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 05:16:48,545:INFO:Calculating mean and std
2024-02-09 05:16:48,546:INFO:Creating metrics dataframe
2024-02-09 05:16:48,548:INFO:Finalizing model
2024-02-09 05:17:33,202:INFO:Uploading results into container
2024-02-09 05:17:33,203:INFO:Uploading model into container now
2024-02-09 05:17:33,203:INFO:_master_model_container: 32
2024-02-09 05:17:33,204:INFO:_display_container: 4
2024-02-09 05:17:33,204:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82DE74C40>
2024-02-09 05:17:33,204:INFO:create_model() successfully completed......................................
2024-02-09 05:17:33,291:INFO:SubProcess create_model() end ==================================
2024-02-09 05:17:33,292:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82DE74C40> result for Accuracy is 0.942
2024-02-09 05:17:33,292:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82D3EA2F0> result for Accuracy is 0.9409
2024-02-09 05:17:33,292:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82DE74C40> is best model
2024-02-09 05:17:33,292:INFO:choose_better completed
2024-02-09 05:17:33,292:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-02-09 05:17:33,309:INFO:_master_model_container: 32
2024-02-09 05:17:33,309:INFO:_display_container: 3
2024-02-09 05:17:33,309:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82DE74C40>
2024-02-09 05:17:33,310:INFO:tune_model() successfully completed......................................
2024-02-09 05:17:33,399:INFO:Initializing tune_model()
2024-02-09 05:17:33,399:INFO:tune_model(estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6467, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>)
2024-02-09 05:17:33,399:INFO:Checking exceptions
2024-02-09 05:17:33,463:INFO:Copying training dataset
2024-02-09 05:17:33,539:INFO:Checking base model
2024-02-09 05:17:33,539:INFO:Base model : Light Gradient Boosting Machine
2024-02-09 05:17:33,543:INFO:Declaring metric variables
2024-02-09 05:17:33,547:INFO:Defining Hyperparameters
2024-02-09 05:17:33,623:INFO:Tuning with n_jobs=-1
2024-02-09 05:17:33,623:INFO:Initializing RandomizedSearchCV
2024-02-09 05:26:31,655:INFO:best_params: {'actual_estimator__reg_lambda': 0.0001, 'actual_estimator__reg_alpha': 0.3, 'actual_estimator__num_leaves': 80, 'actual_estimator__n_estimators': 180, 'actual_estimator__min_split_gain': 0.9, 'actual_estimator__min_child_samples': 41, 'actual_estimator__learning_rate': 0.2, 'actual_estimator__feature_fraction': 0.9, 'actual_estimator__bagging_freq': 4, 'actual_estimator__bagging_fraction': 0.6}
2024-02-09 05:26:31,657:INFO:Hyperparameter search completed
2024-02-09 05:26:31,657:INFO:SubProcess create_model() called ==================================
2024-02-09 05:26:31,659:INFO:Initializing create_model()
2024-02-09 05:26:31,659:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6467, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82D5EDA80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 0.0001, 'reg_alpha': 0.3, 'num_leaves': 80, 'n_estimators': 180, 'min_split_gain': 0.9, 'min_child_samples': 41, 'learning_rate': 0.2, 'feature_fraction': 0.9, 'bagging_freq': 4, 'bagging_fraction': 0.6})
2024-02-09 05:26:31,659:INFO:Checking exceptions
2024-02-09 05:26:31,660:INFO:Importing libraries
2024-02-09 05:26:31,661:INFO:Copying training dataset
2024-02-09 05:26:31,853:INFO:Defining folds
2024-02-09 05:26:31,853:INFO:Declaring metric variables
2024-02-09 05:26:31,860:INFO:Importing untrained model
2024-02-09 05:26:31,861:INFO:Declaring custom model
2024-02-09 05:26:31,872:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-09 05:26:31,890:INFO:Starting cross validation
2024-02-09 05:26:31,896:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 05:27:59,532:INFO:Calculating mean and std
2024-02-09 05:27:59,534:INFO:Creating metrics dataframe
2024-02-09 05:27:59,545:INFO:Finalizing model
2024-02-09 05:28:00,449:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2024-02-09 05:28:00,449:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-02-09 05:28:00,449:INFO:[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4
2024-02-09 05:28:00,971:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2024-02-09 05:28:00,972:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-02-09 05:28:00,972:INFO:[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4
2024-02-09 05:28:00,973:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-09 05:28:01,064:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069799 seconds.
2024-02-09 05:28:01,064:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-09 05:28:01,066:INFO:[LightGBM] [Info] Total Bins 32382
2024-02-09 05:28:01,073:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 127
2024-02-09 05:28:01,080:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-09 05:28:03,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,257:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,288:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,306:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,314:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,354:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,382:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,387:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,399:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,413:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,437:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,459:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,465:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,489:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,515:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,522:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,527:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,531:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,540:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,546:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,551:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,556:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,566:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,571:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,576:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,634:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,660:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,664:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,786:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,804:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,826:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,832:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,837:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 05:28:03,837:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 05:28:03,909:INFO:Uploading results into container
2024-02-09 05:28:03,912:INFO:Uploading model into container now
2024-02-09 05:28:03,913:INFO:_master_model_container: 33
2024-02-09 05:28:03,913:INFO:_display_container: 4
2024-02-09 05:28:03,914:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=4, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=180, n_jobs=-1, num_leaves=80, objective=None,
               random_state=6467, reg_alpha=0.3, reg_lambda=0.0001,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2024-02-09 05:28:03,915:INFO:create_model() successfully completed......................................
2024-02-09 05:28:04,020:INFO:SubProcess create_model() end ==================================
2024-02-09 05:28:04,021:INFO:choose_better activated
2024-02-09 05:28:04,025:INFO:SubProcess create_model() called ==================================
2024-02-09 05:28:04,026:INFO:Initializing create_model()
2024-02-09 05:28:04,026:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6467, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 05:28:04,026:INFO:Checking exceptions
2024-02-09 05:28:04,028:INFO:Importing libraries
2024-02-09 05:28:04,028:INFO:Copying training dataset
2024-02-09 05:28:04,132:INFO:Defining folds
2024-02-09 05:28:04,132:INFO:Declaring metric variables
2024-02-09 05:28:04,132:INFO:Importing untrained model
2024-02-09 05:28:04,133:INFO:Declaring custom model
2024-02-09 05:28:04,133:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-09 05:28:04,134:INFO:Starting cross validation
2024-02-09 05:28:04,136:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 05:29:03,149:INFO:Calculating mean and std
2024-02-09 05:29:03,149:INFO:Creating metrics dataframe
2024-02-09 05:29:03,153:INFO:Finalizing model
2024-02-09 05:29:04,664:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-09 05:29:04,778:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088742 seconds.
2024-02-09 05:29:04,778:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-09 05:29:04,779:INFO:[LightGBM] [Info] Total Bins 32382
2024-02-09 05:29:04,782:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 127
2024-02-09 05:29:04,784:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-09 05:29:08,654:INFO:Uploading results into container
2024-02-09 05:29:08,655:INFO:Uploading model into container now
2024-02-09 05:29:08,657:INFO:_master_model_container: 34
2024-02-09 05:29:08,657:INFO:_display_container: 5
2024-02-09 05:29:08,657:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6467, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-02-09 05:29:08,658:INFO:create_model() successfully completed......................................
2024-02-09 05:29:08,783:INFO:SubProcess create_model() end ==================================
2024-02-09 05:29:08,784:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6467, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.9391
2024-02-09 05:29:08,785:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=4, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=180, n_jobs=-1, num_leaves=80, objective=None,
               random_state=6467, reg_alpha=0.3, reg_lambda=0.0001,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.9393
2024-02-09 05:29:08,786:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=4, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=180, n_jobs=-1, num_leaves=80, objective=None,
               random_state=6467, reg_alpha=0.3, reg_lambda=0.0001,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) is best model
2024-02-09 05:29:08,786:INFO:choose_better completed
2024-02-09 05:29:08,815:INFO:_master_model_container: 34
2024-02-09 05:29:08,815:INFO:_display_container: 4
2024-02-09 05:29:08,816:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=4, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=180, n_jobs=-1, num_leaves=80, objective=None,
               random_state=6467, reg_alpha=0.3, reg_lambda=0.0001,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2024-02-09 05:29:08,817:INFO:tune_model() successfully completed......................................
2024-02-09 05:29:08,970:INFO:Initializing ensemble_model()
2024-02-09 05:29:08,970:INFO:ensemble_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=<catboost.core.CatBoostClassifier object at 0x000001D82DE74C40>, method=Bagging, fold=None, n_estimators=10, round=4, choose_better=True, optimize=Accuracy, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2024-02-09 05:29:08,971:INFO:Checking exceptions
2024-02-09 05:29:09,070:INFO:Importing libraries
2024-02-09 05:29:09,070:INFO:Copying training dataset
2024-02-09 05:29:09,070:INFO:Checking base model
2024-02-09 05:29:09,071:INFO:Base model : CatBoost Classifier
2024-02-09 05:29:09,088:INFO:Importing untrained ensembler
2024-02-09 05:29:09,088:INFO:Ensemble method set to Bagging
2024-02-09 05:29:09,089:INFO:SubProcess create_model() called ==================================
2024-02-09 05:29:09,090:INFO:Initializing create_model()
2024-02-09 05:29:09,091:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=BaggingClassifier(base_estimator='deprecated', bootstrap=True,
                  bootstrap_features=False,
                  estimator=<catboost.core.CatBoostClassifier object at 0x000001D82DE74C40>,
                  max_features=1.0, max_samples=1.0, n_estimators=10,
                  n_jobs=None, oob_score=False, random_state=6467, verbose=0,
                  warm_start=False), fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82D5C6290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 05:29:09,091:INFO:Checking exceptions
2024-02-09 05:29:09,091:INFO:Importing libraries
2024-02-09 05:29:09,091:INFO:Copying training dataset
2024-02-09 05:29:09,268:INFO:Defining folds
2024-02-09 05:29:09,268:INFO:Declaring metric variables
2024-02-09 05:29:09,273:INFO:Importing untrained model
2024-02-09 05:29:09,274:INFO:Declaring custom model
2024-02-09 05:29:09,283:INFO:Bagging Classifier Imported successfully
2024-02-09 05:29:09,297:INFO:Starting cross validation
2024-02-09 05:29:09,302:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 07:08:41,849:INFO:Calculating mean and std
2024-02-09 07:08:41,851:INFO:Creating metrics dataframe
2024-02-09 07:08:41,860:INFO:Finalizing model
2024-02-09 07:15:06,780:INFO:Uploading results into container
2024-02-09 07:15:06,782:INFO:Uploading model into container now
2024-02-09 07:15:06,783:INFO:_master_model_container: 35
2024-02-09 07:15:06,783:INFO:_display_container: 4
2024-02-09 07:15:06,784:INFO:BaggingClassifier(base_estimator='deprecated', bootstrap=True,
                  bootstrap_features=False,
                  estimator=<catboost.core.CatBoostClassifier object at 0x000001D82DEF4DC0>,
                  max_features=1.0, max_samples=1.0, n_estimators=10,
                  n_jobs=None, oob_score=False, random_state=6467, verbose=0,
                  warm_start=False)
2024-02-09 07:15:06,785:INFO:create_model() successfully completed......................................
2024-02-09 07:15:06,887:INFO:SubProcess create_model() end ==================================
2024-02-09 07:15:06,887:INFO:choose_better activated
2024-02-09 07:15:06,893:INFO:SubProcess create_model() called ==================================
2024-02-09 07:15:06,893:INFO:Initializing create_model()
2024-02-09 07:15:06,893:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=<catboost.core.CatBoostClassifier object at 0x000001D82DE74C40>, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 07:15:06,893:INFO:Checking exceptions
2024-02-09 07:15:06,896:INFO:Importing libraries
2024-02-09 07:15:06,896:INFO:Copying training dataset
2024-02-09 07:15:06,997:INFO:Defining folds
2024-02-09 07:15:06,997:INFO:Declaring metric variables
2024-02-09 07:15:06,998:INFO:Importing untrained model
2024-02-09 07:15:06,998:INFO:Declaring custom model
2024-02-09 07:15:06,998:INFO:CatBoost Classifier Imported successfully
2024-02-09 07:15:06,998:INFO:Starting cross validation
2024-02-09 07:15:07,001:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 07:26:29,232:INFO:Calculating mean and std
2024-02-09 07:26:29,232:INFO:Creating metrics dataframe
2024-02-09 07:26:29,235:INFO:Finalizing model
2024-02-09 07:27:12,761:INFO:Uploading results into container
2024-02-09 07:27:12,762:INFO:Uploading model into container now
2024-02-09 07:27:12,762:INFO:_master_model_container: 36
2024-02-09 07:27:12,762:INFO:_display_container: 5
2024-02-09 07:27:12,762:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82D5EC100>
2024-02-09 07:27:12,762:INFO:create_model() successfully completed......................................
2024-02-09 07:27:12,842:INFO:SubProcess create_model() end ==================================
2024-02-09 07:27:12,842:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82D5EC100> result for Accuracy is 0.9421
2024-02-09 07:27:12,843:INFO:BaggingClassifier(base_estimator='deprecated', bootstrap=True,
                  bootstrap_features=False,
                  estimator=<catboost.core.CatBoostClassifier object at 0x000001D82DEF4DC0>,
                  max_features=1.0, max_samples=1.0, n_estimators=10,
                  n_jobs=None, oob_score=False, random_state=6467, verbose=0,
                  warm_start=False) result for Accuracy is 0.942
2024-02-09 07:27:12,843:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82D5EC100> is best model
2024-02-09 07:27:12,843:INFO:choose_better completed
2024-02-09 07:27:12,843:INFO:Original model was better than the ensembled model, hence it will be returned. NOTE: The display metrics are for the ensembled model (not the original one).
2024-02-09 07:27:12,861:INFO:_master_model_container: 36
2024-02-09 07:27:12,861:INFO:_display_container: 4
2024-02-09 07:27:12,862:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82D5EC100>
2024-02-09 07:27:12,862:INFO:ensemble_model() successfully completed......................................
2024-02-09 07:27:12,954:INFO:Initializing ensemble_model()
2024-02-09 07:27:12,954:INFO:ensemble_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=LGBMClassifier(bagging_fraction=0.6, bagging_freq=4, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=180, n_jobs=-1, num_leaves=80, objective=None,
               random_state=6467, reg_alpha=0.3, reg_lambda=0.0001,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), method=Bagging, fold=None, n_estimators=10, round=4, choose_better=True, optimize=Accuracy, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2024-02-09 07:27:12,954:INFO:Checking exceptions
2024-02-09 07:27:13,012:INFO:Importing libraries
2024-02-09 07:27:13,012:INFO:Copying training dataset
2024-02-09 07:27:13,012:INFO:Checking base model
2024-02-09 07:27:13,013:INFO:Base model : Light Gradient Boosting Machine
2024-02-09 07:27:13,023:INFO:Importing untrained ensembler
2024-02-09 07:27:13,024:INFO:Ensemble method set to Bagging
2024-02-09 07:27:13,024:INFO:SubProcess create_model() called ==================================
2024-02-09 07:27:13,025:INFO:Initializing create_model()
2024-02-09 07:27:13,026:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=BaggingClassifier(base_estimator='deprecated', bootstrap=True,
                  bootstrap_features=False,
                  estimator=LGBMClassifier(bagging_fraction=0.6, bagging_freq=4,
                                           boosting_type='gbdt',
                                           class_weight=None,
                                           colsample_bytree=1.0,
                                           feature_fraction=0.9,
                                           importance_type='split',
                                           learning_rate=0.2, max_depth=-1,
                                           min_child_samples=41,
                                           min_child_weight=0.001,
                                           min_split_gain=0.9, n_estimators=180,
                                           n_jobs=-1, num_leaves=80,
                                           objective=None, random_state=6467,
                                           reg_alpha=0.3, reg_lambda=0.0001,
                                           subsample=1.0,
                                           subsample_for_bin=200000,
                                           subsample_freq=0),
                  max_features=1.0, max_samples=1.0, n_estimators=10,
                  n_jobs=None, oob_score=False, random_state=6467, verbose=0,
                  warm_start=False), fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82D5ED480>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 07:27:13,026:INFO:Checking exceptions
2024-02-09 07:27:13,026:INFO:Importing libraries
2024-02-09 07:27:13,026:INFO:Copying training dataset
2024-02-09 07:27:13,144:INFO:Defining folds
2024-02-09 07:27:13,144:INFO:Declaring metric variables
2024-02-09 07:27:13,148:INFO:Importing untrained model
2024-02-09 07:27:13,148:INFO:Declaring custom model
2024-02-09 07:27:13,154:INFO:Bagging Classifier Imported successfully
2024-02-09 07:27:13,161:INFO:Starting cross validation
2024-02-09 07:27:13,165:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 07:38:22,699:INFO:Calculating mean and std
2024-02-09 07:38:22,701:INFO:Creating metrics dataframe
2024-02-09 07:38:22,709:INFO:Finalizing model
2024-02-09 07:38:23,560:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2024-02-09 07:38:23,560:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-02-09 07:38:23,560:INFO:[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4
2024-02-09 07:38:24,033:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2024-02-09 07:38:24,033:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-02-09 07:38:24,034:INFO:[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4
2024-02-09 07:38:24,034:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-09 07:38:24,113:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061337 seconds.
2024-02-09 07:38:24,113:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-09 07:38:24,115:INFO:[LightGBM] [Info] Total Bins 32382
2024-02-09 07:38:24,120:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 127
2024-02-09 07:38:24,125:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500863 -> initscore=0.003454
2024-02-09 07:38:24,125:INFO:[LightGBM] [Info] Start training from score 0.003454
2024-02-09 07:38:25,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:25,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:25,986:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,050:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,068:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,091:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,101:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,107:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,113:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,177:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,206:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,211:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,222:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,228:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,234:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,239:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,248:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,254:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,263:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,279:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,284:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,289:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,308:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,355:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,415:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,427:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,478:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,490:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,521:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,526:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,531:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,535:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,544:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,549:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,554:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,557:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,565:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,570:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,578:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,587:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:26,678:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2024-02-09 07:38:26,678:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-02-09 07:38:26,678:INFO:[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4
2024-02-09 07:38:27,162:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2024-02-09 07:38:27,162:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-02-09 07:38:27,163:INFO:[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4
2024-02-09 07:38:27,163:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-09 07:38:27,231:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054181 seconds.
2024-02-09 07:38:27,231:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-09 07:38:27,233:INFO:[LightGBM] [Info] Total Bins 32382
2024-02-09 07:38:27,237:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 127
2024-02-09 07:38:27,244:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500159 -> initscore=0.000636
2024-02-09 07:38:27,244:INFO:[LightGBM] [Info] Start training from score 0.000636
2024-02-09 07:38:28,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:28,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:28,970:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:28,981:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:28,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,061:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,068:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,098:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,115:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,125:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,184:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,205:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,218:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,222:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,244:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,251:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,256:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,261:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,272:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,279:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,310:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,339:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,355:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,358:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,361:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,361:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,363:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,366:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,368:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,371:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,371:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,374:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,377:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,377:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,379:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,382:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,382:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,385:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,385:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,387:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,387:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,390:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,393:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,395:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,398:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,401:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,403:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,406:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,409:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,411:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,411:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,414:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,414:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,416:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,416:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,419:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,422:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,424:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,427:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,427:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,429:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,432:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,436:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,441:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,447:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,452:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,455:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,460:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,464:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,469:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,473:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,477:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,480:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,480:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,483:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:29,483:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:29,559:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2024-02-09 07:38:29,559:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-02-09 07:38:29,559:INFO:[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4
2024-02-09 07:38:30,080:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2024-02-09 07:38:30,081:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-02-09 07:38:30,081:INFO:[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4
2024-02-09 07:38:30,081:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-09 07:38:30,179:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.077839 seconds.
2024-02-09 07:38:30,179:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-09 07:38:30,181:INFO:[LightGBM] [Info] Total Bins 32382
2024-02-09 07:38:30,185:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 127
2024-02-09 07:38:30,191:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498557 -> initscore=-0.005772
2024-02-09 07:38:30,191:INFO:[LightGBM] [Info] Start training from score -0.005772
2024-02-09 07:38:32,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:32,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:32,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:32,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:32,804:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:32,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:32,832:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:32,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:32,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:32,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:32,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:32,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:32,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:32,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:32,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:32,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:32,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:32,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:32,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,021:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,065:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,100:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,145:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,177:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,184:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,196:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,205:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,238:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,245:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,252:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,263:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,271:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,284:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,310:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,379:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,383:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,399:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,411:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,414:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,414:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,417:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,418:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,421:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,425:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,428:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,429:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,432:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,436:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,440:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,443:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,447:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,450:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,454:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,458:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,461:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,465:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,466:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,470:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,474:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,478:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,478:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,481:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,485:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,488:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,491:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,495:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,498:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,498:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,502:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,505:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:33,509:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:33,583:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2024-02-09 07:38:33,584:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-02-09 07:38:33,584:INFO:[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4
2024-02-09 07:38:34,129:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2024-02-09 07:38:34,130:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-02-09 07:38:34,130:INFO:[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4
2024-02-09 07:38:34,130:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-09 07:38:34,224:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.078495 seconds.
2024-02-09 07:38:34,224:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-09 07:38:34,225:INFO:[LightGBM] [Info] Total Bins 32382
2024-02-09 07:38:34,230:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 127
2024-02-09 07:38:34,235:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500716 -> initscore=0.002863
2024-02-09 07:38:34,235:INFO:[LightGBM] [Info] Start training from score 0.002863
2024-02-09 07:38:36,537:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:36,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:36,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:36,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:36,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:36,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:36,751:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:36,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:36,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:36,787:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:36,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:36,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:36,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:36,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:36,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:36,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:36,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:36,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:36,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:36,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:36,906:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:36,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:36,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:36,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:36,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:36,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:36,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:36,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:36,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,021:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,071:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,078:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,133:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,171:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,195:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,206:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,220:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,236:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,241:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,252:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,262:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,267:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,277:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,302:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:37,306:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,306:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:37,310:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,310:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:37,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,315:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:37,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,319:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:37,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,324:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:37,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,329:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:37,333:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,333:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:37,337:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,337:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:37,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,342:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:37,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,345:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:37,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,350:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:37,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,354:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:37,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,358:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:37,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,362:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:37,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,366:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:37,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,371:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:37,376:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,376:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:37,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,380:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:37,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,384:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:37,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,388:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:37,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,392:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:37,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,396:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:37,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,400:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:37,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:37,405:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:37,478:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2024-02-09 07:38:37,479:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-02-09 07:38:37,479:INFO:[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4
2024-02-09 07:38:37,968:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2024-02-09 07:38:37,968:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-02-09 07:38:37,968:INFO:[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4
2024-02-09 07:38:37,968:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-09 07:38:38,050:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063756 seconds.
2024-02-09 07:38:38,051:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-09 07:38:38,052:INFO:[LightGBM] [Info] Total Bins 32382
2024-02-09 07:38:38,059:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 127
2024-02-09 07:38:38,065:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501568 -> initscore=0.006272
2024-02-09 07:38:38,066:INFO:[LightGBM] [Info] Start training from score 0.006272
2024-02-09 07:38:40,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,100:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,116:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,188:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,199:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,206:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,242:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,247:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,258:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,263:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,267:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,272:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,282:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,289:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,299:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,308:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,337:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,343:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,415:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,490:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,516:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,519:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,523:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,531:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,537:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,540:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,544:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,551:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,555:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,559:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,571:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,579:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,582:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,633:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,643:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,665:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:40,736:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2024-02-09 07:38:40,736:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-02-09 07:38:40,736:INFO:[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4
2024-02-09 07:38:41,209:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2024-02-09 07:38:41,209:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-02-09 07:38:41,209:INFO:[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4
2024-02-09 07:38:41,209:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-09 07:38:41,280:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058628 seconds.
2024-02-09 07:38:41,281:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-09 07:38:41,282:INFO:[LightGBM] [Info] Total Bins 32382
2024-02-09 07:38:41,285:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 127
2024-02-09 07:38:41,289:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502409 -> initscore=0.009635
2024-02-09 07:38:41,290:INFO:[LightGBM] [Info] Start training from score 0.009635
2024-02-09 07:38:42,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:42,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:42,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:42,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:42,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,068:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,102:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,156:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,163:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,193:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,204:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,239:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,255:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,260:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,268:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,274:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,282:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,314:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,354:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,414:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,437:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,480:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,489:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,515:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,520:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,524:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,536:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,540:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,544:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,547:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,547:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:43,550:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,550:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:43,552:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,553:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:43,555:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,555:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:43,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,558:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:43,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,560:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:43,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,563:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:43,566:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,566:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:43,570:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:43,570:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:43,642:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2024-02-09 07:38:43,642:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-02-09 07:38:43,642:INFO:[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4
2024-02-09 07:38:44,113:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2024-02-09 07:38:44,114:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-02-09 07:38:44,114:INFO:[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4
2024-02-09 07:38:44,114:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-09 07:38:44,193:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059786 seconds.
2024-02-09 07:38:44,193:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-09 07:38:44,195:INFO:[LightGBM] [Info] Total Bins 32382
2024-02-09 07:38:44,200:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 127
2024-02-09 07:38:44,205:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500443 -> initscore=0.001772
2024-02-09 07:38:44,205:INFO:[LightGBM] [Info] Start training from score 0.001772
2024-02-09 07:38:46,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,585:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,643:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,771:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,936:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,944:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,984:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:46,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,086:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,094:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,115:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,125:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,199:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,205:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,251:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,266:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,272:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,284:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,323:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,389:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:47,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,393:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:47,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,397:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:47,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,402:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:47,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,406:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:47,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,410:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:47,414:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,415:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:47,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,419:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:47,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,424:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:47,427:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,427:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:47,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,432:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:47,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,436:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:47,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:47,440:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:47,517:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2024-02-09 07:38:47,518:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-02-09 07:38:47,518:INFO:[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4
2024-02-09 07:38:48,041:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2024-02-09 07:38:48,042:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-02-09 07:38:48,042:INFO:[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4
2024-02-09 07:38:48,042:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-09 07:38:48,138:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.077237 seconds.
2024-02-09 07:38:48,138:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-09 07:38:48,139:INFO:[LightGBM] [Info] Total Bins 32382
2024-02-09 07:38:48,142:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 127
2024-02-09 07:38:48,147:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501795 -> initscore=0.007181
2024-02-09 07:38:48,147:INFO:[LightGBM] [Info] Start training from score 0.007181
2024-02-09 07:38:50,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,647:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,772:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,832:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,852:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,930:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:50,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,016:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,036:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,052:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,070:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,086:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,115:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,184:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,201:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,209:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,236:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,242:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,249:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,256:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,267:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,308:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,335:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:51,339:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,356:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,395:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:51,399:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,414:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:51,516:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2024-02-09 07:38:51,517:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-02-09 07:38:51,517:INFO:[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4
2024-02-09 07:38:52,019:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2024-02-09 07:38:52,019:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-02-09 07:38:52,019:INFO:[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4
2024-02-09 07:38:52,020:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-09 07:38:52,122:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.083169 seconds.
2024-02-09 07:38:52,123:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-09 07:38:52,124:INFO:[LightGBM] [Info] Total Bins 32382
2024-02-09 07:38:52,128:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 127
2024-02-09 07:38:52,133:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500579 -> initscore=0.002318
2024-02-09 07:38:52,133:INFO:[LightGBM] [Info] Start training from score 0.002318
2024-02-09 07:38:54,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,236:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,256:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,265:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,271:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,277:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,359:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,437:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,482:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,487:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,523:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,534:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,539:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,548:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,554:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,557:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,561:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,569:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,573:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,578:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,582:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,627:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,667:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,672:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,676:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,742:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,747:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,751:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,756:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,766:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,772:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,820:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,824:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:54,911:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2024-02-09 07:38:54,911:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-02-09 07:38:54,911:INFO:[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4
2024-02-09 07:38:55,385:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2024-02-09 07:38:55,386:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-02-09 07:38:55,386:INFO:[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4
2024-02-09 07:38:55,386:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-09 07:38:55,458:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055792 seconds.
2024-02-09 07:38:55,458:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-09 07:38:55,459:INFO:[LightGBM] [Info] Total Bins 32382
2024-02-09 07:38:55,461:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 127
2024-02-09 07:38:55,464:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499841 -> initscore=-0.000636
2024-02-09 07:38:55,465:INFO:[LightGBM] [Info] Start training from score -0.000636
2024-02-09 07:38:57,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,186:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,193:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,224:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,241:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,247:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,252:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,264:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,283:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,323:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,341:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,365:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,376:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,413:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,470:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,490:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,511:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,517:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,522:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,526:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,534:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,540:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,544:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,548:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,556:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,561:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,565:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,569:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,579:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,585:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,624:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,629:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,634:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,637:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,677:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,701:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:57,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,704:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:57,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,706:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:57,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,709:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:57,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,712:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:57,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,714:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:57,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,717:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:57,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,719:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:57,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,722:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:57,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,725:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:57,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,727:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:57,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,730:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:57,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:38:57,733:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:38:57,773:INFO:Uploading results into container
2024-02-09 07:38:57,775:INFO:Uploading model into container now
2024-02-09 07:38:57,777:INFO:_master_model_container: 37
2024-02-09 07:38:57,777:INFO:_display_container: 5
2024-02-09 07:38:57,782:INFO:BaggingClassifier(base_estimator='deprecated', bootstrap=True,
                  bootstrap_features=False,
                  estimator=LGBMClassifier(bagging_fraction=0.6, bagging_freq=4,
                                           boosting_type='gbdt',
                                           class_weight=None,
                                           colsample_bytree=1.0,
                                           feature_fraction=0.9,
                                           importance_type='split',
                                           learning_rate=0.2, max_depth=-1,
                                           min_child_samples=41,
                                           min_child_weight=0.001,
                                           min_split_gain=0.9, n_estimators=180,
                                           n_jobs=-1, num_leaves=80,
                                           objective=None, random_state=6467,
                                           reg_alpha=0.3, reg_lambda=0.0001,
                                           subsample=1.0,
                                           subsample_for_bin=200000,
                                           subsample_freq=0),
                  max_features=1.0, max_samples=1.0, n_estimators=10,
                  n_jobs=None, oob_score=False, random_state=6467, verbose=0,
                  warm_start=False)
2024-02-09 07:38:57,782:INFO:create_model() successfully completed......................................
2024-02-09 07:38:57,901:INFO:SubProcess create_model() end ==================================
2024-02-09 07:38:57,902:INFO:choose_better activated
2024-02-09 07:38:57,906:INFO:SubProcess create_model() called ==================================
2024-02-09 07:38:57,906:INFO:Initializing create_model()
2024-02-09 07:38:57,907:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=LGBMClassifier(bagging_fraction=0.6, bagging_freq=4, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=180, n_jobs=-1, num_leaves=80, objective=None,
               random_state=6467, reg_alpha=0.3, reg_lambda=0.0001,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 07:38:57,907:INFO:Checking exceptions
2024-02-09 07:38:57,910:INFO:Importing libraries
2024-02-09 07:38:57,910:INFO:Copying training dataset
2024-02-09 07:38:58,016:INFO:Defining folds
2024-02-09 07:38:58,016:INFO:Declaring metric variables
2024-02-09 07:38:58,016:INFO:Importing untrained model
2024-02-09 07:38:58,016:INFO:Declaring custom model
2024-02-09 07:38:58,017:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-09 07:38:58,017:INFO:Starting cross validation
2024-02-09 07:38:58,020:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 07:40:22,452:INFO:Calculating mean and std
2024-02-09 07:40:22,453:INFO:Creating metrics dataframe
2024-02-09 07:40:22,455:INFO:Finalizing model
2024-02-09 07:40:23,311:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2024-02-09 07:40:23,311:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-02-09 07:40:23,311:INFO:[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4
2024-02-09 07:40:23,788:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2024-02-09 07:40:23,788:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-02-09 07:40:23,788:INFO:[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4
2024-02-09 07:40:23,789:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-09 07:40:23,872:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064938 seconds.
2024-02-09 07:40:23,872:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-09 07:40:23,874:INFO:[LightGBM] [Info] Total Bins 32382
2024-02-09 07:40:23,879:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 127
2024-02-09 07:40:23,884:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-09 07:40:25,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:25,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:25,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:25,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:25,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:25,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:25,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,021:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,060:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,066:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,098:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,127:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,131:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,186:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,201:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,207:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,211:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,215:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,224:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,234:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,239:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,254:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,264:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,279:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,289:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,299:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,333:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,371:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,376:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,387:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,412:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,437:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,478:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,482:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,489:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,498:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-02-09 07:40:26,501:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-02-09 07:40:26,551:INFO:Uploading results into container
2024-02-09 07:40:26,552:INFO:Uploading model into container now
2024-02-09 07:40:26,552:INFO:_master_model_container: 38
2024-02-09 07:40:26,552:INFO:_display_container: 6
2024-02-09 07:40:26,553:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=4, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=180, n_jobs=-1, num_leaves=80, objective=None,
               random_state=6467, reg_alpha=0.3, reg_lambda=0.0001,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2024-02-09 07:40:26,553:INFO:create_model() successfully completed......................................
2024-02-09 07:40:26,650:INFO:SubProcess create_model() end ==================================
2024-02-09 07:40:26,651:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=4, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=41, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=180, n_jobs=-1, num_leaves=80, objective=None,
               random_state=6467, reg_alpha=0.3, reg_lambda=0.0001,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.9394
2024-02-09 07:40:26,652:INFO:BaggingClassifier(base_estimator='deprecated', bootstrap=True,
                  bootstrap_features=False,
                  estimator=LGBMClassifier(bagging_fraction=0.6, bagging_freq=4,
                                           boosting_type='gbdt',
                                           class_weight=None,
                                           colsample_bytree=1.0,
                                           feature_fraction=0.9,
                                           importance_type='split',
                                           learning_rate=0.2, max_depth=-1,
                                           min_child_samples=41,
                                           min_child_weight=0.001,
                                           min_split_gain=0.9, n_estimators=180,
                                           n_jobs=-1, num_leaves=80,
                                           objective=None, random_state=6467,
                                           reg_alpha=0.3, reg_lambda=0.0001,
                                           subsample=1.0,
                                           subsample_for_bin=200000,
                                           subsample_freq=0),
                  max_features=1.0, max_samples=1.0, n_estimators=10,
                  n_jobs=None, oob_score=False, random_state=6467, verbose=0,
                  warm_start=False) result for Accuracy is 0.9414
2024-02-09 07:40:26,654:INFO:BaggingClassifier(base_estimator='deprecated', bootstrap=True,
                  bootstrap_features=False,
                  estimator=LGBMClassifier(bagging_fraction=0.6, bagging_freq=4,
                                           boosting_type='gbdt',
                                           class_weight=None,
                                           colsample_bytree=1.0,
                                           feature_fraction=0.9,
                                           importance_type='split',
                                           learning_rate=0.2, max_depth=-1,
                                           min_child_samples=41,
                                           min_child_weight=0.001,
                                           min_split_gain=0.9, n_estimators=180,
                                           n_jobs=-1, num_leaves=80,
                                           objective=None, random_state=6467,
                                           reg_alpha=0.3, reg_lambda=0.0001,
                                           subsample=1.0,
                                           subsample_for_bin=200000,
                                           subsample_freq=0),
                  max_features=1.0, max_samples=1.0, n_estimators=10,
                  n_jobs=None, oob_score=False, random_state=6467, verbose=0,
                  warm_start=False) is best model
2024-02-09 07:40:26,654:INFO:choose_better completed
2024-02-09 07:40:26,670:INFO:_master_model_container: 38
2024-02-09 07:40:26,670:INFO:_display_container: 5
2024-02-09 07:40:26,672:INFO:BaggingClassifier(base_estimator='deprecated', bootstrap=True,
                  bootstrap_features=False,
                  estimator=LGBMClassifier(bagging_fraction=0.6, bagging_freq=4,
                                           boosting_type='gbdt',
                                           class_weight=None,
                                           colsample_bytree=1.0,
                                           feature_fraction=0.9,
                                           importance_type='split',
                                           learning_rate=0.2, max_depth=-1,
                                           min_child_samples=41,
                                           min_child_weight=0.001,
                                           min_split_gain=0.9, n_estimators=180,
                                           n_jobs=-1, num_leaves=80,
                                           objective=None, random_state=6467,
                                           reg_alpha=0.3, reg_lambda=0.0001,
                                           subsample=1.0,
                                           subsample_for_bin=200000,
                                           subsample_freq=0),
                  max_features=1.0, max_samples=1.0, n_estimators=10,
                  n_jobs=None, oob_score=False, random_state=6467, verbose=0,
                  warm_start=False)
2024-02-09 07:40:26,673:INFO:ensemble_model() successfully completed......................................
2024-02-09 07:40:26,788:INFO:Initializing blend_models()
2024-02-09 07:40:26,788:INFO:blend_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator_list=[<catboost.core.CatBoostClassifier object at 0x000001D82DE76A40>, LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6467, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)], fold=None, round=4, choose_better=True, optimize=Accuracy, method=auto, weights=None, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2024-02-09 07:40:26,789:INFO:Checking exceptions
2024-02-09 07:40:26,848:INFO:Importing libraries
2024-02-09 07:40:26,848:INFO:Copying training dataset
2024-02-09 07:40:26,852:INFO:Getting model names
2024-02-09 07:40:26,856:INFO:SubProcess create_model() called ==================================
2024-02-09 07:40:26,858:INFO:Initializing create_model()
2024-02-09 07:40:26,859:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=VotingClassifier(estimators=[('CatBoost Classifier',
                              <catboost.core.CatBoostClassifier object at 0x000001D82DE76A40>),
                             ('Light Gradient Boosting Machine',
                              LGBMClassifier(boosting_type='gbdt',
                                             class_weight=None,
                                             colsample_bytree=1.0,
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=6467, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82D5C62C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 07:40:26,859:INFO:Checking exceptions
2024-02-09 07:40:26,859:INFO:Importing libraries
2024-02-09 07:40:26,859:INFO:Copying training dataset
2024-02-09 07:40:26,968:INFO:Defining folds
2024-02-09 07:40:26,968:INFO:Declaring metric variables
2024-02-09 07:40:26,972:INFO:Importing untrained model
2024-02-09 07:40:26,972:INFO:Declaring custom model
2024-02-09 07:40:26,977:INFO:Voting Classifier Imported successfully
2024-02-09 07:40:26,984:INFO:Starting cross validation
2024-02-09 07:40:26,987:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 07:52:23,851:INFO:Calculating mean and std
2024-02-09 07:52:23,852:INFO:Creating metrics dataframe
2024-02-09 07:52:23,858:INFO:Finalizing model
2024-02-09 07:53:10,032:INFO:Uploading results into container
2024-02-09 07:53:10,033:INFO:Uploading model into container now
2024-02-09 07:53:10,034:INFO:_master_model_container: 39
2024-02-09 07:53:10,034:INFO:_display_container: 5
2024-02-09 07:53:10,039:INFO:VotingClassifier(estimators=[('CatBoost Classifier',
                              <catboost.core.CatBoostClassifier object at 0x000001D82DE70310>),
                             ('Light Gradient Boosting Machine',
                              LGBMClassifier(boosting_type='gbdt',
                                             class_weight=None,
                                             colsample_bytree=1.0,
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=6467, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None)
2024-02-09 07:53:10,040:INFO:create_model() successfully completed......................................
2024-02-09 07:53:10,136:INFO:SubProcess create_model() end ==================================
2024-02-09 07:53:10,136:INFO:choose_better activated
2024-02-09 07:53:10,142:INFO:VotingClassifier(estimators=[('CatBoost Classifier',
                              <catboost.core.CatBoostClassifier object at 0x000001D82DE70310>),
                             ('Light Gradient Boosting Machine',
                              LGBMClassifier(boosting_type='gbdt',
                                             class_weight=None,
                                             colsample_bytree=1.0,
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=6467, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None) result for Accuracy is 0.9414
2024-02-09 07:53:10,142:INFO:SubProcess create_model() called ==================================
2024-02-09 07:53:10,142:INFO:Initializing create_model()
2024-02-09 07:53:10,142:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=<catboost.core.CatBoostClassifier object at 0x000001D82DE76A40>, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 07:53:10,142:INFO:Checking exceptions
2024-02-09 07:53:10,144:INFO:Importing libraries
2024-02-09 07:53:10,145:INFO:Copying training dataset
2024-02-09 07:53:10,252:INFO:Defining folds
2024-02-09 07:53:10,252:INFO:Declaring metric variables
2024-02-09 07:53:10,252:INFO:Importing untrained model
2024-02-09 07:53:10,252:INFO:Declaring custom model
2024-02-09 07:53:10,253:INFO:CatBoost Classifier Imported successfully
2024-02-09 07:53:10,254:INFO:Starting cross validation
2024-02-09 07:53:10,257:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 08:04:25,295:INFO:Calculating mean and std
2024-02-09 08:04:25,296:INFO:Creating metrics dataframe
2024-02-09 08:04:25,301:INFO:Finalizing model
2024-02-09 08:05:09,160:INFO:Uploading results into container
2024-02-09 08:05:09,161:INFO:Uploading model into container now
2024-02-09 08:05:09,161:INFO:_master_model_container: 40
2024-02-09 08:05:09,161:INFO:_display_container: 6
2024-02-09 08:05:09,161:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82DA807F0>
2024-02-09 08:05:09,161:INFO:create_model() successfully completed......................................
2024-02-09 08:05:09,235:INFO:SubProcess create_model() end ==================================
2024-02-09 08:05:09,235:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82DA807F0> result for Accuracy is 0.9418
2024-02-09 08:05:09,235:INFO:SubProcess create_model() called ==================================
2024-02-09 08:05:09,235:INFO:Initializing create_model()
2024-02-09 08:05:09,236:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6467, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 08:05:09,236:INFO:Checking exceptions
2024-02-09 08:05:09,237:INFO:Importing libraries
2024-02-09 08:05:09,237:INFO:Copying training dataset
2024-02-09 08:05:09,330:INFO:Defining folds
2024-02-09 08:05:09,330:INFO:Declaring metric variables
2024-02-09 08:05:09,330:INFO:Importing untrained model
2024-02-09 08:05:09,330:INFO:Declaring custom model
2024-02-09 08:05:09,331:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-09 08:05:09,331:INFO:Starting cross validation
2024-02-09 08:05:09,334:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 08:06:09,196:INFO:Calculating mean and std
2024-02-09 08:06:09,196:INFO:Creating metrics dataframe
2024-02-09 08:06:09,201:INFO:Finalizing model
2024-02-09 08:06:10,346:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-09 08:06:10,444:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.078469 seconds.
2024-02-09 08:06:10,444:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-09 08:06:10,445:INFO:[LightGBM] [Info] Total Bins 32382
2024-02-09 08:06:10,446:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 127
2024-02-09 08:06:10,447:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-09 08:06:13,263:INFO:Uploading results into container
2024-02-09 08:06:13,264:INFO:Uploading model into container now
2024-02-09 08:06:13,265:INFO:_master_model_container: 41
2024-02-09 08:06:13,265:INFO:_display_container: 6
2024-02-09 08:06:13,266:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6467, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-02-09 08:06:13,266:INFO:create_model() successfully completed......................................
2024-02-09 08:06:13,369:INFO:SubProcess create_model() end ==================================
2024-02-09 08:06:13,370:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6467, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.9397
2024-02-09 08:06:13,370:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82DA807F0> is best model
2024-02-09 08:06:13,370:INFO:choose_better completed
2024-02-09 08:06:13,370:INFO:Original model was better than the blended model, hence it will be returned. NOTE: The display metrics are for the blended model (not the original one).
2024-02-09 08:06:13,389:INFO:_master_model_container: 41
2024-02-09 08:06:13,390:INFO:_display_container: 5
2024-02-09 08:06:13,390:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82DA807F0>
2024-02-09 08:06:13,390:INFO:blend_models() successfully completed......................................
2024-02-09 08:06:13,525:INFO:Initializing stack_models()
2024-02-09 08:06:13,526:INFO:stack_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator_list=[<catboost.core.CatBoostClassifier object at 0x000001D82DE76A40>, LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6467, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)], meta_model=None, meta_model_fold=5, fold=None, round=4, method=auto, restack=True, choose_better=True, optimize=Accuracy, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2024-02-09 08:06:13,526:INFO:Checking exceptions
2024-02-09 08:06:13,572:INFO:Defining meta model
2024-02-09 08:06:13,602:INFO:Getting model names
2024-02-09 08:06:13,603:INFO:[('CatBoost Classifier', <catboost.core.CatBoostClassifier object at 0x000001D82DE76A40>), ('Light Gradient Boosting Machine', LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6467, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0))]
2024-02-09 08:06:13,610:INFO:SubProcess create_model() called ==================================
2024-02-09 08:06:13,615:INFO:Initializing create_model()
2024-02-09 08:06:13,615:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=StackingClassifier(cv=5,
                   estimators=[('CatBoost Classifier',
                                <catboost.core.CatBoostClassifier object at 0x000001D82DE76A40>),
                               ('Light Gradient Boosting Machine',
                                LGBMClassifier(boosting_type='gbdt',
                                               class_weight=None,
                                               colsample_bytree=1.0,
                                               importance_type='split',
                                               learning_rate=0.1, max_depth=-1,
                                               min_child_samples=20,
                                               min_child_weight=0.001,
                                               min_split_gain=0.0,
                                               n_est...
                                               subsample_for_bin=200000,
                                               subsample_freq=0))],
                   final_estimator=LogisticRegression(C=1.0, class_weight=None,
                                                      dual=False,
                                                      fit_intercept=True,
                                                      intercept_scaling=1,
                                                      l1_ratio=None,
                                                      max_iter=1000,
                                                      multi_class='auto',
                                                      n_jobs=None, penalty='l2',
                                                      random_state=6467,
                                                      solver='lbfgs',
                                                      tol=0.0001, verbose=0,
                                                      warm_start=False),
                   n_jobs=-1, passthrough=True, stack_method='auto', verbose=0), fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82DA82CE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 08:06:13,616:INFO:Checking exceptions
2024-02-09 08:06:13,616:INFO:Importing libraries
2024-02-09 08:06:13,616:INFO:Copying training dataset
2024-02-09 08:06:13,734:INFO:Defining folds
2024-02-09 08:06:13,734:INFO:Declaring metric variables
2024-02-09 08:06:13,737:INFO:Importing untrained model
2024-02-09 08:06:13,738:INFO:Declaring custom model
2024-02-09 08:06:13,743:INFO:Stacking Classifier Imported successfully
2024-02-09 08:06:13,751:INFO:Starting cross validation
2024-02-09 08:06:13,754:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 08:32:54,992:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 08:32:58,471:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 08:32:59,336:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 08:33:00,235:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 08:33:23,297:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 08:33:26,569:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 08:34:50,319:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 08:36:27,324:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 09:00:07,402:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 09:00:07,511:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 09:00:10,909:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 09:00:18,939:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 09:00:23,034:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 09:00:30,593:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 09:01:15,073:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 09:02:53,608:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 09:13:08,664:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 09:13:54,001:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 09:14:05,246:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 09:14:05,448:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-09 09:14:05,659:INFO:Calculating mean and std
2024-02-09 09:14:05,661:INFO:Creating metrics dataframe
2024-02-09 09:14:05,673:INFO:Finalizing model
2024-02-09 09:17:56,219:INFO:Uploading results into container
2024-02-09 09:17:56,221:INFO:Uploading model into container now
2024-02-09 09:17:56,222:INFO:_master_model_container: 42
2024-02-09 09:17:56,222:INFO:_display_container: 5
2024-02-09 09:17:56,230:INFO:StackingClassifier(cv=5,
                   estimators=[('CatBoost Classifier',
                                <catboost.core.CatBoostClassifier object at 0x000001D82E71DC30>),
                               ('Light Gradient Boosting Machine',
                                LGBMClassifier(boosting_type='gbdt',
                                               class_weight=None,
                                               colsample_bytree=1.0,
                                               importance_type='split',
                                               learning_rate=0.1, max_depth=-1,
                                               min_child_samples=20,
                                               min_child_weight=0.001,
                                               min_split_gain=0.0,
                                               n_est...
                                               subsample_for_bin=200000,
                                               subsample_freq=0))],
                   final_estimator=LogisticRegression(C=1.0, class_weight=None,
                                                      dual=False,
                                                      fit_intercept=True,
                                                      intercept_scaling=1,
                                                      l1_ratio=None,
                                                      max_iter=1000,
                                                      multi_class='auto',
                                                      n_jobs=None, penalty='l2',
                                                      random_state=6467,
                                                      solver='lbfgs',
                                                      tol=0.0001, verbose=0,
                                                      warm_start=False),
                   n_jobs=-1, passthrough=True, stack_method='auto', verbose=0)
2024-02-09 09:17:56,230:INFO:create_model() successfully completed......................................
2024-02-09 09:17:56,324:INFO:SubProcess create_model() end ==================================
2024-02-09 09:17:56,324:INFO:choose_better activated
2024-02-09 09:17:56,331:INFO:StackingClassifier(cv=5,
                   estimators=[('CatBoost Classifier',
                                <catboost.core.CatBoostClassifier object at 0x000001D82E71DC30>),
                               ('Light Gradient Boosting Machine',
                                LGBMClassifier(boosting_type='gbdt',
                                               class_weight=None,
                                               colsample_bytree=1.0,
                                               importance_type='split',
                                               learning_rate=0.1, max_depth=-1,
                                               min_child_samples=20,
                                               min_child_weight=0.001,
                                               min_split_gain=0.0,
                                               n_est...
                                               subsample_for_bin=200000,
                                               subsample_freq=0))],
                   final_estimator=LogisticRegression(C=1.0, class_weight=None,
                                                      dual=False,
                                                      fit_intercept=True,
                                                      intercept_scaling=1,
                                                      l1_ratio=None,
                                                      max_iter=1000,
                                                      multi_class='auto',
                                                      n_jobs=None, penalty='l2',
                                                      random_state=6467,
                                                      solver='lbfgs',
                                                      tol=0.0001, verbose=0,
                                                      warm_start=False),
                   n_jobs=-1, passthrough=True, stack_method='auto', verbose=0) result for Accuracy is 0.8814
2024-02-09 09:17:56,332:INFO:SubProcess create_model() called ==================================
2024-02-09 09:17:56,332:INFO:Initializing create_model()
2024-02-09 09:17:56,332:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=<catboost.core.CatBoostClassifier object at 0x000001D82DE76A40>, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 09:17:56,333:INFO:Checking exceptions
2024-02-09 09:17:56,335:INFO:Importing libraries
2024-02-09 09:17:56,335:INFO:Copying training dataset
2024-02-09 09:17:56,453:INFO:Defining folds
2024-02-09 09:17:56,453:INFO:Declaring metric variables
2024-02-09 09:17:56,453:INFO:Importing untrained model
2024-02-09 09:17:56,453:INFO:Declaring custom model
2024-02-09 09:17:56,453:INFO:CatBoost Classifier Imported successfully
2024-02-09 09:17:56,454:INFO:Starting cross validation
2024-02-09 09:17:56,458:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 09:29:22,668:INFO:Calculating mean and std
2024-02-09 09:29:22,668:INFO:Creating metrics dataframe
2024-02-09 09:29:22,671:INFO:Finalizing model
2024-02-09 09:30:08,147:INFO:Uploading results into container
2024-02-09 09:30:08,148:INFO:Uploading model into container now
2024-02-09 09:30:08,149:INFO:_master_model_container: 43
2024-02-09 09:30:08,149:INFO:_display_container: 6
2024-02-09 09:30:08,149:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82DE71990>
2024-02-09 09:30:08,149:INFO:create_model() successfully completed......................................
2024-02-09 09:30:08,235:INFO:SubProcess create_model() end ==================================
2024-02-09 09:30:08,235:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82DE71990> result for Accuracy is 0.9421
2024-02-09 09:30:08,235:INFO:SubProcess create_model() called ==================================
2024-02-09 09:30:08,236:INFO:Initializing create_model()
2024-02-09 09:30:08,236:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6467, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 09:30:08,236:INFO:Checking exceptions
2024-02-09 09:30:08,238:INFO:Importing libraries
2024-02-09 09:30:08,238:INFO:Copying training dataset
2024-02-09 09:30:08,350:INFO:Defining folds
2024-02-09 09:30:08,350:INFO:Declaring metric variables
2024-02-09 09:30:08,350:INFO:Importing untrained model
2024-02-09 09:30:08,350:INFO:Declaring custom model
2024-02-09 09:30:08,351:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-09 09:30:08,352:INFO:Starting cross validation
2024-02-09 09:30:08,355:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-09 09:31:05,807:INFO:Calculating mean and std
2024-02-09 09:31:05,808:INFO:Creating metrics dataframe
2024-02-09 09:31:05,811:INFO:Finalizing model
2024-02-09 09:31:07,182:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-09 09:31:07,305:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.093992 seconds.
2024-02-09 09:31:07,305:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-09 09:31:07,307:INFO:[LightGBM] [Info] Total Bins 32382
2024-02-09 09:31:07,308:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 127
2024-02-09 09:31:07,310:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-09 09:31:10,182:INFO:Uploading results into container
2024-02-09 09:31:10,183:INFO:Uploading model into container now
2024-02-09 09:31:10,184:INFO:_master_model_container: 44
2024-02-09 09:31:10,184:INFO:_display_container: 6
2024-02-09 09:31:10,184:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6467, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-02-09 09:31:10,185:INFO:create_model() successfully completed......................................
2024-02-09 09:31:10,288:INFO:SubProcess create_model() end ==================================
2024-02-09 09:31:10,289:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6467, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.9382
2024-02-09 09:31:10,289:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82DE71990> is best model
2024-02-09 09:31:10,289:INFO:choose_better completed
2024-02-09 09:31:10,289:INFO:Original model was better than the stacked model, hence it will be returned. NOTE: The display metrics are for the stacked model (not the original one).
2024-02-09 09:31:10,305:INFO:_master_model_container: 44
2024-02-09 09:31:10,306:INFO:_display_container: 5
2024-02-09 09:31:10,306:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82DE71990>
2024-02-09 09:31:10,306:INFO:stack_models() successfully completed......................................
2024-02-09 09:31:10,453:INFO:Initializing automl()
2024-02-09 09:31:10,454:INFO:automl(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, optimize=Accuracy, use_holdout=False, turbo=True, return_train_score=False)
2024-02-09 09:31:10,454:INFO:Model Selection Basis : CV Results on Training set
2024-02-09 09:31:10,455:INFO:Checking model 30
2024-02-09 09:31:10,456:INFO:Checking model 31
2024-02-09 09:31:10,457:INFO:Checking model 32
2024-02-09 09:31:10,457:INFO:Checking model 33
2024-02-09 09:31:10,458:INFO:Checking model 34
2024-02-09 09:31:10,459:INFO:Checking model 35
2024-02-09 09:31:10,459:INFO:Checking model 36
2024-02-09 09:31:10,460:INFO:Checking model 37
2024-02-09 09:31:10,460:INFO:Checking model 38
2024-02-09 09:31:10,460:INFO:Checking model 39
2024-02-09 09:31:10,460:INFO:Checking model 40
2024-02-09 09:31:10,461:INFO:Checking model 41
2024-02-09 09:31:10,461:INFO:Checking model 42
2024-02-09 09:31:10,461:INFO:Checking model 43
2024-02-09 09:31:10,461:INFO:Initializing create_model()
2024-02-09 09:31:10,461:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=<catboost.core.CatBoostClassifier object at 0x000001D82D5EC100>, fold=None, round=4, cross_validation=False, predict=False, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 09:31:10,461:INFO:Checking exceptions
2024-02-09 09:31:10,464:INFO:Importing libraries
2024-02-09 09:31:10,464:INFO:Copying training dataset
2024-02-09 09:31:10,582:INFO:Defining folds
2024-02-09 09:31:10,582:INFO:Declaring metric variables
2024-02-09 09:31:10,583:INFO:Importing untrained model
2024-02-09 09:31:10,583:INFO:Declaring custom model
2024-02-09 09:31:10,583:INFO:CatBoost Classifier Imported successfully
2024-02-09 09:31:10,588:INFO:Cross validation set to False
2024-02-09 09:31:10,588:INFO:Fitting Model
2024-02-09 09:31:55,275:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82DE75AE0>
2024-02-09 09:31:55,275:INFO:create_model() successfully completed......................................
2024-02-09 09:31:55,429:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82DE75AE0>
2024-02-09 09:31:55,429:INFO:automl() successfully completed......................................
2024-02-09 09:31:55,442:INFO:Initializing predict_model()
2024-02-09 09:31:55,442:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=<catboost.core.CatBoostClassifier object at 0x000001D82DE75AE0>, probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D82D652200>)
2024-02-09 09:31:55,442:INFO:Checking exceptions
2024-02-09 09:31:55,443:INFO:Preloading libraries
2024-02-09 09:31:56,073:INFO:Initializing finalize_model()
2024-02-09 09:31:56,073:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=<catboost.core.CatBoostClassifier object at 0x000001D82DE75AE0>, fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-02-09 09:31:56,074:INFO:Finalizing <catboost.core.CatBoostClassifier object at 0x000001D82DE75AE0>
2024-02-09 09:31:56,140:INFO:Initializing create_model()
2024-02-09 09:31:56,141:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=<catboost.core.CatBoostClassifier object at 0x000001D82DE75AE0>, fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-02-09 09:31:56,141:INFO:Checking exceptions
2024-02-09 09:31:56,143:INFO:Importing libraries
2024-02-09 09:31:56,144:INFO:Copying training dataset
2024-02-09 09:31:56,151:INFO:Defining folds
2024-02-09 09:31:56,151:INFO:Declaring metric variables
2024-02-09 09:31:56,151:INFO:Importing untrained model
2024-02-09 09:31:56,151:INFO:Declaring custom model
2024-02-09 09:31:56,152:INFO:CatBoost Classifier Imported successfully
2024-02-09 09:31:56,155:INFO:Cross validation set to False
2024-02-09 09:31:56,155:INFO:Fitting Model
2024-02-09 09:32:49,681:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id', 'last_date_of_month_7',
                                             'last_date_of_month_8', 'arpu_6',
                                             'arpu_7', 'arpu_8', 'onnet_mou_6',
                                             'onnet_mou_7', 'onnet_mou_8',
                                             'offnet_mou_6', 'offnet_mou_7',
                                             'offnet_mou_8', 'roam_ic_mou_6',
                                             'roam_ic_mou_7', 'roam_ic_mou_8',
                                             'roam_og_mou_6', 'roam_og_mou_7...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              n_jobs=None,
                                                                              random_state=None,
                                                                              sampling_strategy='auto')))),
                ('actual_estimator',
                 <catboost.core.CatBoostClassifier object at 0x000001D82DADE6B0>)],
         verbose=False)
2024-02-09 09:32:49,681:INFO:create_model() successfully completed......................................
2024-02-09 09:32:49,757:INFO:_master_model_container: 44
2024-02-09 09:32:49,758:INFO:_display_container: 5
2024-02-09 09:32:49,772:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id', 'last_date_of_month_7',
                                             'last_date_of_month_8', 'arpu_6',
                                             'arpu_7', 'arpu_8', 'onnet_mou_6',
                                             'onnet_mou_7', 'onnet_mou_8',
                                             'offnet_mou_6', 'offnet_mou_7',
                                             'offnet_mou_8', 'roam_ic_mou_6',
                                             'roam_ic_mou_7', 'roam_ic_mou_8',
                                             'roam_og_mou_6', 'roam_og_mou_7...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              n_jobs=None,
                                                                              random_state=None,
                                                                              sampling_strategy='auto')))),
                ('actual_estimator',
                 <catboost.core.CatBoostClassifier object at 0x000001D82DADE6B0>)],
         verbose=False)
2024-02-09 09:32:49,772:INFO:finalize_model() successfully completed......................................
2024-02-09 09:32:49,865:INFO:Initializing predict_model()
2024-02-09 09:32:49,865:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id', 'last_date_of_month_7',
                                             'last_date_of_month_8', 'arpu_6',
                                             'arpu_7', 'arpu_8', 'onnet_mou_6',
                                             'onnet_mou_7', 'onnet_mou_8',
                                             'offnet_mou_6', 'offnet_mou_7',
                                             'offnet_mou_8', 'roam_ic_mou_6',
                                             'roam_ic_mou_7', 'roam_ic_mou_8',
                                             'roam_og_mou_6', 'roam_og_mou_7...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              n_jobs=None,
                                                                              random_state=None,
                                                                              sampling_strategy='auto')))),
                ('actual_estimator',
                 <catboost.core.CatBoostClassifier object at 0x000001D82DADE6B0>)],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D82D652050>)
2024-02-09 09:32:49,865:INFO:Checking exceptions
2024-02-09 09:32:49,865:INFO:Preloading libraries
2024-02-09 09:32:50,409:INFO:Initializing predict_model()
2024-02-09 09:32:50,409:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id', 'last_date_of_month_7',
                                             'last_date_of_month_8', 'arpu_6',
                                             'arpu_7', 'arpu_8', 'onnet_mou_6',
                                             'onnet_mou_7', 'onnet_mou_8',
                                             'offnet_mou_6', 'offnet_mou_7',
                                             'offnet_mou_8', 'roam_ic_mou_6',
                                             'roam_ic_mou_7', 'roam_ic_mou_8',
                                             'roam_og_mou_6', 'roam_og_mou_7...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              n_jobs=None,
                                                                              random_state=None,
                                                                              sampling_strategy='auto')))),
                ('actual_estimator',
                 <catboost.core.CatBoostClassifier object at 0x000001D82DADE6B0>)],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D82DF4CF70>)
2024-02-09 09:32:50,409:INFO:Checking exceptions
2024-02-09 09:32:50,409:INFO:Preloading libraries
2024-02-09 09:32:50,412:INFO:Set up data.
2024-02-09 09:32:50,500:INFO:Set up index.
2024-02-09 09:32:50,857:INFO:Initializing predict_model()
2024-02-09 09:32:50,857:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=<catboost.core.CatBoostClassifier object at 0x000001D82DE75AE0>, probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D82DF4C790>)
2024-02-09 09:32:50,857:INFO:Checking exceptions
2024-02-09 09:32:50,857:INFO:Preloading libraries
2024-02-09 09:32:50,859:INFO:Set up data.
2024-02-09 09:32:50,941:INFO:Set up index.
2024-02-09 09:39:59,144:INFO:Initializing predict_model()
2024-02-09 09:39:59,144:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D3EA170>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id', 'last_date_of_month_7',
                                             'last_date_of_month_8', 'arpu_6',
                                             'arpu_7', 'arpu_8', 'onnet_mou_6',
                                             'onnet_mou_7', 'onnet_mou_8',
                                             'offnet_mou_6', 'offnet_mou_7',
                                             'offnet_mou_8', 'roam_ic_mou_6',
                                             'roam_ic_mou_7', 'roam_ic_mou_8',
                                             'roam_og_mou_6', 'roam_og_mou_7...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              n_jobs=None,
                                                                              random_state=None,
                                                                              sampling_strategy='auto')))),
                ('actual_estimator',
                 <catboost.core.CatBoostClassifier object at 0x000001D82DADE6B0>)],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D82D653250>)
2024-02-09 09:39:59,144:INFO:Checking exceptions
2024-02-09 09:39:59,145:INFO:Preloading libraries
2024-02-10 20:20:18,431:INFO:PyCaret ClassificationExperiment
2024-02-10 20:20:18,432:INFO:Logging name: clf-default-name
2024-02-10 20:20:18,433:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-02-10 20:20:18,433:INFO:version 3.2.0
2024-02-10 20:20:18,433:INFO:Initializing setup()
2024-02-10 20:20:18,433:INFO:self.USI: 2483
2024-02-10 20:20:18,433:INFO:self._variable_keys: {'fold_shuffle_param', 'y_test', '_available_plots', 'idx', 'X', 'fix_imbalance', 'USI', 'pipeline', 'n_jobs_param', 'exp_name_log', 'memory', 'fold_generator', 'log_plots_param', 'y_train', 'seed', '_ml_usecase', 'gpu_param', 'logging_param', 'data', 'X_train', 'target_param', 'html_param', 'exp_id', 'fold_groups_param', 'is_multiclass', 'gpu_n_jobs_param', 'X_test', 'y'}
2024-02-10 20:20:18,434:INFO:Checking environment
2024-02-10 20:20:18,435:INFO:python_version: 3.10.9
2024-02-10 20:20:18,435:INFO:python_build: ('main', 'Jan 11 2023 15:15:40')
2024-02-10 20:20:18,435:INFO:machine: AMD64
2024-02-10 20:20:18,435:INFO:platform: Windows-10-10.0.19045-SP0
2024-02-10 20:20:18,436:INFO:Memory: svmem(total=16856182784, available=5977448448, percent=64.5, used=10878734336, free=5977448448)
2024-02-10 20:20:18,437:INFO:Physical Core: 4
2024-02-10 20:20:18,437:INFO:Logical Core: 8
2024-02-10 20:20:18,437:INFO:Checking libraries
2024-02-10 20:20:18,438:INFO:System:
2024-02-10 20:20:18,438:INFO:    python: 3.10.9 | packaged by conda-forge | (main, Jan 11 2023, 15:15:40) [MSC v.1916 64 bit (AMD64)]
2024-02-10 20:20:18,438:INFO:executable: C:\ProgramData\miniconda3\python.exe
2024-02-10 20:20:18,438:INFO:   machine: Windows-10-10.0.19045-SP0
2024-02-10 20:20:18,438:INFO:PyCaret required dependencies:
2024-02-10 20:20:18,440:INFO:                 pip: 22.3.1
2024-02-10 20:20:18,441:INFO:          setuptools: 65.6.3
2024-02-10 20:20:18,441:INFO:             pycaret: 3.2.0
2024-02-10 20:20:18,441:INFO:             IPython: 8.20.0
2024-02-10 20:20:18,441:INFO:          ipywidgets: 8.0.4
2024-02-10 20:20:18,441:INFO:                tqdm: 4.64.1
2024-02-10 20:20:18,441:INFO:               numpy: 1.25.2
2024-02-10 20:20:18,441:INFO:              pandas: 1.5.3
2024-02-10 20:20:18,441:INFO:              jinja2: 3.1.3
2024-02-10 20:20:18,441:INFO:               scipy: 1.10.1
2024-02-10 20:20:18,441:INFO:              joblib: 1.3.2
2024-02-10 20:20:18,441:INFO:             sklearn: 1.2.2
2024-02-10 20:20:18,441:INFO:                pyod: 1.1.2
2024-02-10 20:20:18,441:INFO:            imblearn: 0.12.0
2024-02-10 20:20:18,441:INFO:   category_encoders: 2.6.3
2024-02-10 20:20:18,441:INFO:            lightgbm: 4.3.0
2024-02-10 20:20:18,441:INFO:               numba: 0.59.0
2024-02-10 20:20:18,441:INFO:            requests: 2.31.0
2024-02-10 20:20:18,442:INFO:          matplotlib: 3.6.0
2024-02-10 20:20:18,442:INFO:          scikitplot: 0.3.7
2024-02-10 20:20:18,442:INFO:         yellowbrick: 1.5
2024-02-10 20:20:18,442:INFO:              plotly: 5.18.0
2024-02-10 20:20:18,442:INFO:    plotly-resampler: Not installed
2024-02-10 20:20:18,442:INFO:             kaleido: 0.2.1
2024-02-10 20:20:18,442:INFO:           schemdraw: 0.15
2024-02-10 20:20:18,442:INFO:         statsmodels: 0.14.1
2024-02-10 20:20:18,442:INFO:              sktime: 0.21.1
2024-02-10 20:20:18,442:INFO:               tbats: 1.1.3
2024-02-10 20:20:18,442:INFO:            pmdarima: 2.0.4
2024-02-10 20:20:18,442:INFO:              psutil: 5.9.0
2024-02-10 20:20:18,442:INFO:          markupsafe: 2.1.3
2024-02-10 20:20:18,442:INFO:             pickle5: Not installed
2024-02-10 20:20:18,442:INFO:         cloudpickle: 3.0.0
2024-02-10 20:20:18,442:INFO:         deprecation: 2.1.0
2024-02-10 20:20:18,442:INFO:              xxhash: 3.4.1
2024-02-10 20:20:18,442:INFO:           wurlitzer: Not installed
2024-02-10 20:20:18,443:INFO:PyCaret optional dependencies:
2024-02-10 20:20:18,443:INFO:                shap: 0.44.1
2024-02-10 20:20:18,443:INFO:           interpret: Not installed
2024-02-10 20:20:18,443:INFO:                umap: Not installed
2024-02-10 20:20:18,443:INFO:     ydata_profiling: Not installed
2024-02-10 20:20:18,443:INFO:  explainerdashboard: 0.4.5
2024-02-10 20:20:18,443:INFO:             autoviz: Not installed
2024-02-10 20:20:18,443:INFO:           fairlearn: Not installed
2024-02-10 20:20:18,443:INFO:          deepchecks: Not installed
2024-02-10 20:20:18,443:INFO:             xgboost: Not installed
2024-02-10 20:20:18,443:INFO:            catboost: 1.2.2
2024-02-10 20:20:18,443:INFO:              kmodes: Not installed
2024-02-10 20:20:18,444:INFO:             mlxtend: Not installed
2024-02-10 20:20:18,444:INFO:       statsforecast: Not installed
2024-02-10 20:20:18,444:INFO:        tune_sklearn: Not installed
2024-02-10 20:20:18,444:INFO:                 ray: Not installed
2024-02-10 20:20:18,444:INFO:            hyperopt: Not installed
2024-02-10 20:20:18,444:INFO:              optuna: Not installed
2024-02-10 20:20:18,444:INFO:               skopt: Not installed
2024-02-10 20:20:18,444:INFO:              mlflow: 2.10.0
2024-02-10 20:20:18,444:INFO:              gradio: Not installed
2024-02-10 20:20:18,444:INFO:             fastapi: Not installed
2024-02-10 20:20:18,444:INFO:             uvicorn: Not installed
2024-02-10 20:20:18,444:INFO:              m2cgen: Not installed
2024-02-10 20:20:18,444:INFO:           evidently: Not installed
2024-02-10 20:20:18,444:INFO:               fugue: Not installed
2024-02-10 20:20:18,444:INFO:           streamlit: Not installed
2024-02-10 20:20:18,444:INFO:             prophet: Not installed
2024-02-10 20:20:18,444:INFO:None
2024-02-10 20:20:18,445:INFO:Set up data.
2024-02-10 20:20:18,714:INFO:Set up folding strategy.
2024-02-10 20:20:18,716:INFO:Set up train/test split.
2024-02-10 20:20:18,940:INFO:Set up index.
2024-02-10 20:20:18,951:INFO:Assigning column types.
2024-02-10 20:20:19,108:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-02-10 20:20:19,172:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-10 20:20:19,177:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-02-10 20:20:19,228:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-10 20:20:19,232:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-10 20:20:19,320:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-10 20:20:19,321:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-02-10 20:20:19,372:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-10 20:20:19,372:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-10 20:20:19,373:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-02-10 20:20:19,436:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-02-10 20:20:19,486:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-10 20:20:19,487:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-10 20:20:19,549:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-02-10 20:20:19,589:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-10 20:20:19,590:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-10 20:20:19,590:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-02-10 20:20:19,703:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-10 20:20:19,703:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-10 20:20:19,879:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-10 20:20:19,879:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-10 20:20:19,889:INFO:Preparing preprocessing pipeline...
2024-02-10 20:20:19,913:INFO:Set up simple imputation.
2024-02-10 20:20:19,915:INFO:Set up imbalanced handling.
2024-02-10 20:20:20,624:INFO:Finished creating preprocessing pipeline.
2024-02-10 20:20:20,639:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\user\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id', 'last_date_of_month_7',
                                             'last_date_of_month_8', 'arpu_6',
                                             'arpu_7', 'arpu_8', 'onnet_mou_6',
                                             'onnet_mou_7', 'onnet_mou_8',
                                             'offnet_mou_6', 'offnet_mou_7',
                                             'offnet_mou_8', 'roam_ic_mou_6',
                                             'roam_ic_mou_7', 'roam_i...
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              n_jobs=None,
                                                                              random_state=None,
                                                                              sampling_strategy='auto'))))],
         verbose=False)
2024-02-10 20:20:20,639:INFO:Creating final display dataframe.
2024-02-10 20:20:24,352:INFO:Setup _display_container:                     Description              Value
0                    Session id               1483
1                        Target  churn_probability
2                   Target type             Binary
3           Original data shape       (69999, 129)
4        Transformed data shape      (109014, 129)
5   Transformed train set shape       (88014, 129)
6    Transformed test set shape       (21000, 129)
7              Numeric features                128
8                    Preprocess               True
9               Imputation type             simple
10           Numeric imputation               mean
11       Categorical imputation               mode
12                Fix imbalance               True
13         Fix imbalance method              SMOTE
14               Fold Generator    StratifiedKFold
15                  Fold Number                 20
16                     CPU Jobs                 -1
17                      Use GPU              False
18               Log Experiment              False
19              Experiment Name   clf-default-name
20                          USI               2483
2024-02-10 20:20:24,519:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-10 20:20:24,520:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-10 20:20:24,660:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-10 20:20:24,660:INFO:Soft dependency imported: catboost: 1.2.2
2024-02-10 20:20:24,666:INFO:setup() successfully completed in 6.35s...............
2024-02-10 20:20:24,973:INFO:Initializing compare_models()
2024-02-10 20:20:24,975:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, include=None, fold=20, round=4, cross_validation=True, sort=Accuracy, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, 'include': None, 'exclude': None, 'fold': 20, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-02-10 20:20:24,975:INFO:Checking exceptions
2024-02-10 20:20:25,099:INFO:Preparing display monitor
2024-02-10 20:20:25,158:INFO:Initializing Logistic Regression
2024-02-10 20:20:25,159:INFO:Total runtime is 2.3392836252848306e-05 minutes
2024-02-10 20:20:25,168:INFO:SubProcess create_model() called ==================================
2024-02-10 20:20:25,170:INFO:Initializing create_model()
2024-02-10 20:20:25,170:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, estimator=lr, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82E730070>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-10 20:20:25,170:INFO:Checking exceptions
2024-02-10 20:20:25,171:INFO:Importing libraries
2024-02-10 20:20:25,171:INFO:Copying training dataset
2024-02-10 20:20:25,355:INFO:Defining folds
2024-02-10 20:20:25,356:INFO:Declaring metric variables
2024-02-10 20:20:25,365:INFO:Importing untrained model
2024-02-10 20:20:25,372:INFO:Logistic Regression Imported successfully
2024-02-10 20:20:25,388:INFO:Starting cross validation
2024-02-10 20:20:25,395:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-10 20:21:44,327:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-10 20:21:45,575:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-10 20:21:46,035:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-10 20:21:46,253:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-10 20:21:46,814:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-10 20:21:46,852:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-10 20:21:47,894:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-10 20:21:48,541:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-10 20:22:49,886:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-10 20:22:49,890:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-10 20:22:50,940:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-10 20:22:51,203:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-10 20:22:53,356:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-10 20:22:54,111:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-10 20:22:54,262:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-10 20:22:55,415:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-10 20:23:30,518:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-10 20:23:30,573:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-10 20:23:30,797:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-10 20:23:31,042:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-02-10 20:23:31,226:INFO:Calculating mean and std
2024-02-10 20:23:31,229:INFO:Creating metrics dataframe
2024-02-10 20:23:31,236:INFO:Uploading results into container
2024-02-10 20:23:31,237:INFO:Uploading model into container now
2024-02-10 20:23:31,239:INFO:_master_model_container: 1
2024-02-10 20:23:31,240:INFO:_display_container: 2
2024-02-10 20:23:31,241:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=1483, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-02-10 20:23:31,241:INFO:create_model() successfully completed......................................
2024-02-10 20:23:31,771:INFO:SubProcess create_model() end ==================================
2024-02-10 20:23:31,772:INFO:Creating metrics dataframe
2024-02-10 20:23:31,793:INFO:Initializing K Neighbors Classifier
2024-02-10 20:23:31,793:INFO:Total runtime is 3.1105892419815064 minutes
2024-02-10 20:23:31,800:INFO:SubProcess create_model() called ==================================
2024-02-10 20:23:31,801:INFO:Initializing create_model()
2024-02-10 20:23:31,801:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, estimator=knn, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82E730070>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-10 20:23:31,802:INFO:Checking exceptions
2024-02-10 20:23:31,802:INFO:Importing libraries
2024-02-10 20:23:31,802:INFO:Copying training dataset
2024-02-10 20:23:31,982:INFO:Defining folds
2024-02-10 20:23:31,982:INFO:Declaring metric variables
2024-02-10 20:23:31,987:INFO:Importing untrained model
2024-02-10 20:23:31,997:INFO:K Neighbors Classifier Imported successfully
2024-02-10 20:23:32,013:INFO:Starting cross validation
2024-02-10 20:23:32,019:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-10 20:24:25,394:INFO:Calculating mean and std
2024-02-10 20:24:25,398:INFO:Creating metrics dataframe
2024-02-10 20:24:25,411:INFO:Uploading results into container
2024-02-10 20:24:25,413:INFO:Uploading model into container now
2024-02-10 20:24:25,414:INFO:_master_model_container: 2
2024-02-10 20:24:25,415:INFO:_display_container: 2
2024-02-10 20:24:25,415:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-02-10 20:24:25,416:INFO:create_model() successfully completed......................................
2024-02-10 20:24:25,563:INFO:SubProcess create_model() end ==================================
2024-02-10 20:24:25,563:INFO:Creating metrics dataframe
2024-02-10 20:24:25,601:INFO:Initializing Naive Bayes
2024-02-10 20:24:25,601:INFO:Total runtime is 4.00739871263504 minutes
2024-02-10 20:24:25,609:INFO:SubProcess create_model() called ==================================
2024-02-10 20:24:25,610:INFO:Initializing create_model()
2024-02-10 20:24:25,611:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, estimator=nb, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82E730070>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-10 20:24:25,612:INFO:Checking exceptions
2024-02-10 20:24:25,613:INFO:Importing libraries
2024-02-10 20:24:25,613:INFO:Copying training dataset
2024-02-10 20:24:25,883:INFO:Defining folds
2024-02-10 20:24:25,884:INFO:Declaring metric variables
2024-02-10 20:24:25,895:INFO:Importing untrained model
2024-02-10 20:24:25,907:INFO:Naive Bayes Imported successfully
2024-02-10 20:24:25,930:INFO:Starting cross validation
2024-02-10 20:24:25,936:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-10 20:24:37,436:INFO:Calculating mean and std
2024-02-10 20:24:37,440:INFO:Creating metrics dataframe
2024-02-10 20:24:37,452:INFO:Uploading results into container
2024-02-10 20:24:37,453:INFO:Uploading model into container now
2024-02-10 20:24:37,455:INFO:_master_model_container: 3
2024-02-10 20:24:37,455:INFO:_display_container: 2
2024-02-10 20:24:37,455:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-02-10 20:24:37,456:INFO:create_model() successfully completed......................................
2024-02-10 20:24:37,619:INFO:SubProcess create_model() end ==================================
2024-02-10 20:24:37,620:INFO:Creating metrics dataframe
2024-02-10 20:24:37,662:INFO:Initializing Decision Tree Classifier
2024-02-10 20:24:37,663:INFO:Total runtime is 4.2084315816561375 minutes
2024-02-10 20:24:37,672:INFO:SubProcess create_model() called ==================================
2024-02-10 20:24:37,673:INFO:Initializing create_model()
2024-02-10 20:24:37,673:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, estimator=dt, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82E730070>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-10 20:24:37,674:INFO:Checking exceptions
2024-02-10 20:24:37,675:INFO:Importing libraries
2024-02-10 20:24:37,675:INFO:Copying training dataset
2024-02-10 20:24:37,926:INFO:Defining folds
2024-02-10 20:24:37,927:INFO:Declaring metric variables
2024-02-10 20:24:37,940:INFO:Importing untrained model
2024-02-10 20:24:37,953:INFO:Decision Tree Classifier Imported successfully
2024-02-10 20:24:37,972:INFO:Starting cross validation
2024-02-10 20:24:37,979:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-10 20:26:40,319:INFO:Calculating mean and std
2024-02-10 20:26:40,324:INFO:Creating metrics dataframe
2024-02-10 20:26:40,339:INFO:Uploading results into container
2024-02-10 20:26:40,342:INFO:Uploading model into container now
2024-02-10 20:26:40,345:INFO:_master_model_container: 4
2024-02-10 20:26:40,346:INFO:_display_container: 2
2024-02-10 20:26:40,346:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=1483, splitter='best')
2024-02-10 20:26:40,348:INFO:create_model() successfully completed......................................
2024-02-10 20:26:40,536:INFO:SubProcess create_model() end ==================================
2024-02-10 20:26:40,537:INFO:Creating metrics dataframe
2024-02-10 20:26:40,569:INFO:Initializing SVM - Linear Kernel
2024-02-10 20:26:40,570:INFO:Total runtime is 6.256870245933532 minutes
2024-02-10 20:26:40,580:INFO:SubProcess create_model() called ==================================
2024-02-10 20:26:40,582:INFO:Initializing create_model()
2024-02-10 20:26:40,582:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, estimator=svm, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82E730070>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-10 20:26:40,582:INFO:Checking exceptions
2024-02-10 20:26:40,583:INFO:Importing libraries
2024-02-10 20:26:40,583:INFO:Copying training dataset
2024-02-10 20:26:40,834:INFO:Defining folds
2024-02-10 20:26:40,834:INFO:Declaring metric variables
2024-02-10 20:26:40,844:INFO:Importing untrained model
2024-02-10 20:26:40,859:INFO:SVM - Linear Kernel Imported successfully
2024-02-10 20:26:40,878:INFO:Starting cross validation
2024-02-10 20:26:40,885:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-10 20:27:04,601:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-10 20:27:06,906:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-10 20:27:08,488:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-10 20:27:11,262:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-10 20:27:15,001:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-10 20:27:18,009:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-10 20:27:18,925:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-10 20:27:22,271:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-10 20:27:34,818:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-10 20:27:36,189:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-10 20:27:47,591:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-10 20:27:49,786:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-10 20:27:51,615:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-10 20:27:51,726:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-10 20:27:55,071:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-10 20:27:59,758:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-10 20:28:07,795:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-10 20:28:13,691:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-10 20:28:13,795:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-10 20:28:19,590:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-02-10 20:28:19,735:INFO:Calculating mean and std
2024-02-10 20:28:19,739:INFO:Creating metrics dataframe
2024-02-10 20:28:19,750:INFO:Uploading results into container
2024-02-10 20:28:19,752:INFO:Uploading model into container now
2024-02-10 20:28:19,753:INFO:_master_model_container: 5
2024-02-10 20:28:19,753:INFO:_display_container: 2
2024-02-10 20:28:19,754:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=1483, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-02-10 20:28:19,754:INFO:create_model() successfully completed......................................
2024-02-10 20:28:19,921:INFO:SubProcess create_model() end ==================================
2024-02-10 20:28:19,922:INFO:Creating metrics dataframe
2024-02-10 20:28:19,963:INFO:Initializing Ridge Classifier
2024-02-10 20:28:19,964:INFO:Total runtime is 7.913449160257975 minutes
2024-02-10 20:28:19,978:INFO:SubProcess create_model() called ==================================
2024-02-10 20:28:19,979:INFO:Initializing create_model()
2024-02-10 20:28:19,979:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, estimator=ridge, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82E730070>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-10 20:28:19,980:INFO:Checking exceptions
2024-02-10 20:28:19,980:INFO:Importing libraries
2024-02-10 20:28:19,980:INFO:Copying training dataset
2024-02-10 20:28:20,265:INFO:Defining folds
2024-02-10 20:28:20,266:INFO:Declaring metric variables
2024-02-10 20:28:20,279:INFO:Importing untrained model
2024-02-10 20:28:20,295:INFO:Ridge Classifier Imported successfully
2024-02-10 20:28:20,322:INFO:Starting cross validation
2024-02-10 20:28:20,331:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-10 20:28:24,820:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-10 20:28:24,827:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-10 20:28:24,840:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-10 20:28:24,884:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-10 20:28:24,892:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-10 20:28:24,966:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-10 20:28:25,003:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-10 20:28:25,050:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-10 20:28:29,201:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-10 20:28:29,393:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-10 20:28:29,430:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-10 20:28:29,487:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-10 20:28:29,506:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-10 20:28:29,526:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-10 20:28:29,542:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-10 20:28:29,586:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-10 20:28:32,371:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-10 20:28:32,533:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-10 20:28:32,598:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-10 20:28:32,611:WARNING:C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\ProgramData\miniconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-02-10 20:28:32,872:INFO:Calculating mean and std
2024-02-10 20:28:32,876:INFO:Creating metrics dataframe
2024-02-10 20:28:32,889:INFO:Uploading results into container
2024-02-10 20:28:32,891:INFO:Uploading model into container now
2024-02-10 20:28:32,892:INFO:_master_model_container: 6
2024-02-10 20:28:32,893:INFO:_display_container: 2
2024-02-10 20:28:32,895:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=1483, solver='auto',
                tol=0.0001)
2024-02-10 20:28:32,895:INFO:create_model() successfully completed......................................
2024-02-10 20:28:33,045:INFO:SubProcess create_model() end ==================================
2024-02-10 20:28:33,046:INFO:Creating metrics dataframe
2024-02-10 20:28:33,076:INFO:Initializing Random Forest Classifier
2024-02-10 20:28:33,077:INFO:Total runtime is 8.131985743840534 minutes
2024-02-10 20:28:33,089:INFO:SubProcess create_model() called ==================================
2024-02-10 20:28:33,090:INFO:Initializing create_model()
2024-02-10 20:28:33,090:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, estimator=rf, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82E730070>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-10 20:28:33,090:INFO:Checking exceptions
2024-02-10 20:28:33,090:INFO:Importing libraries
2024-02-10 20:28:33,091:INFO:Copying training dataset
2024-02-10 20:28:33,313:INFO:Defining folds
2024-02-10 20:28:33,314:INFO:Declaring metric variables
2024-02-10 20:28:33,326:INFO:Importing untrained model
2024-02-10 20:28:33,338:INFO:Random Forest Classifier Imported successfully
2024-02-10 20:28:33,366:INFO:Starting cross validation
2024-02-10 20:28:33,370:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-10 20:38:55,022:INFO:Calculating mean and std
2024-02-10 20:38:55,025:INFO:Creating metrics dataframe
2024-02-10 20:38:55,042:INFO:Uploading results into container
2024-02-10 20:38:55,044:INFO:Uploading model into container now
2024-02-10 20:38:55,045:INFO:_master_model_container: 7
2024-02-10 20:38:55,046:INFO:_display_container: 2
2024-02-10 20:38:55,048:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=1483, verbose=0, warm_start=False)
2024-02-10 20:38:55,049:INFO:create_model() successfully completed......................................
2024-02-10 20:38:55,239:INFO:SubProcess create_model() end ==================================
2024-02-10 20:38:55,241:INFO:Creating metrics dataframe
2024-02-10 20:38:55,281:INFO:Initializing Quadratic Discriminant Analysis
2024-02-10 20:38:55,281:INFO:Total runtime is 18.502062431971233 minutes
2024-02-10 20:38:55,293:INFO:SubProcess create_model() called ==================================
2024-02-10 20:38:55,295:INFO:Initializing create_model()
2024-02-10 20:38:55,295:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, estimator=qda, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82E730070>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-10 20:38:55,296:INFO:Checking exceptions
2024-02-10 20:38:55,296:INFO:Importing libraries
2024-02-10 20:38:55,296:INFO:Copying training dataset
2024-02-10 20:38:55,611:INFO:Defining folds
2024-02-10 20:38:55,611:INFO:Declaring metric variables
2024-02-10 20:38:55,624:INFO:Importing untrained model
2024-02-10 20:38:55,643:INFO:Quadratic Discriminant Analysis Imported successfully
2024-02-10 20:38:55,664:INFO:Starting cross validation
2024-02-10 20:38:55,671:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-10 20:39:05,072:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-10 20:39:05,167:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-10 20:39:05,328:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-10 20:39:05,375:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-10 20:39:05,447:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-10 20:39:05,459:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-10 20:39:05,531:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-10 20:39:05,574:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-10 20:39:20,230:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-10 20:39:21,294:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-10 20:39:21,352:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-10 20:39:21,554:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-10 20:39:21,619:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-10 20:39:21,693:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-10 20:39:21,720:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-10 20:39:21,859:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-10 20:39:32,997:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-10 20:39:33,046:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-10 20:39:33,153:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-10 20:39:33,391:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-02-10 20:39:36,730:INFO:Calculating mean and std
2024-02-10 20:39:36,733:INFO:Creating metrics dataframe
2024-02-10 20:39:36,741:INFO:Uploading results into container
2024-02-10 20:39:36,742:INFO:Uploading model into container now
2024-02-10 20:39:36,743:INFO:_master_model_container: 8
2024-02-10 20:39:36,743:INFO:_display_container: 2
2024-02-10 20:39:36,745:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-02-10 20:39:36,745:INFO:create_model() successfully completed......................................
2024-02-10 20:39:36,877:INFO:SubProcess create_model() end ==================================
2024-02-10 20:39:36,877:INFO:Creating metrics dataframe
2024-02-10 20:39:36,899:INFO:Initializing Ada Boost Classifier
2024-02-10 20:39:36,899:INFO:Total runtime is 19.195692173639934 minutes
2024-02-10 20:39:36,907:INFO:SubProcess create_model() called ==================================
2024-02-10 20:39:36,908:INFO:Initializing create_model()
2024-02-10 20:39:36,908:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, estimator=ada, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82E730070>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-10 20:39:36,908:INFO:Checking exceptions
2024-02-10 20:39:36,908:INFO:Importing libraries
2024-02-10 20:39:36,908:INFO:Copying training dataset
2024-02-10 20:39:37,065:INFO:Defining folds
2024-02-10 20:39:37,065:INFO:Declaring metric variables
2024-02-10 20:39:37,071:INFO:Importing untrained model
2024-02-10 20:39:37,081:INFO:Ada Boost Classifier Imported successfully
2024-02-10 20:39:37,096:INFO:Starting cross validation
2024-02-10 20:39:37,101:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-10 20:45:11,772:INFO:Calculating mean and std
2024-02-10 20:45:11,775:INFO:Creating metrics dataframe
2024-02-10 20:45:11,782:INFO:Uploading results into container
2024-02-10 20:45:11,784:INFO:Uploading model into container now
2024-02-10 20:45:11,784:INFO:_master_model_container: 9
2024-02-10 20:45:11,785:INFO:_display_container: 2
2024-02-10 20:45:11,785:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=1483)
2024-02-10 20:45:11,785:INFO:create_model() successfully completed......................................
2024-02-10 20:45:11,879:INFO:SubProcess create_model() end ==================================
2024-02-10 20:45:11,880:INFO:Creating metrics dataframe
2024-02-10 20:45:11,904:INFO:Initializing Gradient Boosting Classifier
2024-02-10 20:45:11,904:INFO:Total runtime is 24.77910761833191 minutes
2024-02-10 20:45:11,910:INFO:SubProcess create_model() called ==================================
2024-02-10 20:45:11,911:INFO:Initializing create_model()
2024-02-10 20:45:11,911:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, estimator=gbc, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82E730070>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-10 20:45:11,911:INFO:Checking exceptions
2024-02-10 20:45:11,911:INFO:Importing libraries
2024-02-10 20:45:11,911:INFO:Copying training dataset
2024-02-10 20:45:12,037:INFO:Defining folds
2024-02-10 20:45:12,037:INFO:Declaring metric variables
2024-02-10 20:45:12,042:INFO:Importing untrained model
2024-02-10 20:45:12,049:INFO:Gradient Boosting Classifier Imported successfully
2024-02-10 20:45:12,062:INFO:Starting cross validation
2024-02-10 20:45:12,067:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-10 21:20:48,948:INFO:Calculating mean and std
2024-02-10 21:20:48,954:INFO:Creating metrics dataframe
2024-02-10 21:20:48,961:INFO:Uploading results into container
2024-02-10 21:20:48,962:INFO:Uploading model into container now
2024-02-10 21:20:48,963:INFO:_master_model_container: 10
2024-02-10 21:20:48,964:INFO:_display_container: 2
2024-02-10 21:20:48,965:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=1483, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-02-10 21:20:48,965:INFO:create_model() successfully completed......................................
2024-02-10 21:20:49,091:INFO:SubProcess create_model() end ==================================
2024-02-10 21:20:49,092:INFO:Creating metrics dataframe
2024-02-10 21:20:49,110:INFO:Initializing Linear Discriminant Analysis
2024-02-10 21:20:49,110:INFO:Total runtime is 60.39920797348023 minutes
2024-02-10 21:20:49,114:INFO:SubProcess create_model() called ==================================
2024-02-10 21:20:49,115:INFO:Initializing create_model()
2024-02-10 21:20:49,115:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, estimator=lda, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82E730070>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-10 21:20:49,115:INFO:Checking exceptions
2024-02-10 21:20:49,115:INFO:Importing libraries
2024-02-10 21:20:49,115:INFO:Copying training dataset
2024-02-10 21:20:49,245:INFO:Defining folds
2024-02-10 21:20:49,246:INFO:Declaring metric variables
2024-02-10 21:20:49,253:INFO:Importing untrained model
2024-02-10 21:20:49,259:INFO:Linear Discriminant Analysis Imported successfully
2024-02-10 21:20:49,270:INFO:Starting cross validation
2024-02-10 21:20:49,277:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-10 21:21:30,307:INFO:Calculating mean and std
2024-02-10 21:21:30,310:INFO:Creating metrics dataframe
2024-02-10 21:21:30,318:INFO:Uploading results into container
2024-02-10 21:21:30,318:INFO:Uploading model into container now
2024-02-10 21:21:30,320:INFO:_master_model_container: 11
2024-02-10 21:21:30,320:INFO:_display_container: 2
2024-02-10 21:21:30,320:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-02-10 21:21:30,320:INFO:create_model() successfully completed......................................
2024-02-10 21:21:30,426:INFO:SubProcess create_model() end ==================================
2024-02-10 21:21:30,426:INFO:Creating metrics dataframe
2024-02-10 21:21:30,443:INFO:Initializing Extra Trees Classifier
2024-02-10 21:21:30,443:INFO:Total runtime is 61.08809302250545 minutes
2024-02-10 21:21:30,452:INFO:SubProcess create_model() called ==================================
2024-02-10 21:21:30,453:INFO:Initializing create_model()
2024-02-10 21:21:30,453:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, estimator=et, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82E730070>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-10 21:21:30,453:INFO:Checking exceptions
2024-02-10 21:21:30,453:INFO:Importing libraries
2024-02-10 21:21:30,453:INFO:Copying training dataset
2024-02-10 21:21:30,587:INFO:Defining folds
2024-02-10 21:21:30,587:INFO:Declaring metric variables
2024-02-10 21:21:30,592:INFO:Importing untrained model
2024-02-10 21:21:30,600:INFO:Extra Trees Classifier Imported successfully
2024-02-10 21:21:30,613:INFO:Starting cross validation
2024-02-10 21:21:30,618:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-10 21:25:46,071:INFO:Calculating mean and std
2024-02-10 21:25:46,078:INFO:Creating metrics dataframe
2024-02-10 21:25:46,089:INFO:Uploading results into container
2024-02-10 21:25:46,091:INFO:Uploading model into container now
2024-02-10 21:25:46,093:INFO:_master_model_container: 12
2024-02-10 21:25:46,093:INFO:_display_container: 2
2024-02-10 21:25:46,094:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=1483, verbose=0, warm_start=False)
2024-02-10 21:25:46,094:INFO:create_model() successfully completed......................................
2024-02-10 21:25:46,278:INFO:SubProcess create_model() end ==================================
2024-02-10 21:25:46,279:INFO:Creating metrics dataframe
2024-02-10 21:25:46,313:INFO:Initializing Light Gradient Boosting Machine
2024-02-10 21:25:46,315:INFO:Total runtime is 65.35262257655462 minutes
2024-02-10 21:25:46,325:INFO:SubProcess create_model() called ==================================
2024-02-10 21:25:46,325:INFO:Initializing create_model()
2024-02-10 21:25:46,326:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82E730070>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-10 21:25:46,326:INFO:Checking exceptions
2024-02-10 21:25:46,327:INFO:Importing libraries
2024-02-10 21:25:46,327:INFO:Copying training dataset
2024-02-10 21:25:46,513:INFO:Defining folds
2024-02-10 21:25:46,514:INFO:Declaring metric variables
2024-02-10 21:25:46,523:INFO:Importing untrained model
2024-02-10 21:25:46,539:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-10 21:25:46,564:INFO:Starting cross validation
2024-02-10 21:25:46,572:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-10 21:27:05,643:INFO:Calculating mean and std
2024-02-10 21:27:05,646:INFO:Creating metrics dataframe
2024-02-10 21:27:05,654:INFO:Uploading results into container
2024-02-10 21:27:05,657:INFO:Uploading model into container now
2024-02-10 21:27:05,658:INFO:_master_model_container: 13
2024-02-10 21:27:05,658:INFO:_display_container: 2
2024-02-10 21:27:05,659:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1483, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-02-10 21:27:05,659:INFO:create_model() successfully completed......................................
2024-02-10 21:27:05,814:INFO:SubProcess create_model() end ==================================
2024-02-10 21:27:05,816:INFO:Creating metrics dataframe
2024-02-10 21:27:05,854:INFO:Initializing CatBoost Classifier
2024-02-10 21:27:05,854:INFO:Total runtime is 66.67827831904094 minutes
2024-02-10 21:27:05,862:INFO:SubProcess create_model() called ==================================
2024-02-10 21:27:05,863:INFO:Initializing create_model()
2024-02-10 21:27:05,863:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, estimator=catboost, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82E730070>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-10 21:27:05,863:INFO:Checking exceptions
2024-02-10 21:27:05,864:INFO:Importing libraries
2024-02-10 21:27:05,864:INFO:Copying training dataset
2024-02-10 21:27:06,050:INFO:Defining folds
2024-02-10 21:27:06,050:INFO:Declaring metric variables
2024-02-10 21:27:06,057:INFO:Importing untrained model
2024-02-10 21:27:06,067:INFO:CatBoost Classifier Imported successfully
2024-02-10 21:27:06,082:INFO:Starting cross validation
2024-02-10 21:27:06,088:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-10 21:41:01,103:INFO:Calculating mean and std
2024-02-10 21:41:01,106:INFO:Creating metrics dataframe
2024-02-10 21:41:01,114:INFO:Uploading results into container
2024-02-10 21:41:01,116:INFO:Uploading model into container now
2024-02-10 21:41:01,119:INFO:_master_model_container: 14
2024-02-10 21:41:01,119:INFO:_display_container: 2
2024-02-10 21:41:01,120:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82E73D450>
2024-02-10 21:41:01,120:INFO:create_model() successfully completed......................................
2024-02-10 21:41:01,265:INFO:SubProcess create_model() end ==================================
2024-02-10 21:41:01,265:INFO:Creating metrics dataframe
2024-02-10 21:41:01,295:INFO:Initializing Dummy Classifier
2024-02-10 21:41:01,296:INFO:Total runtime is 80.60230667591095 minutes
2024-02-10 21:41:01,303:INFO:SubProcess create_model() called ==================================
2024-02-10 21:41:01,304:INFO:Initializing create_model()
2024-02-10 21:41:01,304:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, estimator=dummy, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D82E730070>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-10 21:41:01,304:INFO:Checking exceptions
2024-02-10 21:41:01,305:INFO:Importing libraries
2024-02-10 21:41:01,305:INFO:Copying training dataset
2024-02-10 21:41:01,510:INFO:Defining folds
2024-02-10 21:41:01,511:INFO:Declaring metric variables
2024-02-10 21:41:01,522:INFO:Importing untrained model
2024-02-10 21:41:01,538:INFO:Dummy Classifier Imported successfully
2024-02-10 21:41:01,556:INFO:Starting cross validation
2024-02-10 21:41:01,561:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-10 21:41:04,823:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-10 21:41:04,898:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-10 21:41:04,934:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-10 21:41:05,312:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-10 21:41:05,319:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-10 21:41:05,377:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-10 21:41:05,396:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-10 21:41:05,517:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-10 21:41:07,147:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-10 21:41:07,407:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-10 21:41:07,502:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-10 21:41:07,729:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-10 21:41:07,787:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-10 21:41:08,134:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-10 21:41:08,159:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-10 21:41:08,318:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-10 21:41:09,428:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-10 21:41:09,495:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-10 21:41:09,590:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-10 21:41:09,680:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-02-10 21:41:09,814:INFO:Calculating mean and std
2024-02-10 21:41:09,815:INFO:Creating metrics dataframe
2024-02-10 21:41:09,824:INFO:Uploading results into container
2024-02-10 21:41:09,825:INFO:Uploading model into container now
2024-02-10 21:41:09,826:INFO:_master_model_container: 15
2024-02-10 21:41:09,826:INFO:_display_container: 2
2024-02-10 21:41:09,827:INFO:DummyClassifier(constant=None, random_state=1483, strategy='prior')
2024-02-10 21:41:09,827:INFO:create_model() successfully completed......................................
2024-02-10 21:41:09,938:INFO:SubProcess create_model() end ==================================
2024-02-10 21:41:09,939:INFO:Creating metrics dataframe
2024-02-10 21:41:09,982:INFO:Initializing create_model()
2024-02-10 21:41:09,982:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, estimator=<catboost.core.CatBoostClassifier object at 0x000001D82E73D450>, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-10 21:41:09,982:INFO:Checking exceptions
2024-02-10 21:41:09,986:INFO:Importing libraries
2024-02-10 21:41:09,986:INFO:Copying training dataset
2024-02-10 21:41:10,135:INFO:Defining folds
2024-02-10 21:41:10,135:INFO:Declaring metric variables
2024-02-10 21:41:10,135:INFO:Importing untrained model
2024-02-10 21:41:10,135:INFO:Declaring custom model
2024-02-10 21:41:10,136:INFO:CatBoost Classifier Imported successfully
2024-02-10 21:41:10,139:INFO:Cross validation set to False
2024-02-10 21:41:10,140:INFO:Fitting Model
2024-02-10 21:42:05,349:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82D3EAEF0>
2024-02-10 21:42:05,350:INFO:create_model() successfully completed......................................
2024-02-10 21:42:05,447:INFO:Initializing create_model()
2024-02-10 21:42:05,448:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1483, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-10 21:42:05,448:INFO:Checking exceptions
2024-02-10 21:42:05,453:INFO:Importing libraries
2024-02-10 21:42:05,453:INFO:Copying training dataset
2024-02-10 21:42:05,591:INFO:Defining folds
2024-02-10 21:42:05,591:INFO:Declaring metric variables
2024-02-10 21:42:05,592:INFO:Importing untrained model
2024-02-10 21:42:05,592:INFO:Declaring custom model
2024-02-10 21:42:05,593:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-10 21:42:05,595:INFO:Cross validation set to False
2024-02-10 21:42:05,596:INFO:Fitting Model
2024-02-10 21:42:07,200:INFO:[LightGBM] [Info] Number of positive: 44007, number of negative: 44007
2024-02-10 21:42:07,312:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090063 seconds.
2024-02-10 21:42:07,313:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-10 21:42:07,315:INFO:[LightGBM] [Info] Total Bins 32373
2024-02-10 21:42:07,317:INFO:[LightGBM] [Info] Number of data points in the train set: 88014, number of used features: 127
2024-02-10 21:42:07,319:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2024-02-10 21:42:09,653:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1483, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-02-10 21:42:09,653:INFO:create_model() successfully completed......................................
2024-02-10 21:42:09,764:INFO:Initializing create_model()
2024-02-10 21:42:09,764:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=1483, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-10 21:42:09,764:INFO:Checking exceptions
2024-02-10 21:42:09,767:INFO:Importing libraries
2024-02-10 21:42:09,768:INFO:Copying training dataset
2024-02-10 21:42:09,954:INFO:Defining folds
2024-02-10 21:42:09,954:INFO:Declaring metric variables
2024-02-10 21:42:09,954:INFO:Importing untrained model
2024-02-10 21:42:09,955:INFO:Declaring custom model
2024-02-10 21:42:09,955:INFO:Extra Trees Classifier Imported successfully
2024-02-10 21:42:09,958:INFO:Cross validation set to False
2024-02-10 21:42:09,958:INFO:Fitting Model
2024-02-10 21:42:22,970:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=1483, verbose=0, warm_start=False)
2024-02-10 21:42:22,971:INFO:create_model() successfully completed......................................
2024-02-10 21:42:23,087:INFO:Initializing create_model()
2024-02-10 21:42:23,087:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=1483, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-10 21:42:23,088:INFO:Checking exceptions
2024-02-10 21:42:23,092:INFO:Importing libraries
2024-02-10 21:42:23,092:INFO:Copying training dataset
2024-02-10 21:42:23,240:INFO:Defining folds
2024-02-10 21:42:23,240:INFO:Declaring metric variables
2024-02-10 21:42:23,240:INFO:Importing untrained model
2024-02-10 21:42:23,240:INFO:Declaring custom model
2024-02-10 21:42:23,241:INFO:Random Forest Classifier Imported successfully
2024-02-10 21:42:23,245:INFO:Cross validation set to False
2024-02-10 21:42:23,245:INFO:Fitting Model
2024-02-10 21:42:51,150:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=1483, verbose=0, warm_start=False)
2024-02-10 21:42:51,150:INFO:create_model() successfully completed......................................
2024-02-10 21:42:51,278:INFO:Initializing create_model()
2024-02-10 21:42:51,279:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=1483, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-10 21:42:51,279:INFO:Checking exceptions
2024-02-10 21:42:51,285:INFO:Importing libraries
2024-02-10 21:42:51,286:INFO:Copying training dataset
2024-02-10 21:42:51,477:INFO:Defining folds
2024-02-10 21:42:51,477:INFO:Declaring metric variables
2024-02-10 21:42:51,478:INFO:Importing untrained model
2024-02-10 21:42:51,478:INFO:Declaring custom model
2024-02-10 21:42:51,479:INFO:Gradient Boosting Classifier Imported successfully
2024-02-10 21:42:51,483:INFO:Cross validation set to False
2024-02-10 21:42:51,483:INFO:Fitting Model
2024-02-10 21:50:48,026:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=1483, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-02-10 21:50:48,027:INFO:create_model() successfully completed......................................
2024-02-10 21:50:48,190:INFO:_master_model_container: 15
2024-02-10 21:50:48,190:INFO:_display_container: 2
2024-02-10 21:50:48,192:INFO:[<catboost.core.CatBoostClassifier object at 0x000001D82D3EAEF0>, LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1483, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=1483, verbose=0, warm_start=False), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=1483, verbose=0, warm_start=False), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=1483, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)]
2024-02-10 21:50:48,193:INFO:compare_models() successfully completed......................................
2024-02-10 22:24:47,290:INFO:Initializing tune_model()
2024-02-10 22:24:47,290:INFO:tune_model(estimator=<catboost.core.CatBoostClassifier object at 0x000001D82D3EAEF0>, fold=None, round=4, n_iter=20, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>)
2024-02-10 22:24:47,290:INFO:Checking exceptions
2024-02-10 22:24:47,363:INFO:Copying training dataset
2024-02-10 22:24:47,450:INFO:Checking base model
2024-02-10 22:24:47,450:INFO:Base model : CatBoost Classifier
2024-02-10 22:24:47,455:INFO:Declaring metric variables
2024-02-10 22:24:47,460:INFO:Defining Hyperparameters
2024-02-10 22:24:47,582:INFO:Tuning with n_jobs=-1
2024-02-10 22:24:47,582:INFO:Initializing RandomizedSearchCV
2024-02-10 22:25:49,124:INFO:Initializing tune_model()
2024-02-10 22:25:49,125:INFO:tune_model(estimator=<catboost.core.CatBoostClassifier object at 0x000001D82D3EAEF0>, fold=None, round=4, n_iter=20, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>)
2024-02-10 22:25:49,125:INFO:Checking exceptions
2024-02-10 22:25:49,205:INFO:Copying training dataset
2024-02-10 22:25:49,293:INFO:Checking base model
2024-02-10 22:25:49,293:INFO:Base model : CatBoost Classifier
2024-02-10 22:25:49,301:INFO:Declaring metric variables
2024-02-10 22:25:49,305:INFO:Defining Hyperparameters
2024-02-10 22:25:49,489:INFO:Tuning with n_jobs=-1
2024-02-10 22:25:49,490:INFO:Initializing RandomizedSearchCV
2024-02-11 00:22:37,200:INFO:best_params: {'actual_estimator__random_strength': 0.1, 'actual_estimator__n_estimators': 220, 'actual_estimator__l2_leaf_reg': 20, 'actual_estimator__eta': 0.15, 'actual_estimator__depth': 10}
2024-02-11 00:22:37,212:INFO:Hyperparameter search completed
2024-02-11 00:22:37,214:INFO:SubProcess create_model() called ==================================
2024-02-11 00:22:37,216:INFO:Initializing create_model()
2024-02-11 00:22:37,216:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, estimator=<catboost.core.CatBoostClassifier object at 0x000001D86E6EFF70>, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D80744EA70>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'random_strength': 0.1, 'n_estimators': 220, 'l2_leaf_reg': 20, 'eta': 0.15, 'depth': 10})
2024-02-11 00:22:37,217:INFO:Checking exceptions
2024-02-11 00:22:37,218:INFO:Importing libraries
2024-02-11 00:22:37,219:INFO:Copying training dataset
2024-02-11 00:22:37,415:INFO:Defining folds
2024-02-11 00:22:37,415:INFO:Declaring metric variables
2024-02-11 00:22:37,431:INFO:Importing untrained model
2024-02-11 00:22:37,431:INFO:Declaring custom model
2024-02-11 00:22:37,439:INFO:CatBoost Classifier Imported successfully
2024-02-11 00:22:37,453:INFO:Starting cross validation
2024-02-11 00:22:37,460:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-11 00:46:20,834:INFO:Calculating mean and std
2024-02-11 00:46:20,836:INFO:Creating metrics dataframe
2024-02-11 00:46:20,850:INFO:Finalizing model
2024-02-11 00:47:56,248:INFO:Uploading results into container
2024-02-11 00:47:56,251:INFO:Uploading model into container now
2024-02-11 00:47:56,252:INFO:_master_model_container: 16
2024-02-11 00:47:56,254:INFO:_display_container: 2
2024-02-11 00:47:56,254:INFO:<catboost.core.CatBoostClassifier object at 0x000001D84A585630>
2024-02-11 00:47:56,254:INFO:create_model() successfully completed......................................
2024-02-11 00:47:56,573:INFO:SubProcess create_model() end ==================================
2024-02-11 00:47:56,573:INFO:choose_better activated
2024-02-11 00:47:56,580:INFO:SubProcess create_model() called ==================================
2024-02-11 00:47:56,581:INFO:Initializing create_model()
2024-02-11 00:47:56,581:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, estimator=<catboost.core.CatBoostClassifier object at 0x000001D82D3EAEF0>, fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-11 00:47:56,582:INFO:Checking exceptions
2024-02-11 00:47:56,585:INFO:Importing libraries
2024-02-11 00:47:56,585:INFO:Copying training dataset
2024-02-11 00:47:56,762:INFO:Defining folds
2024-02-11 00:47:56,763:INFO:Declaring metric variables
2024-02-11 00:47:56,763:INFO:Importing untrained model
2024-02-11 00:47:56,763:INFO:Declaring custom model
2024-02-11 00:47:56,764:INFO:CatBoost Classifier Imported successfully
2024-02-11 00:47:56,764:INFO:Starting cross validation
2024-02-11 00:47:56,770:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-11 01:00:40,954:INFO:Calculating mean and std
2024-02-11 01:00:40,955:INFO:Creating metrics dataframe
2024-02-11 01:00:40,959:INFO:Finalizing model
2024-02-11 01:01:30,860:INFO:Uploading results into container
2024-02-11 01:01:30,861:INFO:Uploading model into container now
2024-02-11 01:01:30,861:INFO:_master_model_container: 17
2024-02-11 01:01:30,861:INFO:_display_container: 3
2024-02-11 01:01:30,862:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82DE76E90>
2024-02-11 01:01:30,862:INFO:create_model() successfully completed......................................
2024-02-11 01:01:31,020:INFO:SubProcess create_model() end ==================================
2024-02-11 01:01:31,021:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82DE76E90> result for Accuracy is 0.9415
2024-02-11 01:01:31,021:INFO:<catboost.core.CatBoostClassifier object at 0x000001D84A585630> result for Accuracy is 0.9402
2024-02-11 01:01:31,021:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82DE76E90> is best model
2024-02-11 01:01:31,021:INFO:choose_better completed
2024-02-11 01:01:31,021:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-02-11 01:01:31,048:INFO:_master_model_container: 17
2024-02-11 01:01:31,049:INFO:_display_container: 2
2024-02-11 01:01:31,049:INFO:<catboost.core.CatBoostClassifier object at 0x000001D82DE76E90>
2024-02-11 01:01:31,049:INFO:tune_model() successfully completed......................................
2024-02-11 01:03:45,471:INFO:Initializing calibrate_model()
2024-02-11 01:03:45,472:INFO:calibrate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, estimator=<catboost.core.CatBoostClassifier object at 0x000001D82DE76E90>, method=sigmoid, calibrate_fold=5, fold=None, round=4, fit_kwargs=None, groups=None, verbose=True, return_train_score=False)
2024-02-11 01:03:45,472:INFO:Checking exceptions
2024-02-11 01:03:45,525:INFO:Preloading libraries
2024-02-11 01:03:45,525:INFO:Preparing display monitor
2024-02-11 01:03:45,539:INFO:Getting model name
2024-02-11 01:03:45,539:INFO:Base model : CatBoost Classifier
2024-02-11 01:03:45,553:INFO:Importing untrained CalibratedClassifierCV
2024-02-11 01:03:45,553:INFO:SubProcess create_model() called ==================================
2024-02-11 01:03:45,554:INFO:Initializing create_model()
2024-02-11 01:03:45,555:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, estimator=CalibratedClassifierCV(base_estimator=<catboost.core.CatBoostClassifier object at 0x000001D82DE76E90>,
                       cv=5, ensemble=True, estimator=None, method='sigmoid',
                       n_jobs=None), fold=StratifiedKFold(n_splits=20, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D86E1C3DC0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-11 01:03:45,555:INFO:Checking exceptions
2024-02-11 01:03:45,555:INFO:Importing libraries
2024-02-11 01:03:45,556:INFO:Copying training dataset
2024-02-11 01:03:45,681:INFO:Defining folds
2024-02-11 01:03:45,681:INFO:Declaring metric variables
2024-02-11 01:03:45,685:INFO:Importing untrained model
2024-02-11 01:03:45,685:INFO:Declaring custom model
2024-02-11 01:03:45,691:INFO:CatBoost Classifier Imported successfully
2024-02-11 01:03:45,701:INFO:Starting cross validation
2024-02-11 01:03:45,705:INFO:Cross validating with StratifiedKFold(n_splits=20, random_state=None, shuffle=False), n_jobs=-1
2024-02-11 01:03:47,596:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\calibration.py:321: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.
  warnings.warn(

2024-02-11 01:03:47,619:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\calibration.py:321: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.
  warnings.warn(

2024-02-11 01:03:47,662:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\calibration.py:321: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.
  warnings.warn(

2024-02-11 01:03:47,673:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\calibration.py:321: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.
  warnings.warn(

2024-02-11 01:04:11,424:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\calibration.py:321: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.
  warnings.warn(

2024-02-11 01:04:11,528:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\calibration.py:321: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.
  warnings.warn(

2024-02-11 01:04:11,923:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\calibration.py:321: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.
  warnings.warn(

2024-02-11 01:04:12,561:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\calibration.py:321: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.
  warnings.warn(

2024-02-11 01:22:03,737:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\calibration.py:321: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.
  warnings.warn(

2024-02-11 01:23:04,241:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\calibration.py:321: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.
  warnings.warn(

2024-02-11 01:23:09,730:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\calibration.py:321: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.
  warnings.warn(

2024-02-11 01:23:16,395:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\calibration.py:321: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.
  warnings.warn(

2024-02-11 01:23:17,545:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\calibration.py:321: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.
  warnings.warn(

2024-02-11 01:23:20,720:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\calibration.py:321: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.
  warnings.warn(

2024-02-11 01:23:26,427:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\calibration.py:321: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.
  warnings.warn(

2024-02-11 01:23:33,865:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\calibration.py:321: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.
  warnings.warn(

2024-02-11 01:41:50,615:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\calibration.py:321: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.
  warnings.warn(

2024-02-11 01:41:59,232:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\calibration.py:321: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.
  warnings.warn(

2024-02-11 01:42:49,017:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\calibration.py:321: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.
  warnings.warn(

2024-02-11 01:43:13,056:WARNING:C:\ProgramData\miniconda3\lib\site-packages\sklearn\calibration.py:321: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.
  warnings.warn(

2024-02-11 01:53:44,853:INFO:Calculating mean and std
2024-02-11 01:53:44,855:INFO:Creating metrics dataframe
2024-02-11 01:53:44,867:INFO:Finalizing model
2024-02-11 01:57:12,923:INFO:Uploading results into container
2024-02-11 01:57:12,924:INFO:Uploading model into container now
2024-02-11 01:57:12,925:INFO:_master_model_container: 18
2024-02-11 01:57:12,926:INFO:_display_container: 2
2024-02-11 01:57:12,926:INFO:CalibratedClassifierCV(base_estimator=<catboost.core.CatBoostClassifier object at 0x000001D86E1C6170>,
                       cv=5, ensemble=True, estimator=None, method='sigmoid',
                       n_jobs=None)
2024-02-11 01:57:12,926:INFO:create_model() successfully completed......................................
2024-02-11 01:57:13,059:INFO:SubProcess create_model() end ==================================
2024-02-11 01:57:13,093:INFO:_master_model_container: 18
2024-02-11 01:57:13,093:INFO:_display_container: 2
2024-02-11 01:57:13,094:INFO:CalibratedClassifierCV(base_estimator=<catboost.core.CatBoostClassifier object at 0x000001D86E1C6170>,
                       cv=5, ensemble=True, estimator=None, method='sigmoid',
                       n_jobs=None)
2024-02-11 01:57:13,094:INFO:calibrate_model() successfully completed......................................
2024-02-11 01:57:13,243:INFO:Initializing automl()
2024-02-11 01:57:13,244:INFO:automl(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, optimize=Accuracy, use_holdout=False, turbo=True, return_train_score=False)
2024-02-11 01:57:13,244:INFO:Model Selection Basis : CV Results on Training set
2024-02-11 01:57:13,244:INFO:Checking model 15
2024-02-11 01:57:13,244:INFO:Checking model 16
2024-02-11 01:57:13,244:INFO:Checking model 17
2024-02-11 01:57:13,245:INFO:Initializing create_model()
2024-02-11 01:57:13,245:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, estimator=<catboost.core.CatBoostClassifier object at 0x000001D82DE76E90>, fold=None, round=4, cross_validation=False, predict=False, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-11 01:57:13,245:INFO:Checking exceptions
2024-02-11 01:57:13,247:INFO:Importing libraries
2024-02-11 01:57:13,247:INFO:Copying training dataset
2024-02-11 01:57:13,363:INFO:Defining folds
2024-02-11 01:57:13,363:INFO:Declaring metric variables
2024-02-11 01:57:13,363:INFO:Importing untrained model
2024-02-11 01:57:13,363:INFO:Declaring custom model
2024-02-11 01:57:13,364:INFO:CatBoost Classifier Imported successfully
2024-02-11 01:57:13,366:INFO:Cross validation set to False
2024-02-11 01:57:13,367:INFO:Fitting Model
2024-02-11 01:57:59,201:INFO:<catboost.core.CatBoostClassifier object at 0x000001D86E1C5D80>
2024-02-11 01:57:59,201:INFO:create_model() successfully completed......................................
2024-02-11 01:57:59,436:INFO:<catboost.core.CatBoostClassifier object at 0x000001D86E1C5D80>
2024-02-11 01:57:59,437:INFO:automl() successfully completed......................................
2024-02-11 01:57:59,452:INFO:Initializing predict_model()
2024-02-11 01:57:59,452:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, estimator=<catboost.core.CatBoostClassifier object at 0x000001D86E1C5D80>, probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D82EB068C0>)
2024-02-11 01:57:59,452:INFO:Checking exceptions
2024-02-11 01:57:59,452:INFO:Preloading libraries
2024-02-11 01:57:59,454:INFO:Set up data.
2024-02-11 01:57:59,578:INFO:Set up index.
2024-02-11 02:05:17,007:INFO:Initializing predict_model()
2024-02-11 02:05:17,008:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, estimator=<catboost.core.CatBoostClassifier object at 0x000001D86E1C5D80>, probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D82EB05630>)
2024-02-11 02:05:17,008:INFO:Checking exceptions
2024-02-11 02:05:17,008:INFO:Preloading libraries
2024-02-11 02:06:50,793:INFO:Initializing predict_model()
2024-02-11 02:06:50,793:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D82D60A3B0>, estimator=CalibratedClassifierCV(base_estimator=<catboost.core.CatBoostClassifier object at 0x000001D86E1C6170>,
                       cv=5, ensemble=True, estimator=None, method='sigmoid',
                       n_jobs=None), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D82EB071C0>)
2024-02-11 02:06:50,793:INFO:Checking exceptions
2024-02-11 02:06:50,793:INFO:Preloading libraries
