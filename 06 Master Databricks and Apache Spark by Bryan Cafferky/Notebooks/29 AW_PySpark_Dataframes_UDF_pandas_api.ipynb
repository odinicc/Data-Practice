{"cells":[{"cell_type":"markdown","source":["## Author: Bryan Cafferky Copyright 10/15/2021\n\n#### Caution:  This code provided for demonstration \"as is\" with no implied warrantees.  Always test and vet any code before using."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52e2caa3-1b76-4427-a7a6-f4780655c6f3"}}},{"cell_type":"markdown","source":["# Coding pandas Function API\n\n### In this notebook we will learn how to code the following Spark pandas API function types:\n\n#### - Grouped Map\n#### - Map\n#### - Cogroup Map"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"913e8653-4278-4231-ab4a-fe12b9c13d64"}}},{"cell_type":"markdown","source":["## Notes:\n\n#### - Leverages Apache Arrow.\n#### - Does not use Python type hints.\n#### - This API is experimental according to the Apache Spark documentation. \n#### - Each pandas.DataFrame size can be controlled by spark.sql.execution.arrow.maxRecordsPerBatch."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53d38a6e-724a-43b2-be9c-c9937768b047"}}},{"cell_type":"markdown","source":["![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png)\n\nMore examples are available on the Spark website: http://spark.apache.org/examples.html\n\nDocumentation on pandas unction API at:\nhttps://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/pandas-function-apis"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"747299d4-9c34-436a-b1d8-d18ef5f7251a"}}},{"cell_type":"code","source":["http://www.ltcconline.net/greenl/courses/201/probdist/zScore.htm%md ### Warning!!!\n\n#### To run this code, you need to have uploaded the files and created the database tables - see Lesson 9 - Creating the SQL Tables on Databricks.  Link in video description to that video."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d1e7671-afac-418e-9edb-e2a002da888f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Skips Code Cells 1 through 9 if you already have Apache Arrow and PyArrow enabled."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"59afd71b-0263-408a-9faf-cbe985dee3a7"}}},{"cell_type":"code","source":["sc.version"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 1 - Check the Spark version","showTitle":true,"inputWidgets":{},"nuid":"b32576c0-ee97-4eb1-97ef-24b30d855c3b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# See if Arrow is enabled.\nspark.conf.get(\"spark.sql.execution.arrow.enabled\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 2 - Check if Arrow is Enabled","showTitle":true,"inputWidgets":{},"nuid":"110beeb1-8ae8-446a-857c-be1a3de15f85"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Enable Arrow-based columnar data transfers\nspark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 3 - You can enable Apache Arrow as follows.","showTitle":true,"inputWidgets":{},"nuid":"3a6bf7f9-45c5-43d4-9106-45dad640916e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Enabling for Conversion to/from Pandas\n\nArrow is available as an optimization when converting a Spark DataFrame to a Pandas DataFrame using the call toPandas() and when creating a Spark DataFrame from a Pandas DataFrame with createDataFrame(pandas_df). To use Arrow when executing these calls, users need to first set the Spark configuration spark.sql.execution.arrow.pyspark.enabled to true. This is disabled by default.\n\nSee https://spark.apache.org/docs/3.0.1/sql-pyspark-pandas-with-arrow.html#enabling-for-conversion-tofrom-pandas"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3fbe5ccf-2572-4f24-bbe3-2642e481e9b0"}}},{"cell_type":"code","source":["# Enable Arrow-based columnar data transfers\nspark.conf.get(\"spark.sql.execution.arrow.pyspark.enabled\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 4 - Confirm PyArrow is enabled.","showTitle":true,"inputWidgets":{},"nuid":"dd5f9089-ca2d-4f89-8886-a98081d3434f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Enable Arrow-based columnar data transfers\nspark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 5 ","showTitle":true,"inputWidgets":{},"nuid":"beb8e591-a313-4627-84b9-ac7ba5293089"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.conf.get(\"spark.sql.execution.arrow.pyspark.enabled\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 6","showTitle":true,"inputWidgets":{},"nuid":"17b428c5-63b1-4072-973b-4e336b77276c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In addition, optimizations enabled by spark.sql.execution.arrow.pyspark.enabled could fallback automatically to non-Arrow optimization implementation if an error occurs before the actual computation within Spark. This can be controlled by spark.sql.execution.arrow.pyspark.fallback.enabled."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7df36aee-80f8-4902-9973-942cdb91bb76"}}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 7","showTitle":true,"inputWidgets":{},"nuid":"e03ac110-1126-4ded-a51a-1c3baaac846c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Recommended Pandas and PyArrow Versions\nFor usage with pyspark.sql, the supported versions of Pandas is 0.24.2 and PyArrow is 0.15.1. Higher versions may be used, however, compatibility and data correctness can not be guaranteed and should be verified by the user.\n\nSee https://spark.apache.org/docs/3.0.0/sql-pyspark-pandas-with-arrow.html#recommended-pandas-and-pyarrow-versions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Versions: pandas and PyArrow","showTitle":true,"inputWidgets":{},"nuid":"6dd49db9-ba8c-431e-8053-f5fdede2a96c"}}},{"cell_type":"code","source":["import pandas as pd\n\npd.show_versions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 8","showTitle":true,"inputWidgets":{},"nuid":"ffa8f2a8-30a5-4075-8687-d1a4b2f57130"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyarrow\n\npyarrow.__version__"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 9 - Checking pyarrow version","showTitle":true,"inputWidgets":{},"nuid":"d5f32cb6-a307-45f4-a9f5-27508fb25376"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Coding pandas API functions starts here!!!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9c9c4d5-7988-48ed-acd6-9409b3d53b3b"}}},{"cell_type":"markdown","source":["### Create dataframe from a Spark SQL table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"742d37e0-6142-4209-90c0-8c624208d4f3"}}},{"cell_type":"markdown","source":["### Dataframe naming prefix convention:\n##### 1st character is s for Spark DF\n##### 2nd character is p for Python\n##### 3rd and 4th character is df for dataframe\n##### 5th = _ separator\n##### rest is a meaningful name\n\n##### spdf_salessummary = a Spark Python dataframe containing sales summary information."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cfff99bb-7207-422e-99af-2addb28b6618"}}},{"cell_type":"code","source":["spark.sql('use awproject')\n\nspdf_sales = spark.sql('select CustomerKey, SalesAmount from factinternetsales limit 15000').dropna()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 10 - Load our Spark data frame","showTitle":true,"inputWidgets":{},"nuid":"27d2b69b-c551-4c75-b466-11c79262c5f3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["display(spdf_sales)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 11 - View the data","showTitle":true,"inputWidgets":{},"nuid":"887f75d5-0382-4e8c-9459-831349a9e48c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Group Map\n\n###### You transform your grouped data via groupBy().applyInPandas() to implement the “split-apply-combine” pattern. Split-apply-combine consists of three steps:\n\n- Split the data into groups by using DataFrame.groupBy.\n- Apply a function on each group. The input and output of the function are both pandas.DataFrame. \n- The input data contains all the rows and columns for each group.\n- Combine the results into a new DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9d090b05-8346-433d-9fd4-704768bfdaf0"}}},{"cell_type":"code","source":["df = spark.createDataFrame(\n    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n    (\"id\", \"v\"))\n\ndef subtract_mean(pdf):\n    # pdf is a pandas.DataFrame\n    v = pdf.v\n    return pdf.assign(v_minus_mean = v - v.mean())\n\ndf.groupby(\"id\").applyInPandas(subtract_mean, schema=\"id long, v double, v_minus_mean double\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 12 - Simple Group Map example","showTitle":true,"inputWidgets":{},"nuid":"828efaf2-773c-4c6d-b6a4-98d065114e63"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### See information at blog\nLink https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.applyInPandas.html#pyspark-sql-groupeddata-applyinpandas\n\n#### For information about calculating the z-score of a number see\nhttps://developers.google.com/machine-learning/data-prep/transform/normalization\n\nhttp://www.ltcconline.net/greenl/courses/201/probdist/zScore.htm\n\n\n### To Get the Z-score:\n\nz-score  = (value - values mean) / standard deviation of the values"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"81f0dc9e-a426-4975-97cc-c5cf7bae30ec"}}},{"cell_type":"code","source":["import pandas as pd\n\ndef get_z_score(pdf):\n    # pdf is a pandas.DataFrame\n    SalesAmount = pdf.SalesAmount\n    return pdf.assign(ZScoreSales = (SalesAmount - SalesAmount.mean()) / SalesAmount.std(), AvgSales =  SalesAmount.mean())\n\nspdf_sales.groupby(\"CustomerKey\").applyInPandas(get_z_score, schema=\"CustomerKey long, SalesAmount double, ZScoreSales double, AvgSales double\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 13 - Group Map - Create a function to get the z-core of each SalesAmount","showTitle":true,"inputWidgets":{},"nuid":"e9bd9da7-3cce-4aa3-b2c0-898e843a8d0f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Map \n\n #### - Note:  Seems to be primarily used to filter Spark dataframes.\n\n\n- Uses DataFrame.mapInPandas() in order to transform an iterator of pandas.DataFrame to another iterator of pandas.DataFrame that represents the current PySpark DataFrame.\n- Returns the result as a PySpark DataFrame.\n- The underlying function takes and outputs an iterator of pandas.DataFrame. \n- It can return the output of arbitrary length in contrast to some pandas UDFs such as Series to Series pandas UDF."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16e5a45d-3313-4cc5-96a6-70ef85108bd9"}}},{"cell_type":"code","source":["df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n\ndef filter_func(iterator):\n    for pdf in iterator:\n        yield pdf[pdf.id == 1]\n\ndf.mapInPandas(filter_func, schema=df.schema).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 14 - Map - Simple Example","showTitle":true,"inputWidgets":{},"nuid":"62ab6a72-e85d-43f6-807d-22f7c1905900"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Some experimentation with the Map API"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"75a09a5a-ecd1-47e9-ac25-4279d7f7278f"}}},{"cell_type":"code","source":["b_multiplier = sc.broadcast(2)\nb_multiplier.value"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 15 - Broadcast a variable","showTitle":true,"inputWidgets":{},"nuid":"34fd6ad1-89f5-47c6-9421-3c483eb60483"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### When you create a pandas UDF as I showed in a prior video, \n#### you can do initialization work before you start the iterator.  This seems to work here as well."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fd66f61c-314d-4324-a4f4-b2cc31995e25"}}},{"cell_type":"code","source":["df = spark.createDataFrame([(1, 21), (1, 15), (2, 30)], (\"id\", \"age\"))\n\ndef filter_func(iterator):\n  \n    # Do some expensive initialization with a state  - This is not mentioned in the docs\n    # so beware.  It may not be a good idea but wanted to see if it worked.  :-) \n    multiplier = b_multiplier.value\n    \n    for pdf in iterator:\n        yield pdf[pdf.id == 1].assign(age_times_x = pdf.age[pdf.id == 1] * multiplier)\n\ndf.mapInPandas(filter_func, schema=\"id long, age long, age_times_x double\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 16 - Map - A bit more complex and experimental","showTitle":true,"inputWidgets":{},"nuid":"b4e0bb8f-846d-4b7d-93fa-43a738e6b78a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def filter_func(iterator):\n    for pdf in iterator:\n        yield pdf[pdf.CustomerKey == 11000]\n\nspdf_sales.mapInPandas(filter_func, schema=spdf_sales.schema).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 16 - Map - Create a function to filter the sales dataframe","showTitle":true,"inputWidgets":{},"nuid":"61cc00f9-e4ae-4365-96ee-81bbd441b04a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def filter_func(iterator):\n    for pdf in iterator:\n        SalesAmount = pdf.SalesAmount\n        pdf = pdf.assign(SalesAmount = (SalesAmount - SalesAmount.mean()) / SalesAmount.std())\n        yield pdf[pdf.CustomerKey == 11000]\n\nspdf_sales.mapInPandas(filter_func, schema=spdf_sales.schema).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 17 - Map - Doing a calculation on a column in the dataframe. Probably not a good idea.","showTitle":true,"inputWidgets":{},"nuid":"0ba2e49d-4537-4b6d-875b-c55a2ada9c0b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Cogrouped Map\n\n#### Purpose:  Join dataframes on specified keys.\n\nThis function requires a full shuffle, i.e. this is an expensive operation.\n\nAll the data of a cogroup will be loaded into memory, so the user should be aware of the potential OOM risk if data is skewed and certain groups are too large to fit in memory.\n\nIf returning a new pandas.DataFrame constructed with a dictionary, it is recommended to explicitly index the columns by name to ensure the positions are correct, or alternatively use an OrderedDict. For example, pd.DataFrame({‘id’: ids, ‘a’: data}, columns=[‘id’, ‘a’]) or pd.DataFrame(OrderedDict([(‘id’, ids), (‘a’, data)]))."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ad87718d-f06b-4abe-814f-ab40dd08351a"}}},{"cell_type":"markdown","source":["#### It consists of the following steps:\n  \n- Shuffle the data such that the groups of each DataFrame which share a key are cogrouped together.\n- Apply a function to each cogroup. The input of the function is two pandas.DataFrame (with an optional tuple representing the key). The output of the function is a pandas.DataFrame.\n- Combine the pandas.DataFrames from all groups into a new PySpark DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb846ec7-d2ab-4639-b58f-85c50269c43d"}}},{"cell_type":"code","source":["import pandas as pd\n\ndf1 = spark.createDataFrame(\n    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n    (\"time\", \"id\", \"v1\"))\n\ndf2 = spark.createDataFrame(\n    [(20000101, 1, \"x\"), (20000101, 2, \"y\")],\n    (\"time\", \"id\", \"v2\"))\n\ndef asof_join(l, r):\n    return pd.merge_asof(l, r, on=\"time\", by=\"id\")\n\ndf1.groupby(\"id\").cogroup(df2.groupby(\"id\")).applyInPandas(\n    asof_join, schema=\"time int, id int, v1 double, v2 string\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 18 - Cogrouped Map - Simple Example","showTitle":true,"inputWidgets":{},"nuid":"0bbca8bc-4ace-4d8c-bfc8-71bc763ba142"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------+---+---+---+\n|    time| id| v1| v2|\n+--------+---+---+---+\n|20000101|  1|1.0|  x|\n|20000102|  1|3.0|  x|\n|20000101|  2|2.0|  y|\n|20000102|  2|4.0|  y|\n+--------+---+---+---+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+---+---+---+\n    time| id| v1| v2|\n+--------+---+---+---+\n20000101|  1|1.0|  x|\n20000102|  1|3.0|  x|\n20000101|  2|2.0|  y|\n20000102|  2|4.0|  y|\n+--------+---+---+---+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### For more information on merge_asof() is a pandas function\nSee https://pandas.pydata.org/pandas-docs/version/0.25.0/reference/api/pandas.merge_asof.html"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f2ee385-0d02-4c3c-9228-6fbd204f938a"}}},{"cell_type":"code","source":["spark.sql('use awproject')\nspdf_customer = spark.sql('select CustomerKey, BirthDate from DimCustomer').dropna()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 19 - Load customer data into a Spark dataframe","showTitle":true,"inputWidgets":{},"nuid":"23d1a8f5-fe3b-4e41-bbf0-6edabd7469da"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import pandas as pd\n\ndef asof_join(l, r):\n    return pd.merge_asof(l, r, on=\"CustomerKey\")\n\nspdf_customer.groupby(\"CustomerKey\").cogroup(spdf_sales.groupby(\"CustomerKey\")).applyInPandas(\n    asof_join, schema=\"CustomerKey int, SalesAmount double, BirthDate string\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 20 - Cogroup Map - Join Customer dataframe to Sales dataframe on CustomerKey","showTitle":true,"inputWidgets":{},"nuid":"1f5e48cd-3fa2-4735-a393-f8be7aacf03c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----------+-----------+----------+\n|CustomerKey|SalesAmount| BirthDate|\n+-----------+-----------+----------+\n|      11000|      21.98|1971-10-06|\n|      11001|       8.99|1976-05-10|\n|      11002|      34.99|1971-02-09|\n|      11003|       8.99|1973-08-14|\n|      11004|      34.99|1979-08-05|\n|      11005|       2.29|1976-08-01|\n|      11006|       4.99|1976-12-02|\n|      11007|      34.99|1969-11-06|\n|      11008|      34.99|1975-07-04|\n|      11009|       2.29|1969-09-29|\n|      11010|    2294.99|1969-08-05|\n|      11011|      53.99|1969-05-03|\n|      11012|      34.99|1979-01-14|\n|      11013|       null|1979-08-03|\n|      11014|      34.99|1973-11-06|\n|      11015|      159.0|1984-08-26|\n|      11016|       2.29|1984-10-25|\n|      11017|      21.98|1949-12-24|\n|      11018|      21.98|1955-10-06|\n|      11019|       8.99|1983-09-04|\n+-----------+-----------+----------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+-----------+----------+\nCustomerKey|SalesAmount| BirthDate|\n+-----------+-----------+----------+\n      11000|      21.98|1971-10-06|\n      11001|       8.99|1976-05-10|\n      11002|      34.99|1971-02-09|\n      11003|       8.99|1973-08-14|\n      11004|      34.99|1979-08-05|\n      11005|       2.29|1976-08-01|\n      11006|       4.99|1976-12-02|\n      11007|      34.99|1969-11-06|\n      11008|      34.99|1975-07-04|\n      11009|       2.29|1969-09-29|\n      11010|    2294.99|1969-08-05|\n      11011|      53.99|1969-05-03|\n      11012|      34.99|1979-01-14|\n      11013|       null|1979-08-03|\n      11014|      34.99|1973-11-06|\n      11015|      159.0|1984-08-26|\n      11016|       2.29|1984-10-25|\n      11017|      21.98|1949-12-24|\n      11018|      21.98|1955-10-06|\n      11019|       8.99|1983-09-04|\n+-----------+-----------+----------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":"2"},"version":"2.7.11","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"AW_PySpark_Dataframes_UDF_pandas_api","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4498232119725116}},"nbformat":4,"nbformat_minor":0}
