{"cells":[{"cell_type":"markdown","source":["# Coding pandas User Defined Functions (UDF)\n\n![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png)\n\nMore examples are available on the Spark website: http://spark.apache.org/examples.html\n\nDocumentation on pandas UDFs at:\nhttps://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/udf-python-pandas"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"913e8653-4278-4231-ab4a-fe12b9c13d64"}}},{"cell_type":"markdown","source":["## Author: Bryan Cafferky Copyright 09/13/2021"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52e2caa3-1b76-4427-a7a6-f4780655c6f3"}}},{"cell_type":"markdown","source":["### Warning!!!\n\n#### To run this code, you need to have uploaded the files and created the database tables - see Lesson 9 - Creating the SQL Tables on Databricks.  Link in video description to that video."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d1e7671-afac-418e-9edb-e2a002da888f"}}},{"cell_type":"code","source":["sc.version"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 1 - Check the Spark version","showTitle":true,"inputWidgets":{},"nuid":"b32576c0-ee97-4eb1-97ef-24b30d855c3b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[20]: &#39;3.0.1&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[20]: &#39;3.0.1&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# See if Arrow is enabled.\nspark.conf.get(\"spark.sql.execution.arrow.enabled\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 2 - Check if Arrow is Enabled","showTitle":true,"inputWidgets":{},"nuid":"110beeb1-8ae8-446a-857c-be1a3de15f85"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Enable Arrow-based columnar data transfers\nspark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 3 - You can enable Apache Arrow as follows.","showTitle":true,"inputWidgets":{},"nuid":"3a6bf7f9-45c5-43d4-9106-45dad640916e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Enabling for Conversion to/from Pandas\n\nArrow is available as an optimization when converting a Spark DataFrame to a Pandas DataFrame using the call toPandas() and when creating a Spark DataFrame from a Pandas DataFrame with createDataFrame(pandas_df). To use Arrow when executing these calls, users need to first set the Spark configuration spark.sql.execution.arrow.pyspark.enabled to true. This is disabled by default.\n\nSee https://spark.apache.org/docs/3.0.1/sql-pyspark-pandas-with-arrow.html#enabling-for-conversion-tofrom-pandas"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3fbe5ccf-2572-4f24-bbe3-2642e481e9b0"}}},{"cell_type":"code","source":["# Enable Arrow-based columnar data transfers\nspark.conf.get(\"spark.sql.execution.arrow.pyspark.enabled\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 4 - Confirm PyArrow is enabled.","showTitle":true,"inputWidgets":{},"nuid":"dd5f9089-ca2d-4f89-8886-a98081d3434f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Enable Arrow-based columnar data transfers\nspark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 5 ","showTitle":true,"inputWidgets":{},"nuid":"beb8e591-a313-4627-84b9-ac7ba5293089"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.conf.get(\"spark.sql.execution.arrow.pyspark.enabled\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 6","showTitle":true,"inputWidgets":{},"nuid":"17b428c5-63b1-4072-973b-4e336b77276c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["In addition, optimizations enabled by spark.sql.execution.arrow.pyspark.enabled could fallback automatically to non-Arrow optimization implementation if an error occurs before the actual computation within Spark. This can be controlled by spark.sql.execution.arrow.pyspark.fallback.enabled."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7df36aee-80f8-4902-9973-942cdb91bb76"}}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 7","showTitle":true,"inputWidgets":{},"nuid":"e03ac110-1126-4ded-a51a-1c3baaac846c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Recommended Pandas and PyArrow Versions\nFor usage with pyspark.sql, the supported versions of Pandas is 0.24.2 and PyArrow is 0.15.1. Higher versions may be used, however, compatibility and data correctness can not be guaranteed and should be verified by the user.\n\nSee https://spark.apache.org/docs/3.0.0/sql-pyspark-pandas-with-arrow.html#recommended-pandas-and-pyarrow-versions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Versions: pandas an PyArrow","showTitle":true,"inputWidgets":{},"nuid":"6dd49db9-ba8c-431e-8053-f5fdede2a96c"}}},{"cell_type":"code","source":["import pandas as pd\n\npd.show_versions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 8","showTitle":true,"inputWidgets":{},"nuid":"ffa8f2a8-30a5-4075-8687-d1a4b2f57130"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["import pyarrow\n\npyarrow.__version__"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 9 - Checking pyarrow version","showTitle":true,"inputWidgets":{},"nuid":"d5f32cb6-a307-45f4-a9f5-27508fb25376"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Create dataframe from a Spark SQL table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"742d37e0-6142-4209-90c0-8c624208d4f3"}}},{"cell_type":"markdown","source":["### Dataframe naming prefix convention:\n##### 1st character is s for Spark DF\n##### 2nd character is p for Python\n##### 3rd and 4th character is df for dataframe\n##### 5th = _ separator\n##### rest is a meaningful name\n\n##### spdf_salessummary = a Spark Python dataframe containing sales summary information."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cfff99bb-7207-422e-99af-2addb28b6618"}}},{"cell_type":"code","source":["spark.sql('use awproject')\nspdf_sales = spark.sql('select CustomerKey, OrderDateKey, SalesAmount, TotalProductCost from factinternetsales limit 10').dropna()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 10 - Use Spark SQL to load a PySpark dataframe from the t_salesinfo table...","showTitle":true,"inputWidgets":{},"nuid":"732c5c11-4ec9-43df-9bd4-72e359eec2ef"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["display(spdf_sales)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 11 - View the data","showTitle":true,"inputWidgets":{},"nuid":"887f75d5-0382-4e8c-9459-831349a9e48c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["import pandas as pd\nfrom pyspark.sql.functions import pandas_udf       \n\n@pandas_udf('double')  \ndef margin_precent_udf(salesamount: pd.Series, productcost: pd.Series) -> pd.Series:\n  return (salesamount - productcost) / salesamount\n\nspdf_sales.select(\"SalesAmount\", \"TotalProductCost\", margin_precent_udf(\"SalesAmount\", \"TotalProductCost\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 12 - Series to Series","showTitle":true,"inputWidgets":{},"nuid":"4aa4906a-7f3f-49c7-86b8-8fc04bdbe789"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["b_taxrate = sc.broadcast(.07)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 13 - Broadcast a variable","showTitle":true,"inputWidgets":{},"nuid":"fe02fd3b-ae97-4451-b3d7-36f36dd5b153"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["b_taxrate.value"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 14 - Retrieve the value of a broadcast variable","showTitle":true,"inputWidgets":{},"nuid":"bf47381b-ef98-49c6-b603-9a44c7114170"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["from typing import Iterator\nimport pandas as pd\nfrom pyspark.sql.functions import pandas_udf      \n\n@pandas_udf(\"long\")\ndef tax_udf(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n\n    # Do some expensive initialization with a state   \n    taxrate = b_taxrate.value\n    \n    for salesamount in iterator:\n        # Use that state for the whole iterator.\n        yield (taxrate * salesamount)\n\nspdf_sales.select(tax_udf(\"SalesAmount\").alias(\"Tax\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 15 - Iterator of Series to Iterator of Series","showTitle":true,"inputWidgets":{},"nuid":"823de255-3e02-49cb-867e-f3885cd8305c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["from typing import Iterator, Tuple\nimport pandas as pd\n\nfrom pyspark.sql.functions import pandas_udf\n\n@pandas_udf('double')  \ndef margin_precent_multi_iter_udf(iterator: Iterator[Tuple[pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n   for salesamount, productcost in iterator:\n        yield (salesamount - productcost) / salesamount"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 16 - Iterator of Multiple Series to Iterator of Series","showTitle":true,"inputWidgets":{},"nuid":"3f154a47-7cf8-411e-ac64-4a209e18e249"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# spdf_sales.select(multiply_two_cols(\"SalesAmount\", \"SalesAmount\")).show()\nspdf_sales.select(\"SalesAmount\", \"TotalProductCost\", margin_precent_multi_iter_udf(\"SalesAmount\", \"TotalProductCost\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code Cell 17 - Calling Iterator of Multiple Series to Iterator of Series","showTitle":true,"inputWidgets":{},"nuid":"9072ad2a-c2c0-4923-a1e4-c1672ad40af5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":"2"},"version":"2.7.11","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"AW_PySpark_Dataframes_UDF_pandas","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4498232119725090}},"nbformat":4,"nbformat_minor":0}
