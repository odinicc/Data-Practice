sc

#SparkContext
spark

#Load file into RDD
poem_rdd1 = sc.textFile("dbfs:/FileStore/tables/hamlet.txt")

poem_rdd1

#take first 2 rows
poem_rdd1.take(2)

#Transformations:  map()
lineLengths = poem_rdd1.map(lambda s: len(s))         # transformation
totalLength = lineLengths.reduce(lambda a, b: a + b)  # action
print(totalLength)

#Get the rdd item count
my_rdd  = sc.parallelize([1, 2, 3, 4, 5]) # creates RDD
my_rdd.count()  # action

print(my_rdd.take(3))  # action returns elements to the driver

#Show the number of partitions
poem_rdd1.getNumPartitions() 

#Using a defined function with map
def transfunc(lines):
      lines = lines.lower()
      lines = lines.split()
      return lines
    
poem_rdd2 = poem_rdd1.map(transfunc)
poem_rdd2.take(5)

#Using flatmap to flatten out the rdd
poem_rdd3 = poem_rdd1.flatMap(transfunc)
poem_rdd3.take(15)

#Get distinct values in an rdd
poem_rdd3.distinct().take(5)

#Using filter to apply a search condition
skipwords = ['to','the','of']
poem_rdd4 = poem_rdd3.filter(lambda x: x not in skipwords)
poem_rdd4.take(10)


#Getting statistics on an rdd
numbers_rdd = sc.parallelize(range(1,500))
numbers_rdd.max(), numbers_rdd.min(), numbers_rdd.sum(),numbers_rdd.variance(),numbers_rdd.stdev()